# Combined Elliptic GNN + Temporal Fusion Streamlit app
# Paste/run in the same environment where torch_geometric and Elliptic dataset are available.

import os, json, io, inspect
import numpy as np
import pandas as pd
import streamlit as st
import torch, torch.nn.functional as F
from torch_geometric.datasets import EllipticBitcoinDataset
from torch_geometric.utils import k_hop_subgraph
from torch_geometric.nn import SAGEConv, GATConv
from streamlit.components.v1 import html

# ----------------------
# UI setup
# ----------------------
st.set_page_config(page_title="Crypto Graph+Temporal Fusion â€” Elliptic", layout="wide")
st.title("ðŸ•¸ï¸ Crypto Graph Fraud â€” GNN + Temporal Fusion Demo (Elliptic)")

with st.sidebar:
    st.header("Controls")
    model_name = st.selectbox("GNN Model", ["GraphSAGE (baseline)", "GAT (influence)"])
    fuse_toggle = st.checkbox("Fuse Global Temporal Embedding", value=True)
    temporal_model = st.selectbox("Temporal Encoder", ["None","LSTM","GRU","Transformer"])
    seq_len = st.slider("Temporal sequence length (timesteps)", 16, 512, 128, step=16)
    k_top = st.slider("Top suspicious to list", 10, 200, 50, step=10)
    khop = st.slider("Neighborhood radius (k-hop)", 1, 3, 2)
    score_thr = st.slider("Alert threshold (prob â‰¥)", 0.50, 0.99, 0.90, 0.01)
    show_attn = st.checkbox("Show attention (GAT only)", value=True, disabled=(model_name!="GAT (influence)"))

st.info("Dataset: Elliptic Bitcoin Transaction Graph â€” node classification (illicit vs licit).")

# ----------------------
# GNN models
# ----------------------
class SageNet(torch.nn.Module):
    def __init__(self, in_dim, hid=128, out=2):
        super().__init__()
        self.c1 = SAGEConv(in_dim, hid)
        self.c2 = SAGEConv(hid, hid)
        self.out = torch.nn.Linear(hid, out)
    def forward(self, x, ei):
        x = F.relu(self.c1(x, ei))
        x = F.dropout(x, 0.5, training=self.training)
        x = F.relu(self.c2(x, ei))
        return self.out(x)

class GATNet(torch.nn.Module):
    def __init__(self, in_dim, hid=64, heads=4, out=2):
        super().__init__()
        self.g1 = GATConv(in_dim, hid, heads=heads, dropout=0.4)
        self.g2 = GATConv(hid*heads, hid, heads=1, dropout=0.4)
        self.out = torch.nn.Linear(hid, out)
    def forward(self, x, ei, return_attention=False):
        if return_attention:
            h1, attn1 = self.g1(x, ei, return_attention_weights=True)
            h1 = F.elu(h1); h1 = F.dropout(h1, 0.5, training=self.training)
            h2, attn2 = self.g2(h1, ei, return_attention_weights=True)
            logits = self.out(h2)
            return logits, (attn1, attn2)
        h1 = F.elu(self.g1(x, ei))
        h1 = F.dropout(h1, 0.5, training=self.training)
        h2 = self.g2(h1, ei)
        return self.out(h2)

# ----------------------
# Temporal encoders (lightweight demo versions)
# ----------------------
class LSTMEncoder(torch.nn.Module):
    def __init__(self, in_dim, hid=64, nlayers=1, bidir=False):
        super().__init__()
        self.lstm = torch.nn.LSTM(in_dim, hid, nlayers, batch_first=True, bidirectional=bidir)
        self.out_dim = hid * (2 if bidir else 1)
    def forward(self, x):  # x: [B, T, F]
        out, (h, c) = self.lstm(x)
        # use last layer hidden
        if self.lstm.bidirectional:
            h_last = torch.cat([h[-2], h[-1]], dim=-1)
        else:
            h_last = h[-1]
        return h_last  # [B, out_dim]

class GRUEncoder(torch.nn.Module):
    def __init__(self, in_dim, hid=64, nlayers=1, bidir=False):
        super().__init__()
        self.gru = torch.nn.GRU(in_dim, hid, nlayers, batch_first=True, bidirectional=bidir)
        self.out_dim = hid * (2 if bidir else 1)
    def forward(self, x):
        out, h = self.gru(x)
        if self.gru.bidirectional:
            h_last = torch.cat([h[-2], h[-1]], dim=-1)
        else:
            h_last = h[-1]
        return h_last

# Minimal transformer encoder with CLS pooling
class PosEnc(torch.nn.Module):
    def __init__(self, d_model, max_len=2048):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len).unsqueeze(1).float()
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer('pe', pe.unsqueeze(0))
    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

import math
class TransformerEncoderSmall(torch.nn.Module):
    def __init__(self, in_dim, model_dim=64, nhead=4, nlayers=2):
        super().__init__()
        self.proj = torch.nn.Linear(in_dim, model_dim)
        enc_layer = torch.nn.TransformerEncoderLayer(d_model=model_dim, nhead=nhead, batch_first=True)
        self.encoder = torch.nn.TransformerEncoder(enc_layer, num_layers=nlayers)
        self.cls = torch.nn.Parameter(torch.randn(1,1,model_dim)*0.01)
        self.out_dim = model_dim
    def forward(self, x):
        B = x.size(0)
        x = self.proj(x)
        cls = self.cls.expand(B, -1, -1)
        x = torch.cat([cls, x], dim=1)
        x = self.encoder(x)
        return x[:,0]  # CLS pooled

# ----------------------
# FiLM fusion (global conditioning)
# ----------------------
class FiLMFusionModule(torch.nn.Module):
    def __init__(self, node_dim, temp_dim):
        super().__init__()
        self.gamma = torch.nn.Linear(temp_dim, node_dim)
        self.beta  = torch.nn.Linear(temp_dim, node_dim)
    def forward(self, node_x, temp_emb):
        # node_x: [N, F_node] or [B, N, F_node]; temp_emb: [temp_dim] or [B, temp_dim]
        if node_x.dim()==2:
            # single batch
            N, F = node_x.shape
            if temp_emb.dim()==1:
                gamma = self.gamma(temp_emb).unsqueeze(0)  # [1, F]
                beta  = self.beta(temp_emb).unsqueeze(0)
                node_mod = node_x * (1 + gamma) + beta
                return node_mod
            else:
                # temp_emb [B, D], but node_x [N, F] - not expected
                raise ValueError("Incompatible dims")
        else:
            B, N, F = node_x.shape
            if temp_emb.dim()==2:
                gamma = self.gamma(temp_emb).unsqueeze(1)  # [B,1,F]
                beta  = self.beta(temp_emb).unsqueeze(1)
                return node_x * (1 + gamma) + beta
            else:
                raise ValueError("Incompatible dims")

# ----------------------
# Load Elliptic dataset (cached)
# ----------------------
@st.cache_resource(show_spinner=True)
def load_elliptic():
    ds = EllipticBitcoinDataset(root="data/elliptic")
    d = ds[0]
    y = d.y.numpy()
    known = y != 0
    labels = (y == 2).astype(int)
    idx = np.where(known)[0]; rng = np.random.RandomState(42); rng.shuffle(idx)
    n = len(idx)
    tr = np.zeros(d.num_nodes, bool); va = np.zeros(d.num_nodes, bool); te = np.zeros(d.num_nodes, bool)
    tr[idx[:int(.7*n)]] = True; va[idx[int(.7*n):int(.85*n)]] = True; te[idx[int(.85*n):]] = True
    meta = dict(num_nodes=int(d.num_nodes), num_edges=int(d.edge_index.shape[1]))
    return d, labels, tr, va, te, meta

data, labels, train_mask, val_mask, test_mask, meta = load_elliptic()

# ----------------------
# Build base GNN model and fusion module
# ----------------------
def build_gnn_with_optional_fusion(model_name, node_feat_dim, temp_emb_dim=None):
    if model_name == "GraphSAGE (baseline)":
        gnn = SageNet(in_dim=node_feat_dim)
    else:
        gnn = GATNet(in_dim=node_feat_dim)
    film = FiLMFusionModule(node_dim=node_feat_dim, temp_dim=temp_emb_dim) if temp_emb_dim is not None else None
    return gnn, film

# ----------------------
# Temporal embedding creator (global)
# ----------------------
# For demo: use simple synthetic global series if you don't provide market series.
def get_global_series(length):
    # Here you can replace with real OHLCV read (e.g., ccxt fetch) and compute features (close, vol, returns,...)
    # For the demo we generate a synthetic close and volume features.
    t = np.arange(length)
    close = 10000 + np.cumsum(np.random.normal(0, 1, size=length))  # random walk
    vol = np.random.uniform(1,10,size=length)
    # stack simple features: close normalized + vol
    series = np.stack([ (close - close.mean())/ (close.std()+1e-9), (vol - vol.mean())/(vol.std()+1e-9) ], axis=-1)
    return torch.tensor(series, dtype=torch.float32).unsqueeze(0)  # [1, T, F]

# ----------------------
# Build temporal encoder based on UI choice
# ----------------------
import math
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def make_temporal_encoder(kind, in_dim, hid=64):
    if kind == "LSTM":
        enc = LSTMEncoder(in_dim, hid=hid).to(device)
    elif kind == "GRU":
        enc = GRUEncoder(in_dim, hid=hid).to(device)
    elif kind == "Transformer":
        enc = TransformerEncoderSmall(in_dim, model_dim=hid).to(device)
    else:
        enc = None
    return enc

# ----------------------
# Prepare models + fusion according to current selection
# ----------------------
# Node features
node_feat_dim = data.num_node_features
# Build a temp encoder to determine its output dim
temp_encoder = make_temporal_encoder(temporal_model, in_dim=2, hid=64) if temporal_model!="None" and fuse_toggle else None
temp_dim = temp_encoder.out_dim if temp_encoder is not None else None

gnn, film_module = build_gnn_with_optional_fusion(model_name, node_feat_dim, temp_emb_dim=temp_dim)

# load weights if available (keeps your behavior)
def load_model_weights(m, name):
    ckpt = {"GraphSAGE (baseline)": "models/elliptic_sage.pt", "GAT (influence)": "models/elliptic_gat.pt"}.get(name)
    if ckpt and os.path.exists(ckpt):
        try:
            m.load_state_dict(torch.load(ckpt, map_location='cpu'))
            st.success(f"Loaded weights from {ckpt}")
        except Exception as e:
            st.warning(f"Failed loading {ckpt}: {e}")
    else:
        st.info("No pre-trained GNN weights found; running with random init (demo mode).")

load_model_weights(gnn, model_name)

# Move modules to device
gnn = gnn.to(device)
if film_module is not None:
    film_module = film_module.to(device)
if temp_encoder is not None:
    temp_encoder = temp_encoder.to(device)

# ----------------------
# Build temporal embedding (global) - run encoder (no training)
# ----------------------
if fuse_toggle and temp_encoder is not None:
    # get synthetic or real series and run encoder
    series = get_global_series(seq_len).to(device)  # [1, T, F=2]
    with torch.no_grad():
        temp_emb = temp_encoder(series)   # [1, D] (Transformer returns [B,D], LSTM/GRU returns [B,D])
    temp_emb = temp_emb.squeeze(0).cpu()  # [D]
else:
    temp_emb = None

# ----------------------
# Run GNN inference using fused node features
# ----------------------
def run_inference(gnn_model, film_mod, temp_emb, return_attention=False):
    # node features as float tensor
    x = data.x.clone().float()  # [N, F_node]
    x_dev = x.to(device)
    if film_mod is not None and temp_emb is not None:
        # apply FiLM modulation to node features (global)
        # film expects node_x [N,F] and temp_emb [D]
        with torch.no_grad():
            x_mod = film_mod(x_dev, temp_emb.to(device))  # [N, F]
        x_dev = x_mod
    # run gnn - GAT can return attention if requested
    gnn_model.eval()
    with torch.no_grad():
        if return_attention and hasattr(gnn_model, 'forward') and 'return_attention' in inspect.signature(gnn_model.forward).parameters:
            logits, att = gnn_model(x_dev, data.edge_index, return_attention=True)
            probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()
            return probs, att
        else:
            logits = gnn_model(x_dev, data.edge_index)
            probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()
            return probs, None

probs, attns = run_inference(gnn, film_module, temp_emb, return_attention=(model_name=="GAT (influence)" and show_attn))

# ----------------------
# Build ranking DataFrame and UI elements (reuse much of your original layout)
# ----------------------
df_rank = pd.DataFrame({
    "node_id": np.arange(data.num_nodes),
    "prob_illicit": probs,
    "label": labels
})
df_rank["is_alert"] = df_rank["prob_illicit"] >= score_thr
df_rank = df_rank.sort_values("prob_illicit", ascending=False)

# header
c1, c2, c3, c4 = st.columns(4)
c1.metric("Nodes", f"{int(meta['num_nodes']):,}")
c2.metric("Edges", f"{int(meta['num_edges']):,}")
c3.metric("Illicit (labels)", f"{int((labels==1).sum()):,}")  # actually labels is 0/1 for illicit
c4.metric("Licit (labels)", f"{int((labels==0).sum()):,}")

# test metrics if test_mask exists
from sklearn.metrics import average_precision_score, roc_auc_score
test_idx = np.where(test_mask)[0]
if len(test_idx)>0:
    ap = average_precision_score(labels[test_idx], probs[test_idx])
    auc = roc_auc_score(labels[test_idx], probs[test_idx])
    st.write(f"**Test AUPRC:** {ap:.4f} &nbsp;&nbsp; **Test AUC:** {auc:.4f}")

# top list and node inspector
left, right = st.columns([2,1])
with left:
    st.subheader("Top suspicious nodes")
    st.dataframe(df_rank.head(k_top), width='stretch', height=420)
with right:
    st.subheader("Investigate a node")
    default_node = int(df_rank.head(k_top).iloc[0]["node_id"]) if len(df_rank)>0 else 0
    node_id = st.number_input("Node ID", min_value=0, max_value=int(data.num_nodes-1), value=default_node, step=1)
    st.write("Score:", float(probs[node_id]))
    st.write("Label (if known):", int(labels[node_id]))

# subgraph + attention visualization
st.subheader("Neighborhood subgraph")
subset, edge_index, mapping, edge_mask = k_hop_subgraph(node_id, khop, data.edge_index, relabel_nodes=True)
sub_nodes = pd.DataFrame({"node": subset.numpy()})
src = edge_index[0].numpy(); dst = edge_index[1].numpy()
edge_df = pd.DataFrame({"src": subset.numpy()[src], "dst": subset.numpy()[dst]})
edge_df["w"] = 1.0

# if attention available, compute widths
if attns is not None and show_attn:
    # attns is a tuple of attention weights per layer returned by GAT (see your GATNet)
    # We will use last layer attn if shapes line up
    try:
        _, att2 = attns  # attns = (attn1, attn2) where attn2 = (edge_index2, alpha2)
        alpha = att2[1].detach().cpu().numpy().flatten()
        if len(alpha)==len(edge_df):
            a = (alpha - alpha.min()) / (alpha.ptp() + 1e-9)
            edge_df["w"] = 1.0 + 7.0*a
    except Exception:
        pass

st.write("Nodes in subgraph:", len(sub_nodes), "Edges:", len(edge_df))
st.dataframe(edge_df.head(50), width='stretch', height=240)

seed = int(subset[mapping])
seed_score = float(probs[seed])
seed_lab = int(labels[seed])
st.markdown(f"**Seed node:** `{seed}` &nbsp; **Score:** {seed_score:.3f} &nbsp; **Label:** {seed_lab}")

# provide CSVs for download
sub_nodes.to_csv("subgraph_nodes.csv", index=False)
edge_df.to_csv("subgraph_edges.csv", index=False)
st.download_button("Download subgraph nodes CSV", data=open("subgraph_nodes.csv","rb"), file_name="subgraph_nodes.csv")
st.download_button("Download subgraph edges CSV", data=open("subgraph_edges.csv","rb"), file_name="subgraph_edges.csv")

# show temporal embedding info
if fuse_toggle and temp_emb is not None:
    st.sidebar.markdown("**Temporal Embedding**")
    st.sidebar.write("Temporal model:", temporal_model)
    st.sidebar.write("Embedding dim:", int(temp_emb.shape[0]))
    st.sidebar.write("First 8 dims:", np.round(temp_emb.numpy()[:8],3))
else:
    st.sidebar.write("No temporal fusion applied.")
