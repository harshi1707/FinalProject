{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ejl7dQZbwQ3L",
        "outputId": "171c0f4c-a3bb-4b45-effa-da73f12cd0d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m447.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Collecting jupyterlab\n",
            "  Downloading jupyterlab-4.4.6-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (0.28.1)\n",
            "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (6.17.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (5.8.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab)\n",
            "  Downloading jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (75.2.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (6.4.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from jupyterlab) (5.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab) (0.16.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab) (26.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.0.3->jupyterlab) (3.0.2)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (25.1.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel!=6.30.0,>=6.5.0->jupyterlab)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.10.4)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.22.1)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core->jupyterlab) (4.3.8)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (4.25.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.25.0->jupyterlab) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.25.0->jupyterlab) (4.15.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (25.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.27.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.21.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (2.5.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.8.5)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.0.0)\n",
            "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab) (24.11.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (2.22)\n",
            "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab)\n",
            "  Downloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading jupyterlab-4.4.6-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250822-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, lark, json5, jedi, fqdn, async-lru, rfc3987-syntax, jupyter-server-terminals, jupyter-client, arrow, isoduration, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires jupyter-server==1.16.0, but you have jupyter-server 2.17.0 which is incompatible.\n",
            "notebook 6.5.7 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\n",
            "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 async-lru-2.0.5 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.12.1 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.6 jupyterlab-server-2.27.3 lark-1.2.2 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rfc3987-syntax-1.1.0 types-python-dateutil-2.9.0.20250822 uri-template-1.3.0\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Collecting torch_geometric_temporal\n",
            "  Downloading torch_geometric_temporal-0.56.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric_temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torch_geometric_temporal) (2.8.0+cu126)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from torch_geometric_temporal) (3.0.12)\n",
            "Collecting torch-sparse (from torch_geometric_temporal)\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m814.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-scatter (from torch_geometric_temporal)\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch_geometric_temporal) (3.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (1.13.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch_geometric_temporal) (3.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse->torch_geometric_temporal) (1.16.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torch_geometric_temporal) (1.3.0)\n",
            "Downloading torch_geometric_temporal-0.56.2-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.3/102.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-scatter, torch-sparse\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl size=640889 sha256=02b2aa4e4bbdfe0f3139a6615c51b1d3df8775056a3b7b2d920a8209fd99575a\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/20/50/44800723f57cd798630e77b3ec83bc80bd26a1e3dc3a672ef5\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl size=1158903 sha256=9a71256a42f61c5dc1535d7ed056efaf5007e9b8880e75e528f94348ed80419e\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/fa/21/bd1d78ce1629aec4ecc924a63b82f6949dda484b6321eac6f2\n",
            "Successfully built torch-scatter torch-sparse\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch_geometric_temporal\n",
            "Successfully installed torch-scatter-2.1.2 torch-sparse-0.6.18 torch_geometric_temporal-0.56.2\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.36.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.0.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2.11.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from fastparquet) (2025.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n",
            "Downloading fastparquet-2024.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastparquet\n",
            "Successfully installed fastparquet-2024.11.0\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pyvis) (4.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, pyvis, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.3.0 pyvis-0.3.2 streamlit-1.49.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m644.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires jupyter-server==1.16.0, but you have jupyter-server 2.17.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m612.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_spline_conv (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
            "    _, build_failures = build(\n",
            "                        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
            "    wheel_file = _build_one(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/wheel_builder.py\", line 240, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen codecs>\", line 319, in decode\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1527, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1280, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 999, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 711, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 661, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 733, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 438, in _extract_from_extended_frame_gen\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.12/traceback.py\", line 323, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/linecache.py\", line 141, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 459, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 428, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/tokenize.py\", line 386, in read_or_stop\n",
            "    return readline()\n",
            "           ^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "unzip:  cannot find or open data/elliptic/elliptic-data-set.zip, data/elliptic/elliptic-data-set.zip.zip or data/elliptic/elliptic-data-set.zip.ZIP.\n",
            "ls: cannot access 'data/elliptic': No such file or directory\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "# example (CPU-safe)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# install PyTorch Geometric (PyG)\n",
        "!pip install torch_geometric\n",
        "\n",
        "!pip install pandas numpy scikit-learn matplotlib plotly networkx jupyterlab\n",
        "!pip install torch_geometric torch_geometric_temporal   # temporal helpers\n",
        "!pip install kaggle google-cloud-bigquery pyarrow fastparquet   # data I/O\n",
        "!pip install streamlit pyngrok pyvis  # demo/visualization\n",
        "\n",
        "# Colab cell 1: install kaggle and tools\n",
        "!pip install --upgrade --quiet kaggle pandas numpy matplotlib networkx scikit-learn\n",
        "# (you'll install PyTorch / PyG later — see A.3)\n",
        "\n",
        "!pip install --quiet torch torchvision torchaudio\n",
        "!pip install --quiet torch_geometric torch_sparse torch_scatter torch_cluster torch_spline_conv\n",
        "\n",
        "!pip install -q kaggle\n",
        "!kaggle datasets download -d ellipticco/elliptic-data-set -p data/elliptic\n",
        "!unzip -o data/elliptic/elliptic-data-set.zip -d data/elliptic\n",
        "!ls -lh data/elliptic\n",
        "\n",
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "xI6J3uzJLWd9",
        "outputId": "0cc1e43f-b366-45e5-f9ed-e6d951d2129a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30b0031b-0b96-4437-9b1c-584b7803c5b3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-30b0031b-0b96-4437-9b1c-584b7803c5b3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # choose kaggle.json from your computer\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJrB-aNQVXUx"
      },
      "source": [
        "# ==== One-shot setup cell for Colab (PyTorch Geometric + Elliptic Dataset) ====\n",
        "\n",
        "# 1. Install PyTorch (CPU build; use CUDA if you plan GPU training)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# 2. Install PyTorch Geometric (auto-handles extensions: torch_scatter, torch_sparse, etc.)\n",
        "!pip install torch-geometric\n",
        "\n",
        "# 3. Install common ML / Data packages\n",
        "!pip install pandas numpy scikit-learn matplotlib networkx plotly kaggle pyarrow fastparquet\n",
        "\n",
        "# 4. Install visualization/demo tools\n",
        "!pip install streamlit pyngrok pyvis\n",
        "\n",
        "# 5. Download Elliptic Dataset from Kaggle (50 MB)\n",
        "!kaggle datasets download -d ellipticco/elliptic-data-set -p data/elliptic\n",
        "!unzip -o data/elliptic/elliptic-data-set.zip -d data/elliptic\n",
        "!ls -lh data/elliptic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnXMEo0sNDMh",
        "outputId": "10e25dda-c38c-4eda-aa29-178da6353925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ellipticco/elliptic-data-set\n",
            "License(s): Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)\n",
            "Downloading elliptic-data-set.zip to data/elliptic\n",
            " 77% 112M/146M [00:00<00:00, 567MB/s] \n",
            "100% 146M/146M [00:00<00:00, 486MB/s]\n",
            "total 147M\n",
            "drwxr-xr-x 2 root root 4.0K Aug 31 14:35 elliptic_bitcoin_dataset\n",
            "-rw-r--r-- 1 root root 147M Jul 31  2019 elliptic-data-set.zip\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/elliptic\n",
        "!kaggle datasets download -d ellipticco/elliptic-data-set -p data/elliptic\n",
        "!unzip -q data/elliptic/elliptic-data-set.zip -d data/elliptic\n",
        "# listing files:\n",
        "!ls -lh data/elliptic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "3K5CCu4ENX5S",
        "outputId": "fa4d10d3-5f1c-48fb-b7ee-90d721365a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features shape: (203769, 167)\n",
            "edges shape: (234355, 2)\n",
            "classes shape: (203769, 2)\n",
            "class\n",
            "unknown    157205\n",
            "2           42019\n",
            "1            4545\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+IAAAFACAYAAAA4UcaHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASJtJREFUeJzt3X18zvX////7sfMZ2xjbjNmWk1gRiZnISctiKaUT73ijnERUkorKeRmdiEqkdyHpXToPERE+5awmJRFyGm28w+Z0wx7fP/x2/By2cczJYbhdL5fjUsfr+Xw9jufrcOx1HPfj+TpeL4eZmQAAAAAAgEd4XewBAAAAAABwJSGIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAADyKIAwBwFmJjY9W5c+eLPYxio3PnzoqNjb3YwwAA4JJAEAcAAG7ZuXOnhgwZolWrVl3soZw3I0aM0BdffHGxhwEAuMIQxAEAgFt27typoUOHFhjE3377bf3xxx+eH9Q5IogDAC4GgjgAAHA6ePDgWa3n6+srf3//8zwaAAAuTwRxAMBlY8iQIXI4HNq4caM6d+6s0NBQhYSE6IEHHtChQ4dc+h47dkzDhw9X5cqV5e/vr9jYWD3zzDPKzs526Wdmev7551WxYkWVKFFCzZo105o1awp8/H379qlPnz6Kjo6Wv7+/qlSpolGjRik3N9el34cffqi6deuqVKlSCg4OVs2aNTV27NjTbtuWLVvkcDj08ssv69VXX1VMTIwCAwPVpEkT/fbbb/n6r1u3TnfffbfKlCmjgIAA3XDDDfrqq69c+kyePFkOh0OLFi3Sww8/rPDwcFWsWLHAx1+4cKHq1asnSXrggQfkcDjkcDg0efJkSfl/I37yeMeNG6errrpKJUqUUIsWLbR9+3aZmYYPH66KFSsqMDBQd9xxh/bs2ZPvcWfPnq3GjRsrKChIpUqVUkpKSqHP/6k2bNigtm3bKjIyUgEBAapYsaLatWunzMxMSZLD4dDBgwc1ZcoU5/ac/Lv/HTt26MEHH1RERIT8/f11zTXX6N133833vDgcDn300Ud65plnFBkZqaCgIN1+++3avn27W+MEAFx5fC72AAAAON/uvfdexcXFKTU1VStXrtR//vMfhYeHa9SoUc4+Xbt21ZQpU3T33XfriSee0PLly5Wamqq1a9fq888/d/YbNGiQnn/+ebVq1UqtWrXSypUr1aJFC+Xk5Lg85qFDh9SkSRPt2LFDDz30kCpVqqQlS5ZowIAB+vvvvzVmzBhJ0rx58/Svf/1LN998s3M8a9eu1Q8//KDHHnvsjNv23nvvaf/+/erVq5eOHDmisWPHqnnz5lq9erUiIiIkSWvWrNGNN96oChUqqH///goKCtL06dPVpk0bffrpp7rzzjtdaj788MMqV66cBg0aVOiMeI0aNTRs2DANGjRI3bt3V+PGjSVJDRs2PO14p02bppycHD3yyCPas2ePXnzxRd17771q3ry5Fi5cqKefflobN27U66+/rn79+rkE3alTp6pTp05KTk7WqFGjdOjQIY0fP16NGjXSzz//fNqTw+Xk5Cg5OVnZ2dl65JFHFBkZqR07dmjmzJnat2+fQkJCNHXqVHXt2lX169dX9+7dJUmVK1eWJGVkZKhBgwZyOBzq3bu3ypUrp9mzZ6tLly7KyspSnz59XB7vhRdekMPh0NNPP61du3ZpzJgxSkpK0qpVqxQYGHja5wgAcAUyAAAuE4MHDzZJ9uCDD7osv/POOy0sLMx5f9WqVSbJunbt6tKvX79+JskWLFhgZma7du0yPz8/S0lJsdzcXGe/Z555xiRZp06dnMuGDx9uQUFBtn79epea/fv3N29vb9u2bZuZmT322GMWHBxsx44dK9K2bd682SRZYGCg/fXXX87ly5cvN0n2+OOPO5fdfPPNVrNmTTty5IhzWW5urjVs2NCqVq3qXDZp0iSTZI0aNXJrPD/++KNJskmTJuVr69Spk8XExOQbb7ly5Wzfvn3O5QMGDDBJdt1119nRo0edy//1r3+Zn5+fc8z79++30NBQ69atm8vjpKenW0hISL7lp/r5559Nkn388cen7RcUFOTy75inS5cuVr58efvf//7nsrxdu3YWEhJihw4dMjOz7777ziRZhQoVLCsry9lv+vTpJsnGjh172scHAFyZODQdAHDZ6dGjh8v9xo0b659//lFWVpYk6euvv5Yk9e3b16XfE088IUmaNWuWJOnbb791zuY6HA5nv1NnQyXp448/VuPGjVW6dGn973//c96SkpJ0/PhxLV68WJIUGhqqgwcPat68eWe1bW3atFGFChWc9+vXr6+EhATnNu3Zs0cLFizQvffeq/379zvH8c8//yg5OVkbNmzQjh07XGp269ZN3t7eZzWeM7nnnnsUEhLivJ+QkCBJ6tChg3x8fFyW5+TkOMc2b9487du3T//6179cnk9vb28lJCTou+++O+3j5j3mN998k+9nCWdiZvr000/VunVrmZnL4ycnJyszM1MrV650Wadjx44qVaqU8/7dd9+t8uXLO/9dAAA4GYemAwAuO5UqVXK5X7p0aUnS3r17FRwcrK1bt8rLy0tVqlRx6RcZGanQ0FBt3bpVkpz/rVq1qku/cuXKOWvm2bBhg3799VeVK1euwDHt2rVL0onDwKdPn66WLVuqQoUKatGihe69917deuutbm3bqWORpGrVqmn69OmSpI0bN8rMNHDgQA0cOLDQsZwc5uPi4tx67LNx6r9FXkCOjo4ucPnevXslnXg+Jal58+YF1g0ODpYkHT582Pmb7zyRkZGKi4tT3759NXr0aE2bNk2NGzfW7bffrg4dOrh8MVCQ3bt3a9++fZo4caImTpxYYJ+8f888p/67OBwOValSRVu2bDntYwEArkwEcQDAZaew2V0zc7l/8iz3ucrNzdUtt9yip556qsD2atWqSZLCw8O1atUqffPNN5o9e7Zmz56tSZMmqWPHjpoyZcp5GYck9evXT8nJyQX2OfULiAv5G+bC/i3O9G+Utx1Tp05VZGRkvn55s+kfffSRHnjggQJrvPLKK+rcubO+/PJLzZ07V48++qhSU1O1bNmyQk9Kd/Jjd+jQQZ06dSqwT61atQpdHwCAMyGIAwCuODExMcrNzdWGDRtUo0YN5/KMjAzt27dPMTExzn7SidnZq666ytlv9+7dzpnbPJUrV9aBAweUlJR0xsf38/NT69at1bp1a+Xm5urhhx/WW2+9pYEDB+YLyafKmyk+2fr1650nLssbp6+vr1tjKYrz+cXFmeSdNC08PPy025GcnHzaw/xr1qypmjVr6rnnntOSJUt04403asKECXr++eclFbxN5cqVU6lSpXT8+HG3n8NT/13MTBs3biSwAwAKxG/EAQBXnFatWkmS80zmeUaPHi1JSklJkSQlJSXJ19dXr7/+usts+qnrSSfO1L506VJ98803+dr27dunY8eOSZL++ecflzYvLy9nWDv10mkF+eKLL1x+471ixQotX75cLVu2lHQiuDZt2lRvvfWW/v7773zr7969+4yPUZigoCBJJ7bnQktOTlZwcLBGjBiho0eP5mvP247y5csrKSnJ5SZJWVlZzuc8T82aNeXl5eXyPAcFBeXbHm9vb7Vt21affvppgZeGK+g5zDubfZ5PPvlEf//9t/PfBQCAkzEjDgC44lx33XXq1KmTJk6cqH379qlJkyZasWKFpkyZojZt2qhZs2aSTsyM9uvXT6mpqbrtttvUqlUr/fzzz5o9e7bKli3rUvPJJ5/UV199pdtuu02dO3dW3bp1dfDgQa1evVqffPKJtmzZorJly6pr167as2ePmjdvrooVK2rr1q16/fXXVbt2bZfZ+cJUqVJFjRo1Us+ePZWdna0xY8YoLCzM5ZD4cePGqVGjRqpZs6a6deumq666ShkZGVq6dKn++usv/fLLL2f1vFWuXFmhoaGaMGGCSpUqpaCgICUkJFyQ35gHBwdr/Pjx+ve//63rr79e7dq1U7ly5bRt2zbNmjVLN954o954441C11+wYIF69+6te+65R9WqVdOxY8c0depUZ8jOU7duXX377bcaPXq0oqKiFBcXp4SEBI0cOVLfffedEhIS1K1bN8XHx2vPnj1auXKlvv3223zXPC9TpowaNWqkBx54QBkZGRozZoyqVKmibt26nffnBgBw6SOIAwCuSP/5z3901VVXafLkyfr8888VGRmpAQMGaPDgwS79nn/+eQUEBGjChAnOYDZ37lznrHmeEiVKaNGiRRoxYoQ+/vhjvffeewoODla1atU0dOhQ5wnCOnTooIkTJ+rNN9/Uvn37FBkZqfvuu09DhgyRl9eZD1Tr2LGjvLy8NGbMGO3atUv169fXG2+8ofLlyzv7xMfH66efftLQoUM1efJk/fPPPwoPD1edOnU0aNCgs37OfH19NWXKFA0YMEA9evTQsWPHNGnSpAt2srf7779fUVFRGjlypF566SVlZ2erQoUKaty4cb7fhZ/quuuuU3JysmbMmKEdO3aoRIkSuu666zR79mw1aNDA2W/06NHq3r27nnvuOR0+fFidOnVSQkKCIiIitGLFCg0bNkyfffaZ3nzzTYWFhemaa65xuR59nmeeeUa//vqrUlNTtX//ft1888168803VaJEifP+vAAALn0OO/XMNQAAoNjZsmWL4uLi9NJLL6lfv34Xezj4/yxcuFDNmjXTxx9/rLvvvvtiDwcAcIngN+IAAAAAAHgQQRwAAAAAAA8iiAMAAAAA4EH8RhwAAAAAAA9iRhwAAAAAAA8iiAMAAAAA4EGX7XXEc3NztXPnTpUqVUoOh+NiDwcAAAAAcJkzM+3fv19RUVHy8ip83vuyDeI7d+5UdHT0xR4GAAAAAOAKs337dlWsWLHQ9ss2iJcqVUrSiScgODj4Io8GAAAAAHC5y8rKUnR0tDOPFuayDeJ5h6MHBwcTxAEAAAAAHnOmn0cX6WRtx48f18CBAxUXF6fAwEBVrlxZw4cP18lXQDMzDRo0SOXLl1dgYKCSkpK0YcMGlzp79uxR+/btFRwcrNDQUHXp0kUHDhxw6fPrr7+qcePGCggIUHR0tF588cWiDBUAAAAAgGKpSEF81KhRGj9+vN544w2tXbtWo0aN0osvvqjXX3/d2efFF1/Ua6+9pgkTJmj58uUKCgpScnKyjhw54uzTvn17rVmzRvPmzdPMmTO1ePFide/e3dmelZWlFi1aKCYmRmlpaXrppZc0ZMgQTZw48TxsMgAAAAAAF4/DTp7OPoPbbrtNEREReuedd5zL2rZtq8DAQL3//vsyM0VFRemJJ55Qv379JEmZmZmKiIjQ5MmT1a5dO61du1bx8fH68ccfdcMNN0iS5syZo1atWumvv/5SVFSUxo8fr2effVbp6eny8/OTJPXv319ffPGF1q1b59ZYs7KyFBISoszMTA5NBwAAAABccO7m0CLNiDds2FDz58/X+vXrJUm//PKLvv/+e7Vs2VKStHnzZqWnpyspKcm5TkhIiBISErR06VJJ0tKlSxUaGuoM4ZKUlJQkLy8vLV++3NnnpptucoZwSUpOTtYff/yhvXv3Fji27OxsZWVludwAAAAAAChuinSytv79+ysrK0vVq1eXt7e3jh8/rhdeeEHt27eXJKWnp0uSIiIiXNaLiIhwtqWnpys8PNx1ED4+KlOmjEufuLi4fDXy2kqXLp1vbKmpqRo6dGhRNgcAAAAAAI8r0oz49OnTNW3aNH3wwQdauXKlpkyZopdffllTpky5UONz24ABA5SZmem8bd++/WIPCQAAAACAfIo0I/7kk0+qf//+ateunSSpZs2a2rp1q1JTU9WpUydFRkZKkjIyMlS+fHnnehkZGapdu7YkKTIyUrt27XKpe+zYMe3Zs8e5fmRkpDIyMlz65N3P63Mqf39/+fv7F2VzAAAAAADwuCLNiB86dEheXq6reHt7Kzc3V5IUFxenyMhIzZ8/39melZWl5cuXKzExUZKUmJioffv2KS0tzdlnwYIFys3NVUJCgrPP4sWLdfToUWefefPm6eqrry7wsHQAAAAAAC4VRZoRb926tV544QVVqlRJ11xzjX7++WeNHj1aDz74oKQTFy3v06ePnn/+eVWtWlVxcXEaOHCgoqKi1KZNG0lSjRo1dOutt6pbt26aMGGCjh49qt69e6tdu3aKioqSJN1///0aOnSounTpoqefflq//fabxo4dq1dfffWcNzi2/6wz9tkyMuWcHwcAAAAAgIIUKYi//vrrGjhwoB5++GHt2rVLUVFReuihhzRo0CBnn6eeekoHDx5U9+7dtW/fPjVq1Ehz5sxRQECAs8+0adPUu3dv3XzzzfLy8lLbtm312muvOdtDQkI0d+5c9erVS3Xr1lXZsmU1aNAgl2uNAwAAAABwKSrSdcQvJYVdv40ZcQAAAADAhXBBriMOAAAAAADODUEcAAAAAAAPIogDAAAAAOBBBHEAAAAAADyIIA4AAAAAgAcRxAEAAAAA8CCCOAAAAAAAHkQQBwAAAADAgwjiAAAAAAB4EEEcAAAAAAAPIogDAAAAAOBBBHEAAAAAADyIIA4AAAAAgAcRxAEAAAAA8CCCOAAAAAAAHkQQBwAAAADAgwjiAAAAAAB4kM/FHgAAAAAAXGyx/Wedsc+WkSkeGAmuBMyIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAADyKIAwAAAADgQT4XewAo3mL7zzpjny0jUzwwEgAAAAC4PDAjDgAAAACABzEjDhRD7hyJIHE0AgAAAHApYkYcAAAAAAAPIogDAAAAAOBBHJpeTHBSNAAAAAC4MjAjDgAAAACABxHEAQAAAADwIII4AAAAAAAeRBAHAAAAAMCDCOIAAAAAAHgQZ00HAAAAzgFXvwFQVMyIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIOKHMR37NihDh06KCwsTIGBgapZs6Z++uknZ7uZadCgQSpfvrwCAwOVlJSkDRs2uNTYs2eP2rdvr+DgYIWGhqpLly46cOCAS59ff/1VjRs3VkBAgKKjo/Xiiy+e5SYCAAAAAFB8FCmI7927VzfeeKN8fX01e/Zs/f7773rllVdUunRpZ58XX3xRr732miZMmKDly5crKChIycnJOnLkiLNP+/bttWbNGs2bN08zZ87U4sWL1b17d2d7VlaWWrRooZiYGKWlpemll17SkCFDNHHixPOwyQAAAAAAXDw+Rek8atQoRUdHa9KkSc5lcXFxzv83M40ZM0bPPfec7rjjDknSe++9p4iICH3xxRdq166d1q5dqzlz5ujHH3/UDTfcIEl6/fXX1apVK7388suKiorStGnTlJOTo3fffVd+fn665pprtGrVKo0ePdolsAMAAAAAcKkpUhD/6quvlJycrHvuuUeLFi1ShQoV9PDDD6tbt26SpM2bNys9PV1JSUnOdUJCQpSQkKClS5eqXbt2Wrp0qUJDQ50hXJKSkpLk5eWl5cuX684779TSpUt10003yc/Pz9knOTlZo0aN0t69e11m4PNkZ2crOzvbeT8rK6som4ZLSGz/WW712zIy5QKPBAAAAACKrkiHpm/atEnjx49X1apV9c0336hnz5569NFHNWXKFElSenq6JCkiIsJlvYiICGdbenq6wsPDXdp9fHxUpkwZlz4F1Tj5MU6VmpqqkJAQ5y06OroomwYAAAAAgEcUKYjn5ubq+uuv14gRI1SnTh11795d3bp104QJEy7U+Nw2YMAAZWZmOm/bt2+/2EMCAAAAACCfIgXx8uXLKz4+3mVZjRo1tG3bNklSZGSkJCkjI8OlT0ZGhrMtMjJSu3btcmk/duyY9uzZ49KnoBonP8ap/P39FRwc7HIDAAAAAKC4KdJvxG+88Ub98ccfLsvWr1+vmJgYSSdO3BYZGan58+erdu3akk78Vnv58uXq2bOnJCkxMVH79u1TWlqa6tatK0lasGCBcnNzlZCQ4Ozz7LPP6ujRo/L19ZUkzZs3T1dffXWBvw8HAOBC4bwUAADgfCvSjPjjjz+uZcuWacSIEdq4caM++OADTZw4Ub169ZIkORwO9enTR88//7y++uorrV69Wh07dlRUVJTatGkj6cQM+q233qpu3bppxYoV+uGHH9S7d2+1a9dOUVFRkqT7779ffn5+6tKli9asWaOPPvpIY8eOVd++fc/v1gMAAAAA4GFFmhGvV6+ePv/8cw0YMEDDhg1TXFycxowZo/bt2zv7PPXUUzp48KC6d++uffv2qVGjRpozZ44CAgKcfaZNm6bevXvr5ptvlpeXl9q2bavXXnvN2R4SEqK5c+eqV69eqlu3rsqWLatBgwZx6TIAAAAAwCWvSEFckm677TbddttthbY7HA4NGzZMw4YNK7RPmTJl9MEHH5z2cWrVqqX/+7//K+rwAAAAAAAo1op0aDoAAAAAADg3BHEAAAAAADyoyIemo/hz5wy/nN0XAAAAAC4OZsQBAAAAAPAggjgAAAAAAB7EoekAAAAAcAXgJ6zFBzPiAAAAAAB4EDPiAAAAuOIwMwjgYmJGHAAAAAAADyKIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHcbI2AAAAAJckTrqHSxUz4gAAAAAAeBBBHAAAAAAADyKIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHcdZ0AAAAAMBFcyWe/Z4gDgAAAADn0ZUYLFE0HJoOAAAAAIAHMSMOnCfufPMp8e0nAAAAcKUjiAMAAABAMcVh7pcnDk0HAAAAAMCDCOIAAAAAAHgQQRwAAAAAAA8iiAMAAAAA4EEEcQAAAAAAPIggDgAAAACABxHEAQAAAADwIII4AAAAAAAeRBAHAAAAAMCDCOIAAAAAAHiQz8UeAAAAKLrY/rPc6rdlZMoFHgkAACgqgvg5cOdDEB+AgIIRIgCg+OIzDgBcWByaDgAAAACABzEjjisas7IAAACexREXADPiAAAAAAB4FEEcAAAAAAAPIogDAAAAAOBB/EYcHsFvsQEAAADgBII4cJnjSxBcKnit4lLBaxUAcK44NB0AAAAAAA8iiAMAAAAA4EEEcQAAAAAAPOicgvjIkSPlcDjUp08f57IjR46oV69eCgsLU8mSJdW2bVtlZGS4rLdt2zalpKSoRIkSCg8P15NPPqljx4659Fm4cKGuv/56+fv7q0qVKpo8efK5DBUAgIsutv8st24AAODydtZB/Mcff9Rbb72lWrVquSx//PHHNWPGDH388cdatGiRdu7cqbvuusvZfvz4caWkpCgnJ0dLlizRlClTNHnyZA0aNMjZZ/PmzUpJSVGzZs20atUq9enTR127dtU333xztsMFAAAAAKBYOKsgfuDAAbVv315vv/22Spcu7VyemZmpd955R6NHj1bz5s1Vt25dTZo0SUuWLNGyZcskSXPnztXvv/+u999/X7Vr11bLli01fPhwjRs3Tjk5OZKkCRMmKC4uTq+88opq1Kih3r176+6779arr756HjYZAAAAAICL56yCeK9evZSSkqKkpCSX5WlpaTp69KjL8urVq6tSpUpaunSpJGnp0qWqWbOmIiIinH2Sk5OVlZWlNWvWOPucWjs5OdlZoyDZ2dnKyspyuQEAAAAAUNwU+TriH374oVauXKkff/wxX1t6err8/PwUGhrqsjwiIkLp6enOPieH8Lz2vLbT9cnKytLhw4cVGBiY77FTU1M1dOjQom4OAAAAAAAeVaQZ8e3bt+uxxx7TtGnTFBAQcKHGdFYGDBigzMxM52379u0Xe0gAAAAAAORTpCCelpamXbt26frrr5ePj498fHy0aNEivfbaa/Lx8VFERIRycnK0b98+l/UyMjIUGRkpSYqMjMx3FvW8+2fqExwcXOBsuCT5+/srODjY5QYAAAAAQHFTpCB+8803a/Xq1Vq1apXzdsMNN6h9+/bO//f19dX8+fOd6/zxxx/atm2bEhMTJUmJiYlavXq1du3a5ewzb948BQcHKz4+3tnn5Bp5ffJqAAAAAABwqSrSb8RLlSqla6+91mVZUFCQwsLCnMu7dOmivn37qkyZMgoODtYjjzyixMRENWjQQJLUokULxcfH69///rdefPFFpaen67nnnlOvXr3k7+8vSerRo4feeOMNPfXUU3rwwQe1YMECTZ8+XbNmcW1VAAAAAMClrcgnazuTV199VV5eXmrbtq2ys7OVnJysN99809nu7e2tmTNnqmfPnkpMTFRQUJA6deqkYcOGOfvExcVp1qxZevzxxzV27FhVrFhR//nPf5ScnHy+hwsAAAAAgEedcxBfuHChy/2AgACNGzdO48aNK3SdmJgYff3116et27RpU/3888/nOjwAAAAAAIqVs7qOOAAAAAAAODsEcQAAAAAAPIggDgAAAACAB533k7UBwKUstr97V2fYMjLlAo8EAAAAlytmxAEAAAAA8CCCOAAAAAAAHsSh6QCAyw4/MQAAAMUZQRwAcNYIvAAAXJnc+QzA+3/hODQdAAAAAAAPIogDAAAAAOBBHJoOAJcADgEHAAC4fDAjDgAAAACABxHEAQAAAADwIA5NBwAAAIoJzkQNXBmYEQcAAAAAwIMI4gAAAAAAeBBBHAAAAAAAD+I34gAAALhg+M0zAOTHjDgAAAAAAB5EEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAAD+JkbQBwgbhzgiKJkxQBAACcL5fKCSKZEQcAAAAAwIOYEQcAAAAuQ5fKzCBwJWJGHAAAAAAAD2JGHAAA4DLA7CcAXDqYEQcAAAAAwIOYEQcAAABwWhxxAZxfzIgDAAAAAOBBBHEAAAAAADyIIA4AAAAAgAcRxAEAAAAA8CCCOAAAAAAAHsRZ0wEAwHnjzpmVJc6uDAC4shHEAQAAcEngEloALhcEcQAAgIuEYAkAxdeF3EfzG3EAAAAAADyIIA4AAAAAgAcRxAEAAAAA8CB+Iw4AwBWOM50DAOBZzIgDAAAAAOBBBHEAAAAAADyIIA4AAAAAgAcRxAEAAAAA8KAiBfHU1FTVq1dPpUqVUnh4uNq0aaM//vjDpc+RI0fUq1cvhYWFqWTJkmrbtq0yMjJc+mzbtk0pKSkqUaKEwsPD9eSTT+rYsWMufRYuXKjrr79e/v7+qlKliiZPnnx2WwgAAAAAQDFSpCC+aNEi9erVS8uWLdO8efN09OhRtWjRQgcPHnT2efzxxzVjxgx9/PHHWrRokXbu3Km77rrL2X78+HGlpKQoJydHS5Ys0ZQpUzR58mQNGjTI2Wfz5s1KSUlRs2bNtGrVKvXp00ddu3bVN998cx42GQAAAACAi6dIly+bM2eOy/3JkycrPDxcaWlpuummm5SZmal33nlHH3zwgZo3by5JmjRpkmrUqKFly5apQYMGmjt3rn7//Xd9++23ioiIUO3atTV8+HA9/fTTGjJkiPz8/DRhwgTFxcXplVdekSTVqFFD33//vV599VUlJyefp00HAAAAAMDzzuk64pmZmZKkMmXKSJLS0tJ09OhRJSUlOftUr15dlSpV0tKlS9WgQQMtXbpUNWvWVEREhLNPcnKyevbsqTVr1qhOnTpaunSpS428Pn369Cl0LNnZ2crOznbez8rKOpdNA1CA4nqt4eI6LgAAAKAgZ32yttzcXPXp00c33nijrr32WklSenq6/Pz8FBoa6tI3IiJC6enpzj4nh/C89ry20/XJysrS4cOHCxxPamqqQkJCnLfo6Oiz3TQAAAAAAC6Ys54R79Wrl3777Td9//3353M8Z23AgAHq27ev835WVhZhHAAKwBEEAAAAF9dZBfHevXtr5syZWrx4sSpWrOhcHhkZqZycHO3bt89lVjwjI0ORkZHOPitWrHCpl3dW9ZP7nHqm9YyMDAUHByswMLDAMfn7+8vf3/9sNgcAAAAAAI8p0qHpZqbevXvr888/14IFCxQXF+fSXrduXfn6+mr+/PnOZX/88Ye2bdumxMRESVJiYqJWr16tXbt2OfvMmzdPwcHBio+Pd/Y5uUZen7waAAAAAABcqoo0I96rVy998MEH+vLLL1WqVCnnb7pDQkIUGBiokJAQdenSRX379lWZMmUUHBysRx55RImJiWrQoIEkqUWLFoqPj9e///1vvfjii0pPT9dzzz2nXr16OWe0e/TooTfeeENPPfWUHnzwQS1YsEDTp0/XrFnuHU4JAAAAAEBxVaQZ8fHjxyszM1NNmzZV+fLlnbePPvrI2efVV1/VbbfdprZt2+qmm25SZGSkPvvsM2e7t7e3Zs6cKW9vbyUmJqpDhw7q2LGjhg0b5uwTFxenWbNmad68ebruuuv0yiuv6D//+Q+XLgMAAAAAXPKKNCNuZmfsExAQoHHjxmncuHGF9omJidHXX3992jpNmzbVzz//XJThAQAAAABQ7J315csAAAAAAEDREcQBAAAAAPAggjgAAAAAAB50VtcRBwAAuJTE9nfvyitbRqZc4JEAAMCMOAAAAAAAHkUQBwAAAADAgwjiAAAAAAB4EEEcAAAAAAAPIogDAAAAAOBBBHEAAAAAADyIIA4AAAAAgAcRxAEAAAAA8CCCOAAAAAAAHkQQBwAAAADAgwjiAAAAAAB4EEEcAAAAAAAPIogDAAAAAOBBBHEAAAAAADzI52IPAAAAoCCx/We51W/LyJQLPBIAAM4vZsQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAADyKIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAADyKIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAADyKIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5UrIP4uHHjFBsbq4CAACUkJGjFihUXe0gAAAAAAJyTYhvEP/roI/Xt21eDBw/WypUrdd111yk5OVm7du262EMDAAAAAOCsFdsgPnr0aHXr1k0PPPCA4uPjNWHCBJUoUULvvvvuxR4aAAAAAABnzediD6AgOTk5SktL04ABA5zLvLy8lJSUpKVLlxa4TnZ2trKzs533MzMzJUlZWVku/XKzD53x8U9dpzDUcr+WO3WoRS1qUYta1KLW2dWhFrWoRS1qFY9aeffN7LTrOexMPS6CnTt3qkKFClqyZIkSExOdy5966iktWrRIy5cvz7fOkCFDNHToUE8OEwAAAACAfLZv366KFSsW2l4sZ8TPxoABA9S3b1/n/dzcXO3Zs0dhYWFyOBwFrpOVlaXo6Ght375dwcHB5/T41KIWtahFLWpRi1rUoha1qHUpjIlaF66WmWn//v2Kioo6bb1iGcTLli0rb29vZWRkuCzPyMhQZGRkgev4+/vL39/fZVloaKhbjxccHHzO/zDUoha1qEUtalGLWtSiFrWodaHrUKv41woJCTljnWJ5sjY/Pz/VrVtX8+fPdy7Lzc3V/PnzXQ5VBwAAAADgUlMsZ8QlqW/fvurUqZNuuOEG1a9fX2PGjNHBgwf1wAMPXOyhAQAAAABw1optEL/vvvu0e/duDRo0SOnp6apdu7bmzJmjiIiI8/YY/v7+Gjx4cL5D2qlFLWpRi1rUoha1qEUtalHrfNQqjmOi1sWtJRXTs6YDAAAAAHC5Kpa/EQcAAAAA4HJFEAcAAAAAwIMI4gAAAAAAeBBBHAAAAAAADyKIXwE4Hx8AAAAAFB/F9vJlOH/8/f31yy+/qEaNGhd7KJe1v//+W+PHj9f333+vv//+W15eXrrqqqvUpk0bde7cWd7e3hd7iAAAAACKAYL4SbZv367Bgwfr3XffPWPfw4cPKy0tTWXKlFF8fLxL25EjRzR9+nR17NjRrcddu3atli1bpsTERFWvXl3r1q3T2LFjlZ2drQ4dOqh58+Zu1enbt2+By48fP66RI0cqLCxMkjR69Gi36p3s4MGDmj59ujZu3Kjy5cvrX//6l7PemaxcuVKlS5dWXFycJGnq1KmaMGGCtm3bppiYGPXu3Vvt2rVzq9Yjjzyie++9V40bNy7yNhTkjTfe0IoVK9SqVSu1a9dOU6dOVWpqqnJzc3XXXXdp2LBh8vE585/JTz/9pKSkJFWpUkWBgYHasGGD7r//fuXk5Khfv3569913NWfOHJUqVeq8jBu4XK1YsUJLly5Venq6JCkyMlKJiYmqX7/+eXuMvXv3asaMGW7voyUpNzdXXl75DyLLzc3VX3/9pUqVKrlVx8y0ZcsWRUdHy8fHRzk5Ofr888+VnZ2tVq1aqWzZsm6PqSDNmzfXpEmTFBMTc051Nm/e7NzfX3vttW6vl52dLS8vL/n6+kqS/vzzT7377rvO/X2XLl2c7wVn8umnn6ply5YqUaLEWW3DqX755RelpaWpadOmuuqqq7RmzRqNGzdOubm5uvPOO5WcnFykegsWLMj3xevtt9+uqlWrnpfxApcz9vXs6/Nc0ft6g9OqVavMy8vrjP3++OMPi4mJMYfDYV5eXnbTTTfZzp07ne3p6elu1TEzmz17tvn5+VmZMmUsICDAZs+ebeXKlbOkpCRr3ry5eXt72/z5892q5XA4rHbt2ta0aVOXm8PhsHr16lnTpk2tWbNmbtWqUaOG/fPPP2Zmtm3bNouNjbWQkBCrV6+elSlTxsLDw23Tpk1u1apVq5bNmzfPzMzefvttCwwMtEcffdTGjx9vffr0sZIlS9o777zj9jZ6eXlZ1apVbeTIkfb333+7tV5Bhg8fbqVKlbK2bdtaZGSkjRw50sLCwuz555+3ESNGWLly5WzQoEFu1brxxhttyJAhzvtTp061hIQEMzPbs2eP1a5d2x599NEijS87O9s++ugj69Onj7Vr187atWtnffr0senTp1t2dnaRap1Oenq6DR06tEjrbN++3fbv359veU5Oji1atMjtOv/73/9swYIFztfa7t27beTIkTZ06FD7/fffizSmgsTFxdn69evPqUZubq4tWLDAJk6caDNmzLCcnBy3192+fbvt3r3beX/x4sV2//33W6NGjax9+/a2ZMkSt2u9/PLLtmXLliKN/XRmzJhhAwcOtO+//97MzObPn28tW7a05ORke+utt4pU69ChQ/bOO+/YAw88YLfeequ1atXKevfubd9++63bNTIyMqxRo0bmcDgsJibG6tevb/Xr13fuaxs1amQZGRlFGldh3N3Xm5llZmbaPffcYwEBARYeHm4DBw60Y8eOOduLsr9ft26dxcTEmJeXl1WpUsU2bdpkdevWtaCgICtRooSVLVvW7dfrl19+WeDN29vb3njjDed9d/Ts2dP593zo0CFr27ateXl5Ofe3zZo1K/DvvSBNmjSxjz/+2MzMvv/+e/P397datWrZfffdZ3Xq1LESJUq4/bp3OBwWHBxs3bp1s2XLlrm1TmE+/fRT8/b2trCwMCtZsqTNmzfPQkNDLSkpyZKTk83b29umTZvmVq2MjAyrX7++eXl5mY+Pj3l5eVndunUtMjLSvL297cknnyzy+JYvX25jxoyx/v37W//+/W3MmDG2fPnyItc5nT179tiUKVOKtM7x48cLXb5161a36+Tm5tqmTZvs6NGjZnbi/e3DDz+0KVOmuOwjz1azZs3Oy/5x06ZNNnfuXFu9enWR1jty5IjLe8PGjRvtmWeesQ4dOtizzz7r9uclM7NPPvnEDh48WKTHP51Vq1bZO++8Y3/++aeZmf3222/Ws2dPe+ihh2zOnDlFrjd//nwbOnSo9ejRwx5++GF7+eWXi/Q+y76eff2prqR9/amuqCBe2Is57/bqq6+69UfWpk0bS0lJsd27d9uGDRssJSXF4uLinG9KRfljTUxMtGeffdbMzP773/9a6dKl7ZlnnnG29+/f32655Ra3aqWmplpcXFy+4O7j42Nr1qxxq0Yeh8Ph3BG2b9/eGjZsaPv27TMzs/3791tSUpL961//cqtWYGCg8w2yTp06NnHiRJf2adOmWXx8vNvj+vbbb+2xxx6zsmXLmq+vr91+++02Y8aMQj8wFKZy5cr26aefmtmJnbW3t7e9//77zvbPPvvMqlSp4latwMBA55uc2YkPKb6+vpaenm5mZnPnzrWoqCi3x7Zhwwa76qqrLCAgwJo0aWL33nuv3XvvvdakSRMLCAiwKlWq2IYNG9yudzpFeaPauXOn1atXz7y8vMzb29v+/e9/u+y0i/LaX758uYWEhJjD4bDSpUvbTz/9ZHFxcVa1alWrXLmyBQYGWlpamlu1xo4dW+DN29vbBgwY4LzvjpYtWzpf6//8848lJCSYw+GwcuXKmZeXl1WvXt127drlVq369evbjBkzzMzsiy++MC8vL7v99tvt6aeftjvvvNN8fX2d7WficDjM29vbkpKS7MMPPzynL2MmTJhgPj4+VrduXQsODrapU6daqVKlrGvXrvbQQw9ZYGCgjRkzxq1aGzZssJiYGAsPD7fo6GhzOByWkpJiCQkJ5u3tbffcc4/zw/fptG3b1hITE23dunX52tatW2cNGza0u+++260xZWZmnvb2f//3f26/Th999FGrVq2affzxx/b2229bTEyMpaSkOJ//9PR0czgcbtW644477Pbbb7dff/3V+vTpYzVq1LA77rjDcnJy7MiRI9a6dWvr0KGDW7XyPjg5HI5Cb+5uo5eXl3N/P2DAAKtYsaItWLDADh48aN9//71VrlzZ+vfv71at4OBg5wfMJk2a2OOPP+7S/txzz9mNN97o9jYOGzbM6tSpYw6Hw6655hp79dVX7X//+59b65/s+uuvt+eff97MTrzXhoaG2rBhw5ztL7/8stWuXdutWvfdd5+1adPGMjMz7ciRI9a7d2/r2LGjmZ0IKWFhYW7//RBKCCWnutxDCft69vUFbePlvq8vzBUVxM/Xizk8PNx+/fVX5/3c3Fzr0aOHVapUyf78888ivUEFBwc7A9Xx48fNx8fHVq5c6WxfvXq1RUREuL2NK1assGrVqtkTTzzh/Hb2XIP4VVddZXPnznVp/+GHHyw6OtqtWmFhYfbTTz+Z2YnnbtWqVS7tGzdutMDAwCKPKycnxz766CPnm0lUVJQ988wzbgfUwMBAl2/0fX197bfffnPe37Jli5UoUcKtWjExMc6ZRbMTgdXhcNihQ4fMzGzz5s0WEBDgVi0zs6SkJLvjjjssMzMzX1tmZqbdcccd1qJFC7dq/fLLL6e9ffTRR26/Xjt27GgJCQn2448/2rx586xu3bp2ww032J49e8ysaG9USUlJ1rVrV8vKyrKXXnrJKlasaF27dnW2P/DAA9amTRu3ajkcDqtYsaLFxsa63BwOh1WoUMFiY2MtLi7O7Vp5r7GePXtafHy8czZj+/btVrduXevRo4dbtYKCgpzrJiQk2MiRI13aX3/9datTp47b45o0aZLdcccd5uvra2FhYfbYY48VeebGzCw+Pt75hdiCBQssICDAxo0b52yfNGmS1ahRw61aLVu2tIceeshyc3PNzGzkyJHWsmVLMzNbv369xcbG2uDBg89Yp2TJki77vlP99NNPVrJkSbfGlLcvL+xWlA8ulSpVsu+++855f/fu3Va/fn1r0aKFHTlypEj7+3LlytnPP/9sZmYHDhwwh8Nh//d//+ds/+GHH6xSpUpu1br11lstJSUlX0g71/39tddeax988IFL+5dffmnVqlVzq1ZQUJCtXbvWzMwiIiIK3N8X5d8xb1w//fST9ezZ00JDQ83f39/uueeefO9LZxrX5s2bzezEe7avr6/L+/iff/7p9riCg4Nd3isOHDhgvr6+zv311KlT7eqrr3arFqGEUFLQNl7OoYR9Pfv6043rct3XF+aKCuJRUVH2xRdfFNr+888/u/VHVqpUqQIPm+3Vq5dVrFjRFi9eXKQgvnHjRuf9kiVLusysbtmypUgBzuzEjHXHjh2tVq1atnr1avP19T2rP9a8Wb+oqKh8H/aLMq4OHTpYly5dzMzsnnvuseeee86lfcSIEVazZk23x1XQ7MDWrVtt8ODBzm/d3REXF2ezZ882sxOBwcvLy6ZPn+5snzVrlsXGxrpV67HHHrNrr73WZs+ebQsWLLBmzZpZ06ZNne1z5syxypUru1XL7MSXBKcLWL/++muRvrwo7INLUd+ooqKiXA6XzPsQVbt2bfvnn3+K9EZVunRp599RTk6OeXl5udROS0uzChUquFXroYcestq1a+f7uzzXN6qrr7463+zKt99+63aoDwkJsV9++cXMTnwJlff/eTZu3Oj2lz0njysjI8NGjRpl1atXNy8vL6tXr55NnDjRsrKy3KpV0JdQJ7/eNm/e7Pa4SpQo4TKblZ2dbb6+vs4PjV988YVbf0dhYWG2cOHCQtu/++47CwsLc2tMwcHBNmrUKFu4cGGBt7ffftvt12lgYGC+w0qzsrIsMTHRmjdvbps2bSpSrZOf95IlS7rs/7dt22b+/v5u1TIzGz16tEVHR7scVXG2r/m8/X3ZsmVdPniYndjfu7u/ad68ub344otmZtawYcN8h0J/8sknbn8ALWh/f/jwYXvvvfesadOm5uXl5fY+OjIy0vmF8J49e8zhcLh86F6xYoVFRka6VatcuXIuz/GhQ4fMy8vL+RObP//80+1/R0IJoeR047ocQwn7evb1BY3rct/XF+aKCuKtW7e2gQMHFtq+atUqt77hrVevnr333nsFtvXq1ctCQ0Pd/mOtVauWMwyanZgBP/kwzsWLF7v9of9U//3vfy0iIsK8vLzO6o+1Zs2aVqdOHStZsqR98sknLu2LFi1yOyTt2LHDYmNj7aabbrK+fftaYGCgNWrUyLp162Y33XST+fn52axZs9we1+kO08vNzXX7Teq5556zcuXKWdeuXS0uLs769+9vlSpVsvHjx9uECRMsOjo637fahdm/f7/de++95uPjYw6Hwxo2bOiyQ//mm29cQv6ZlC9f/rSHLH/11VdWvnx5t2qFhYXZO++8Y1u2bCnwNmvWLLdfr0FBQfkOITx69Ki1adPGatWqZb/++muRauV9QDDL/yXU1q1bi/Ql1GeffWbR0dH2+uuvO5ed6xtVeHh4gW9U7u54b7/9dueMSnJycr7D499++22rWrWq2+Mq6LW/ePFi69SpkwUFBVlQUJBbtfK+MDQ78ffpcDhc/gYXLlxoFStWdKtWVFSUy08I9u7daw6Hw/mlwKZNm9x6vh5++GGLiYmxzz77zOVIkMzMTPvss88sNjbWevfu7daYmjZtaqNGjSq03d19vdmJL2MK2j/t37/fEhMT7brrrnP7NV+5cmWXAPLmm2+6fHmSlpbm9geEPD///LPFx8db9+7d7eDBg2f9mn/ooYfs8ccft/Dw8Hz70LS0NCtbtqxbtZYsWWIhISE2ePBge/31161s2bL23HPP2bRp02zQoEEWGhp62n+bk508Y1mQDRs2uPyU63Q6dOhgCQkJ9v7771vr1q0tOTnZGjRoYGvXrrV169ZZkyZN3J55vvPOO61t27Z24MABy8nJsT59+rj8jGnZsmVu/zsSSgglBY3rcg4l7OvZ15/qStjXF+aKCuKLFy92Cb2nOnDgwGnfEPOMGDHCeehlQXr27On2H/748eNt5syZhbYPGDDAOZt8NrZv325ffPGFHThwoEjrDRkyxOV26gk9+vXrZ+3atXO73t69e+3pp5+2+Ph4CwgIMD8/P4uJibH777/ffvzxR7frxMbGntXhWQU5fvy4vfDCC3bbbbfZiBEjLDc31/773/9adHS0hYWFWefOnYv8vB0+fNjt35SdzsCBA6106dI2evRo++WXXyw9Pd3S09Ptl19+sdGjR1uZMmXcOtzXzKxFixY2fPjwQtuL8kZVs2bNfF/KmP3/YbxSpUpuv1FVr17d5XwGM2fOdB7Kb3ZiB+duGMzz119/WfPmze3WW2+1v//++6zfqFq1amV33nmnlS5dOt8XIsuWLXP75yK///67hYWFWceOHW348OFWsmRJ69Chg73wwgvWsWNH8/f3t0mTJrlV60xvVJmZmfnOv1CYXr16WdWqVe3555+3+vXrW6dOnax69eo2e/ZsmzNnjtWsWdMefPBBt2p16tTJmjRpYmvXrrVNmzY5fxOZZ+HChW79jOXIkSPWo0cP8/PzMy8vLwsICLCAgADz8vIyPz8/69mzpx05csStMU2cOPG05wRIT093Obni6TzyyCOFvmlnZWVZQkKC26/5hx56yN5+++1C21NTU61Vq1Zu1TrZoUOH7KGHHrKqVauat7d3kV/zTZo0cTnB56ljHD58uDVp0sTtekuWLLEGDRrkOwKnQoUKRfo93Zm+eC2K9PR0u+WWW6xkyZKWnJxs+/bts969e7ucAPTkcHg6f/75p1WuXNl8fHzM19fXQkNDnSckNTvx0w53D2kmlBBKTnW5h5LC9vUOh4N9/Rmwrz+z4rqvL8wVFcSBS8nIkSOtfPnyLocbOhwOK1++vNtv6GYnZoqnTp1aaPuePXts8uTJbtV66qmnCv1t+tGjR+322293+4PekCFD7L///W+h7c8884zdddddbtU6WW5uro0YMcJ5ApmivlF17tzZ5fbRRx+5tD/55JOWnJzsdr2NGzdau3btrFSpUs43KV9fX2vYsKF9/vnnbtc5n29UBw4csG7dutm1115r3bt3t+zsbHvppZfMz8/PHA6HNW3a1O3HysjIcL4Re3l5WUxMjMuhth9//LG99tprbo8tMzPTFixYYB988IF98MEHtmDBggLPleApe/bsyTdTdrKsrCy3vsB1x6ZNm1yuwFFUX375pfXp0+e8vU7y/Pnnn7Z9+/Yir7dr1y5btmyZLVmyxOXoF3dt2bLFee6BC+XPP//MdySaOw4ePGjffPONzZgx45zO+n2+v4A63YdfQomrKz2UOByOixpKMjMzbf78+c59/fz588/bvv5s9huF7evzap2PfX1erfOxr3/00UfP2+skb1znY19flCsE5NmyZUuBJ1w+n/v/87mvP5/jcpiZnftF0ABcKJs3b3a5zqa712W8EI4dO6ZDhw4pODi40PYdO3ac83UtJenQoUPy9vaWv7//Wa2flpam77//Xh07dlTp0qXPeTx5Dh48KG9vbwUEBBRpPTPTrl27lJubq7JlyzqvvVmcHDlyREePHj2r691v2LBB2dnZql69unx8fC7A6IDLV1ZWltLS0lz29XXr1i10X3uh7d27Vzt37tQ111xTYPv+/fu1cuVKNWnS5Jwfa/PmzQoICFD58uXPav2vvvpK3333nQYMGKDw8PBzHk+eTZs2yc/PTxUrVizSert379amTZuUm5ur8uXLKzY2tkjrb926VZUqVZLD4SjSekWxadMmHTp0qMj760OHDumHH35Qdna2GjRocM7Xwj6Zn5+ffvnlF9WoUYNa1LpiavFpCSjm4uLi8oXv7du3a/DgwXr33XfPuX5Ravn4+Jz2g+Hff/+toUOHnpdx/fPPP+e0jXXr1lXdunUlnd/na8+ePWdVy+FwKCIiwmXZxfp3LExAQIACAgLOqlbVqlXPeVyHDx9WWlqaypQpo/j4eJe2I0eOaPr06erYsaNb46EWtS6VWmvXrtWyZcuUmJioZs2aad26dRo7dqymTp2qDh06qHnz5m7VObVW9erVnbWys7OLVKt06dJKT0/XpEmTzrnW+RxXQbWqVaumr7/+Wv379z/rWg0bNtTVV199XsbVsGFDJSQkaN26dRo1alSRa8XExFyQ5+t8bOPWrVv1119/KTExUWXLlj2rWn379i1w+fHjxzVy5EiFhYVJkkaPHk0tal02tQp13ubWAXhMUa4HSy1qXQq1/vjjD+e1k728vOymm26yHTt2ONuLcpbmgmqdfBggtahVXGrNnj3b/Pz8rEyZMhYQEGCzZ8+2cuXKWVJSkjVv3ty8vb1dzqVBLWpd6rUcDofVrl3b5WcBTZs2NYfDYfXq1bOmTZtas2bN3BoTtah1qdQqDEEcKIa+/PLL095effVVtz/oUYtal0KtNm3aWEpKiu3evds2bNhgKSkpFhcX5zzbclHCDbWodanUSkxMtGeffdbMTlzppHTp0i4n4urfv7/dcsst1KLWZVMrNTXV4uLi8oX2sznhHrWodanUKgxBHCiGTnft75OvAU4tal0utcLDw12ub5ubm2s9evSwSpUq2Z9//lmkcEMtal0qtYKDg23Dhg1mduJKHj4+Pi4nO1y9erXbV2mgFrUulVorVqywatWq2RNPPGE5OTlmdvbhhlrUulRqFcTr7A9qB3ChlC9fXp999plyc3MLvK1cuZJa1Lqsah0+fNjlpEEOh0Pjx49X69at1aRJE61fv97tMVGLWpdKrbz1JcnLy0sBAQEKCQlxtpUqVUqZmZnUotZlVatevXpKS0vT7t27dcMNN+i3334765PTUYtal0qtghDEgWKobt26SktLK7Td4XDI3LzgAbWodSnUql69un766ad8y9944w3dcccduv32290aD7WodSnVio2N1YYNG5z3ly5dqkqVKjnvb9u2ze2ziVOLWpdKLUkqWbKkpkyZogEDBigpKUnHjx93e11qUetSrZXPeZlXB3BeLV682GbPnl1o+4EDB9y+niW1qHUp1BoxYoS1bNmy0PaePXu6fY16alHrUqk1fvx4mzlzZqHtAwYMsC5dulCLWpdVrVNt377dvvjiCztw4MBZrU8tal2Ktcy4jjgAAAAAAB7FoekAAAAAAHgQQRwAAAAAAA8iiAMAAAAA4EEEcQAAAAAAPIggDgBAMbdw4UI5HA7t27fvYg8FAACcBwRxAACKmaZNm6pPnz7O+w0bNtTff/+tkJCQizeo0+CLAgAAisbnYg8AAACcnp+fnyIjIy/2MAAAwHnCjDgAAMVI586dtWjRIo0dO1YOh0MOh0OTJ092mXGePHmyQkNDNXPmTF199dUqUaKE7r77bh06dEhTpkxRbGysSpcurUcffVTHjx931s7Ozla/fv1UoUIFBQUFKSEhQQsXLnRrXFu3blXr1q1VunRpBQUF6ZprrtHXX3+tLVu2qFmzZpKk0qVLy+FwqHPnzpKk3NxcpaamKi4uToGBgbruuuv0ySefOGvmzaTPmjVLtWrVUkBAgBo0aKDffvvtvDyXAAAUV8yIAwBQjIwdO1br16/Xtddeq2HDhkmS1qxZk6/foUOH9Nprr+nDDz/U/v37ddddd+nOO+9UaGiovv76a23atElt27bVjTfeqPvuu0+S1Lt3b/3+++/68MMPFRUVpc8//1y33nqrVq9erapVq552XL169VJOTo4WL16soKAg/f777ypZsqSio6P16aefqm3btvrjjz8UHByswMBASVJqaqref/99TZgwQVWrVtXixYvVoUMHlStXTk2aNHHWfvLJJzV27FhFRkbqmWeeUevWrbV+/Xr5+vqer6cVAIBihSAOAEAxEhISIj8/P5UoUcJ5OPq6devy9Tt69KjGjx+vypUrS5LuvvtuTZ06VRkZGSpZsqTi4+PVrFkzfffdd7rvvvu0bds2TZo0Sdu2bVNUVJQkqV+/fpozZ44mTZqkESNGnHZc27ZtU9u2bVWzZk1J0lVXXeVsK1OmjCQpPDxcoaGhkk7Mvo8YMULffvutEhMTnet8//33euutt1yC+ODBg3XLLbdIkqZMmaKKFSvq888/17333lvk5w8AgEsBQRwAgEtQiRIlnCFckiIiIhQbG6uSJUu6LNu1a5ckafXq1Tp+/LiqVavmUic7O1thYWFnfLxHH31UPXv21Ny5c5WUlKS2bduqVq1ahfbfuHGjDh065AzYeXJyclSnTh2XZXlBXToR6q+++mqtXbv2jGMCAOBSRRAHAOASdOph2w6Ho8Blubm5kqQDBw7I29tbaWlp8vb2dul3cngvTNeuXZWcnKxZs2Zp7ty5Sk1N1SuvvKJHHnmkwP4HDhyQJM2aNUsVKlRwafP39z/j4wEAcDkjiAMAUMz4+fm5nGTtfKhTp46OHz+uXbt2qXHjxmdVIzo6Wj169FCPHj00YMAAvf3223rkkUfk5+cnSS5jjo+Pl7+/v7Zt2+ZyGHpBli1bpkqVKkmS9u7dq/Xr16tGjRpnNUYAAC4FBHEAAIqZ2NhYLV++XFu2bFHJkiWds9rnolq1amrfvr06duyoV155RXXq1NHu3bs1f/581apVSykpKaddv0+fPmrZsqWqVaumvXv36rvvvnOG5ZiYGDkcDs2cOVOtWrVSYGCgSpUqpX79+unxxx9Xbm6uGjVqpMzMTP3www8KDg5Wp06dnLWHDRumsLAwRURE6Nlnn1XZsmXVpk2bc95mAACKKy5fBgBAMdOvXz95e3srPj5e5cqV07Zt285L3UmTJqljx4564okndPXVV6tNmzb68ccfnbPRp3P8+HH16tVLNWrU0K233qpq1arpzTfflCRVqFBBQ4cOVf/+/RUREaHevXtLkoYPH66BAwcqNTXVud6sWbMUFxfnUnvkyJF67LHHVLduXaWnp2vGjBnOWXYAAC5HDjOziz0IAABw5Vm4cKGaNWumvXv3Os+2DgDAlYAZcQAAAAAAPIggDgAA1LJlS5UsWbLA25muMQ4AAIqGQ9MBAIB27Nihw4cPF9hWpkwZlSlTxsMjAgDg8kUQBwAAAADAgzg0HQAAAAAADyKIAwAAAADgQQRxAAAAAAA8iCAOAAAAAIAHEcQBAAAAAPAggjgAAAAAAB5EEAcAAAAAwIP+H9WSr8JaannKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1200x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "basic stats: nodes, edges: 203769 234355\n"
          ]
        }
      ],
      "source": [
        "#EDA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE = \"data/elliptic/elliptic_bitcoin_dataset\"\n",
        "\n",
        "# 1) load files\n",
        "feat_df = pd.read_csv(f\"{BASE}/elliptic_txs_features.csv\", header=None)\n",
        "# the CSV: col0=txId, col1=time_step, cols2.. = features\n",
        "feat_df = feat_df.rename(columns={0: \"txId\", 1: \"time_step\"})\n",
        "edge_df = pd.read_csv(f\"{BASE}/elliptic_txs_edgelist.csv\")  # two cols: source,dst\n",
        "class_df = pd.read_csv(f\"{BASE}/elliptic_txs_classes.csv\")  # likely columns: txId, class\n",
        "\n",
        "print(\"features shape:\", feat_df.shape)\n",
        "print(\"edges shape:\", edge_df.shape)\n",
        "print(\"classes shape:\", class_df.shape)\n",
        "\n",
        "# 2) label distribution\n",
        "print(class_df['class'].value_counts(dropna=False))\n",
        "\n",
        "# 3) nodes per timestep\n",
        "counts_by_time = feat_df['time_step'].value_counts().sort_index()\n",
        "counts_by_time.plot(kind='bar', figsize=(12,3), title='nodes per time-step')\n",
        "plt.show()\n",
        "\n",
        "# 4) degree distribution (use networkx, but only on edges)\n",
        "G = nx.from_pandas_edgelist(edge_df, source=edge_df.columns[0], target=edge_df.columns[1], create_using=nx.DiGraph)\n",
        "deg = np.array([d for _, d in G.degree()])\n",
        "plt.hist(deg, bins=50)\n",
        "plt.title(\"degree distribution (undirected degree)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"basic stats: nodes, edges:\", G.number_of_nodes(), G.number_of_edges())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8g9tpnumWG9"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > train_elliptic_sage.py <<'PY'\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "train_elliptic_sage.py\n",
        "Simple GraphSAGE baseline for Elliptic dataset (PyTorch Geometric).\n",
        "- Loads PyG EllipticBitcoinDataset\n",
        "- Trains a small SAGE model on timestamp-based train_mask\n",
        "- Reports AUPRC, ROC-AUC, Precision@K\n",
        "- Produces a 1-hop subgraph plot of top illicit prediction\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Hyperparams\n",
        "HIDDEN = 128\n",
        "NUM_LAYERS = 3\n",
        "EPOCHS = 50\n",
        "LR = 0.01\n",
        "WEIGHT_DECAY = 5e-4\n",
        "DROPOUT = 0.5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "def make_model(in_channels, hidden, out_channels, num_layers=3):\n",
        "    class SAGE(torch.nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.convs = torch.nn.ModuleList()\n",
        "            self.convs.append(SAGEConv(in_channels, hidden))\n",
        "            for _ in range(num_layers - 2):\n",
        "                self.convs.append(SAGEConv(hidden, hidden))\n",
        "            self.convs.append(SAGEConv(hidden, out_channels))\n",
        "            self.dropout = DROPOUT\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            for conv in self.convs[:-1]:\n",
        "                x = conv(x, edge_index)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = self.convs[-1](x, edge_index)\n",
        "            return x\n",
        "    return SAGE()\n",
        "\n",
        "def precision_at_k(y_true, scores, K):\n",
        "    # y_true: 0/1 array; scores: float array (higher -> more likely 1)\n",
        "    order = np.argsort(-scores)\n",
        "    topk = order[:K]\n",
        "    return float(y_true[topk].sum() / K)\n",
        "\n",
        "def main(root='data/elliptic/pyg'):\n",
        "    print(\"Loading dataset (PyG will download if needed) ...\")\n",
        "    dataset = EllipticBitcoinDataset(root)\n",
        "    data = dataset[0]\n",
        "    print(\"Data stats: x.shape\", data.x.shape, \"edge_index.shape\", data.edge_index.shape)\n",
        "\n",
        "    # Create a validation split from the train_mask\n",
        "    train_nodes = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
        "    n_val = max(1, int(0.1 * train_nodes.size(0)))\n",
        "    perm = torch.randperm(train_nodes.size(0))\n",
        "    val_nodes = train_nodes[perm[:n_val]]\n",
        "    train_nodes_new = train_nodes[perm[n_val:]]\n",
        "    train_mask = torch.zeros_like(data.train_mask)\n",
        "    val_mask = torch.zeros_like(data.train_mask)\n",
        "    train_mask[train_nodes_new] = True\n",
        "    val_mask[val_nodes] = True\n",
        "\n",
        "    # send to device\n",
        "    data = data.to(DEVICE)\n",
        "    train_mask = train_mask.to(DEVICE)\n",
        "    val_mask = val_mask.to(DEVICE)\n",
        "\n",
        "    model = make_model(data.num_features, HIDDEN, dataset.num_classes, NUM_LAYERS).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_ap = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)                # logits shape [N, num_classes]\n",
        "        loss = loss_fn(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(data.x, data.edge_index)\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # prob of class=1 (illicit)\n",
        "            # val indices and labels\n",
        "            val_idx = val_mask.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "            if len(val_idx) > 0:\n",
        "                y_val = data.y[val_mask].cpu().numpy()\n",
        "                y_scores_val = probs[val_idx]\n",
        "                val_ap = average_precision_score(y_val, y_scores_val)\n",
        "            else:\n",
        "                val_ap = 0.0\n",
        "\n",
        "        if val_ap > best_val_ap:\n",
        "            best_val_ap = val_ap\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:3d} loss: {loss.item():.4f}  val_AP: {val_ap:.4f}  best_val_AP: {best_val_ap:.4f}\")\n",
        "\n",
        "    # Load best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # TEST metrics\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x, data.edge_index)\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "        test_idx = data.test_mask.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "        y_test = data.y[data.test_mask].cpu().numpy()\n",
        "        scores_test = probs[test_idx]\n",
        "\n",
        "        # AUPRC\n",
        "        ap = average_precision_score(y_test, scores_test)\n",
        "        # ROC AUC (needs at least two classes present)\n",
        "        try:\n",
        "            roc = roc_auc_score(y_test, scores_test)\n",
        "        except ValueError:\n",
        "            roc = float('nan')\n",
        "\n",
        "        p_at_10 = precision_at_k(y_test, scores_test, K=10) if len(scores_test) >= 10 else None\n",
        "        p_at_50 = precision_at_k(y_test, scores_test, K=50) if len(scores_test) >= 50 else None\n",
        "\n",
        "        print(\"TEST: AUPRC (avg precision):\", ap)\n",
        "        print(\"TEST: ROC-AUC:\", roc)\n",
        "        print(\"TEST: Precision@10:\", p_at_10, \" Precision@50:\", p_at_50)\n",
        "\n",
        "    # Visualize 1-hop subgraph around top predicted illicit (from test set)\n",
        "    # pick top predicted test node\n",
        "    order = np.argsort(-scores_test)\n",
        "    if len(order) == 0:\n",
        "        print(\"No test nodes to visualize.\")\n",
        "        return\n",
        "\n",
        "    top_test_node = int(test_idx[order[0]])\n",
        "    # k-hop subgraph (k=1)\n",
        "    subset, edge_index_sub, mapping, edge_mask = k_hop_subgraph(top_test_node, 1, data.edge_index, relabel_nodes=True)\n",
        "    # convert to networkx and plot\n",
        "    sub_x = data.x[subset].cpu().numpy()\n",
        "    sub_y = data.y[subset].cpu().numpy()\n",
        "    # reconstruct Data for to_networkx\n",
        "    from torch_geometric.data import Data\n",
        "    sub_data = Data(x=data.x[subset].cpu(), edge_index=edge_index_sub)\n",
        "    G = to_networkx(sub_data, to_undirected=True)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "    # color nodes by ground-truth (unknown will be 2; convert to colors)\n",
        "    colors = []\n",
        "    for i, n in enumerate(G.nodes()):\n",
        "        label = int(sub_y[i]) if i < len(sub_y) else 2\n",
        "        if label == 1:\n",
        "            colors.append('red')   # illicit\n",
        "        elif label == 0:\n",
        "            colors.append('green') # licit\n",
        "        else:\n",
        "            colors.append('grey')  # unknown\n",
        "    nx.draw(G, pos, node_color=colors, with_labels=True, node_size=200)\n",
        "    plt.title(f\"1-hop subgraph around node {top_test_node} (top predicted illicit)\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmvRfYRsmgFK",
        "outputId": "18f16938-5648-4e03-a3da-6b33c1b036b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Loading dataset (PyG will download if needed) ...\n",
            "Downloading https://data.pyg.org/datasets/elliptic/elliptic_txs_features.csv.zip\n",
            "Downloading https://data.pyg.org/datasets/elliptic/elliptic_txs_edgelist.csv.zip\n",
            "Downloading https://data.pyg.org/datasets/elliptic/elliptic_txs_classes.csv.zip\n",
            "Processing...\n",
            "Done!\n",
            "Data stats: x.shape torch.Size([203769, 165]) edge_index.shape torch.Size([2, 234355])\n",
            "Epoch   1 loss: 0.8481  val_AP: 0.2575  best_val_AP: 0.2575\n",
            "Epoch   5 loss: 0.3152  val_AP: 0.5533  best_val_AP: 0.5533\n",
            "Epoch  10 loss: 0.2116  val_AP: 0.7395  best_val_AP: 0.7395\n",
            "Epoch  15 loss: 0.1872  val_AP: 0.8207  best_val_AP: 0.8207\n",
            "Epoch  20 loss: 0.1701  val_AP: 0.8680  best_val_AP: 0.8680\n",
            "Epoch  25 loss: 0.1566  val_AP: 0.8887  best_val_AP: 0.8887\n",
            "Epoch  30 loss: 0.1451  val_AP: 0.8875  best_val_AP: 0.8887\n",
            "Epoch  35 loss: 0.1335  val_AP: 0.9029  best_val_AP: 0.9029\n",
            "Epoch  40 loss: 0.1224  val_AP: 0.9208  best_val_AP: 0.9208\n",
            "Epoch  45 loss: 0.1093  val_AP: 0.9415  best_val_AP: 0.9415\n",
            "Epoch  50 loss: 0.0957  val_AP: 0.9509  best_val_AP: 0.9509\n",
            "TEST: AUPRC (avg precision): 0.6387877336891976\n",
            "TEST: ROC-AUC: 0.8983259068140514\n",
            "TEST: Precision@10: 1.0  Precision@50: 0.96\n",
            "Figure(600x600)\n"
          ]
        }
      ],
      "source": [
        "# Run the script (in Colab you can just run it here)\n",
        "!python train_elliptic_sage.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGUdWwr2nRcx"
      },
      "source": [
        "From here its step 2\n",
        "B — Ethereum (wallet → wallet graphs) — detailed step-by-step\n",
        "\n",
        "Goal: build wallet graph (nodes = addresses, edges = transfers, timestamps) and small, reviewable sample graphs for GNN training & TGN events.\n",
        "\n",
        "B.1 Why BigQuery\n",
        "\n",
        "Public Ethereum tables are huge — use Google BigQuery public datasets to query and export only the slice you need (date range / top wallets). This is standard practice.\n",
        "Google Cloud\n",
        "Google Cloud\n",
        "\n",
        "B.2 Setup BigQuery (GCP)\n",
        "\n",
        "Create a Google Cloud Project and enable billing (queries cost money by bytes scanned).\n",
        "\n",
        "In the console enable the BigQuery API and the Blockchain Analytics: Ethereum Mainnet dataset (public).\n",
        "Google Cloud\n",
        "\n",
        "Create a service account and download the JSON key if you want to query from Python. Set GOOGLE_APPLICATION_CREDENTIALS to that key path.\n",
        "\n",
        "Install python client:\n",
        "\n",
        "pip install google-cloud-bigquery\n",
        "\n",
        "B.3 Example SQL (grab ETH transfers 1-year slice)\n",
        "\n",
        "Run this in BigQuery console or from Python:\n",
        "\n",
        "SELECT\n",
        "  LOWER(from_address) AS src,\n",
        "  LOWER(to_address)   AS dst,\n",
        "  block_timestamp AS ts,\n",
        "  SAFE_DIVIDE(value, 1e18) AS eth_value\n",
        "FROM `bigquery-public-data.crypto_ethereum.transactions`\n",
        "WHERE block_timestamp BETWEEN TIMESTAMP('2024-01-01') AND TIMESTAMP('2024-12-31')\n",
        "AND from_address IS NOT NULL\n",
        "AND to_address IS NOT NULL\n",
        "LIMIT 1000000;  -- <- sample size, adjust\n",
        "\n",
        "\n",
        "(Export as CSV/parquet to Cloud Storage if you need local processing.)\n",
        "Google Cloud\n",
        "\n",
        "B.4 Build a manageable graph (important)\n",
        "\n",
        "Ethereum full graph is huge. For development & reviewer-friendly demos, sample:\n",
        "\n",
        "Top N wallets by tx count (e.g., N=50k) or\n",
        "\n",
        "Subgraph around known labels (exchanges, flagged addresses) with radius-2 neighborhood.\n",
        "This lets you run GNNs and TGN locally.\n",
        "\n",
        "B.5 Convert to PyG / TemporalData\n",
        "\n",
        "Static graph (PyG Data):\n",
        "\n",
        "# after exporting edges.csv\n",
        "import pandas as pd\n",
        "edges = pd.read_csv('eth_edges.csv', parse_dates=['ts'])\n",
        "# build address->id mapping (only for sampled addresses)\n",
        "addresses = pd.unique(edges[['src','dst']].values.ravel())\n",
        "addr2id = {a:i for i,a in enumerate(addresses)}\n",
        "edges['src_id'] = edges['src'].map(addr2id); edges['dst_id'] = edges['dst'].map(addr2id)\n",
        "# compute simple node features\n",
        "# save torch_geometric.data.Data with x, edge_index\n",
        "\n",
        "\n",
        "Temporal event stream (for TGN/TGAT):\n",
        "Save src_id,dst_id,ts,edge_features sorted by ts. Use PyG TemporalData or the TGN example (PyG docs have TGN example).\n",
        "pytorch-geometric.readthedocs.io\n",
        "\n",
        "B.6 Training & experiments\n",
        "\n",
        "Train GraphSAGE on the sampled static graph (labels from a smaller labeled set or weak labels).\n",
        "\n",
        "For temporal modeling, train TGN/TGAT on the event stream to predict next-node risk or next-edge risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9RcYtIdloe_A",
        "outputId": "5c1a6dc6-3568-4032-e292-de4434457aef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0fa4334-0c4e-44ec-9fbd-0b6ec2542d95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d0fa4334-0c4e-44ec-9fbd-0b6ec2542d95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Install BigQuery client\n",
        "!pip install --quiet google-cloud-bigquery pandas pyarrow\n",
        "\n",
        "# Authenticate (upload your GCP service account key JSON)\n",
        "from google.colab import files\n",
        "files.upload()  # <- upload service-account.json\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"traffic-detection-453110-1668ff6074c9.json\"\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zxAnozeuze7"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet google-cloud-bigquery pandas pyarrow\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  LOWER(from_address) AS src,\n",
        "  LOWER(to_address) AS dst,\n",
        "  block_timestamp AS ts,\n",
        "  SAFE_DIVIDE(value, 1e18) AS eth_value\n",
        "FROM `bigquery-public-data.crypto_ethereum.transactions`\n",
        "WHERE block_timestamp BETWEEN TIMESTAMP('2024-01-01') AND TIMESTAMP('2024-12-31')\n",
        "AND from_address IS NOT NULL\n",
        "AND to_address IS NOT NULL\n",
        "LIMIT 2000000;  -- adjust sample size\n",
        "\"\"\"\n",
        "\n",
        "df = client.query(query).to_dataframe()\n",
        "print(df.head())\n",
        "print(\"Shape:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJp_wlLZxUlc"
      },
      "source": [
        "B.4 Build Manageable Graph\n",
        "\n",
        "Ethereum is huge → sample for dev:\n",
        "\n",
        "Option 1: Top N active wallets\n",
        "\n",
        "Option 2: Neighborhood around known labeled addresses\n",
        "\n",
        "Option 3: Just random slice (for experiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfvANgk8vgRC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume df is already loaded from BigQuery (columns: src, dst, ts, eth_value)\n",
        "print(\"Original df shape:\", df.shape)\n",
        "\n",
        "# Choose sampling strategy: \"top_n\", \"neighborhood\", or \"random\"\n",
        "sampling_strategy = \"top_n\"  # change to \"neighborhood\" or \"random\"\n",
        "\n",
        "if sampling_strategy == \"top_n\":\n",
        "    # -----------------------------\n",
        "    # Option 1: Top N active wallets by degree\n",
        "    # -----------------------------\n",
        "    N = 50000\n",
        "    all_addrs = pd.concat([df['src'], df['dst']])\n",
        "    top_addrs = all_addrs.value_counts().head(N).index\n",
        "    sampled = df[(df['src'].isin(top_addrs)) & (df['dst'].isin(top_addrs))].copy()\n",
        "    print(f\"[Top N] Sampled edges (top {N} wallets):\", sampled.shape)\n",
        "\n",
        "elif sampling_strategy == \"neighborhood\":\n",
        "    # -----------------------------\n",
        "    # Option 2: Neighborhood around known labeled addresses\n",
        "    # -----------------------------\n",
        "    # Example labeled wallets (replace with actual list of known exchange or flagged addresses)\n",
        "    labeled_wallets = ['0xabc123...', '0xdef456...']  # placeholder\n",
        "    radius = 2  # neighborhood radius (2-hop)\n",
        "\n",
        "    # Build adjacency dict\n",
        "    adj = {}\n",
        "    for _, row in df.iterrows():\n",
        "        adj.setdefault(row['src'], set()).add(row['dst'])\n",
        "        adj.setdefault(row['dst'], set()).add(row['src'])\n",
        "\n",
        "    # BFS to collect radius-neighborhood\n",
        "    selected_wallets = set(labeled_wallets)\n",
        "    frontier = set(labeled_wallets)\n",
        "    for _ in range(radius):\n",
        "        next_frontier = set()\n",
        "        for w in frontier:\n",
        "            next_frontier |= adj.get(w, set())\n",
        "        next_frontier -= selected_wallets\n",
        "        selected_wallets |= next_frontier\n",
        "        frontier = next_frontier\n",
        "\n",
        "    sampled = df[df['src'].isin(selected_wallets) & df['dst'].isin(selected_wallets)].copy()\n",
        "    print(f\"[Neighborhood] Sampled edges (radius={radius} around {len(labeled_wallets)} labeled wallets):\", sampled.shape)\n",
        "\n",
        "elif sampling_strategy == \"random\":\n",
        "    # -----------------------------\n",
        "    # Option 3: Random slice for experiments\n",
        "    # -----------------------------\n",
        "    sample_size = 500000  # number of edges\n",
        "    sampled = df.sample(n=min(sample_size, len(df)), random_state=42).copy()\n",
        "    print(f\"[Random] Sampled random {sample_size} edges:\", sampled.shape)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Unknown sampling_strategy. Choose 'top_n', 'neighborhood', or 'random'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMI6sYKcheOu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"PyG:\", torch_geometric.__version__)\n",
        "\n",
        "# quick sanity check: make a tiny graph\n",
        "edge_index = torch.tensor([[0, 1, 1, 2],\n",
        "                           [1, 0, 2, 1]], dtype=torch.long)\n",
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gIplfvZfeN6"
      },
      "outputs": [],
      "source": [
        "# Check python/torch/cuda info\n",
        "import sys, torch\n",
        "print(\"python:\", sys.version.splitlines()[0])\n",
        "print(\"torch:\", getattr(torch, \"__version__\", \"NOT INSTALLED\"))\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"torch.version.cuda:\", getattr(torch, \"version\", None) and torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTJBv-7Nfzg0"
      },
      "outputs": [],
      "source": [
        "# Robust PyG install: auto-detects torch version and CUDA and installs matching PyG wheels.\n",
        "import sys, subprocess, importlib\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    torch = None\n",
        "\n",
        "print(\"Python:\", sys.executable)\n",
        "if torch is None:\n",
        "    print(\"Torch not found. Will install torch (CPU) first.\")\n",
        "else:\n",
        "    print(\"Found torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available(), \"torch.version.cuda:\", getattr(torch, \"version\", None) and torch.version.cuda)\n",
        "\n",
        "# Upgrade pip\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "\n",
        "# If torch missing, install a lightweight CPU torch (safe fallback)\n",
        "if torch is None:\n",
        "    print(\"Installing CPU torch (safe default)...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\",\n",
        "                           \"torch\", \"torchvision\", \"torchaudio\"])\n",
        "    import torch\n",
        "\n",
        "# Build wheel URL for PyG\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA_TAG = \"cpu\" if not torch.cuda.is_available() or torch.version.cuda is None else (\"cu\" + torch.version.cuda.replace('.', ''))\n",
        "PYG_WHL = f\"https://data.pyg.org/whl/torch-{TORCH}+{CUDA_TAG}.html\"\n",
        "print(\"Using TORCH tag:\", TORCH, \"CUDA tag:\", CUDA_TAG)\n",
        "print(\"PyG wheel index:\", PYG_WHL)\n",
        "\n",
        "# Try installing the binary wheels for the heavy deps, then torch_geometric\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--find-links\", PYG_WHL,\n",
        "                           \"torch_scatter\", \"torch_sparse\", \"torch_cluster\", \"torch_spline_conv\"], stderr=subprocess.STDOUT)\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch_geometric\"], stderr=subprocess.STDOUT)\n",
        "    print(\"PyG and dependencies installed successfully.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"Primary wheel-based install failed. Trying fallback: single-step install of torch_geometric (may attempt builds).\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch_geometric\"], stderr=subprocess.STDOUT)\n",
        "    except subprocess.CalledProcessError as e2:\n",
        "        print(\"Fallback install also failed. See the pip output above for errors.\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X9jLjSOf_uj"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhAeWzitgg2P"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTRKWZ3jgl7W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import torch_scatter, torch_sparse, torch_cluster, torch_spline_conv\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"PyG:\", torch_geometric.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDVIVwXsxKL5"
      },
      "source": [
        "B.5 Convert to PyTorch Geometric Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56bbnoYribgI"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet networkx torch torch_geometric\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.data import Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxf8ihvdigEC"
      },
      "outputs": [],
      "source": [
        "print(df.head())\n",
        "# Columns: src, dst, ts, eth_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "950a6c6d"
      },
      "outputs": [],
      "source": [
        "# Re-run the cell to load the data into df after runtime restart\n",
        "!pip install --quiet google-cloud-bigquery pandas pyarrow\n",
        "from google.colab import files\n",
        "# You may need to re-upload your service account key if prompted\n",
        "# files.upload()\n",
        "\n",
        "import os\n",
        "# Make sure the path to your service account key is correct\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"traffic-detection-453110-1668ff6074c9.json\"\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client()\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  LOWER(from_address) AS src,\n",
        "  LOWER(to_address) AS dst,\n",
        "  block_timestamp AS ts,\n",
        "  SAFE_DIVIDE(value, 1e18) AS eth_value\n",
        "FROM `bigquery-public-data.crypto_ethereum.transactions`\n",
        "WHERE block_timestamp BETWEEN TIMESTAMP('2024-01-01') AND TIMESTAMP('2024-12-31')\n",
        "AND from_address IS NOT NULL\n",
        "AND to_address IS NOT NULL\n",
        "LIMIT 2000000;  -- adjust sample size\n",
        "\"\"\"\n",
        "\n",
        "df = client.query(query).to_dataframe()\n",
        "print(df.head())\n",
        "print(\"Shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlndXej-lsEN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, networkx as nx, matplotlib.pyplot as plt, torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# Define the SageNet model class locally, based on train_elliptic_sage.py\n",
        "class SageNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden, out_channels, num_layers=3, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden, hidden))\n",
        "        self.convs.append(SAGEConv(hidden, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Load data using EllipticBitcoinDataset directly\n",
        "print(\"Loading dataset (PyG will download if needed) ...\")\n",
        "dataset = EllipticBitcoinDataset(\"data/elliptic/pyg\")\n",
        "data = dataset[0]\n",
        "print(\"Data stats: x.shape\", data.x.shape, \"edge_index.shape\", data.edge_index.shape)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SageNet(data.x.shape[1], 64, dataset.num_classes).to(device) # Use dataset.num_classes\n",
        "# Check if the model file exists before loading\n",
        "if os.path.exists(\"elliptic_sage.pt\"):\n",
        "    model.load_state_dict(torch.load(\"elliptic_sage.pt\", map_location=device))\n",
        "else:\n",
        "    print(\"Model file 'elliptic_sage.pt' not found. Skipping model loading.\")\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "# get predictions\n",
        "model.eval()\n",
        "out = model(data.x, data.edge_index).detach()\n",
        "probs = torch.softmax(out,1)[:,1].cpu().numpy()\n",
        "test_idx = np.where(data.test_mask.cpu().numpy())[0]\n",
        "\n",
        "# Ensure there are test nodes before attempting to find the top one\n",
        "if len(test_idx) == 0:\n",
        "    print(\"No test nodes available for visualization.\")\n",
        "else:\n",
        "    top_idx = test_idx[np.argsort(-probs[test_idx])[:1]][0]\n",
        "\n",
        "    # build graph with txids\n",
        "    BASE = \"data/elliptic/elliptic_bitcoin_dataset\"\n",
        "    edge_df = pd.read_csv(f\"{BASE}/elliptic_txs_edgelist.csv\")\n",
        "    # Load txids from the features file to create the mapping\n",
        "    feat_df = pd.read_csv(f\"{BASE}/elliptic_txs_features.csv\", header=None)\n",
        "    txids = feat_df[0].tolist()\n",
        "    id2idx = {txid:i for i,txid in enumerate(txids)}\n",
        "    idx2id = {v:k for k,v in id2idx.items()}\n",
        "    G = nx.from_pandas_edgelist(edge_df, edge_df.columns[0], edge_df.columns[1])\n",
        "\n",
        "    # Check if the top_idx is in the mapping before creating ego graph\n",
        "    if idx2id[top_idx] not in G:\n",
        "        print(f\"Top predicted node {idx2id[top_idx]} not found in the loaded graph.\")\n",
        "    else:\n",
        "        ego = nx.ego_graph(G, idx2id[top_idx], radius=1)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        # Use a layout for better visualization\n",
        "        pos = nx.spring_layout(ego)\n",
        "        nx.draw(ego, pos, with_labels=True, node_size=50)\n",
        "        plt.title(f\"1-hop ego graph of tx {idx2id[top_idx]}\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rRlXQmjoFDd"
      },
      "source": [
        "C -  load Ethereum or Elliptic data, build a graph with nodes and edges, and train a Graph Attention Network (GAT). GAT learns which connections matter most. After training, you extract attention weights to see which wallets or transactions most influence predictions—making the model interpretable and useful for AML insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C92IlmoYoDzT"
      },
      "outputs": [],
      "source": [
        "# Run as a single cell in Colab. It tries to match PyG wheels to your torch+CUDA.\n",
        "# If this fails in your environment, follow PyG install docs for exact torch/cuda versions.\n",
        "\n",
        "# NOTE: This cell may take a couple minutes.\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q pandas numpy matplotlib scikit-learn networkx\n",
        "\n",
        "# Install google colab friendly torch if needed (Colab usually has torch preinstalled)\n",
        "import importlib, sys\n",
        "try:\n",
        "    import torch\n",
        "    print(\"Torch already installed:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
        "except Exception as e:\n",
        "    print(\"Torch not installed; installing CPU fallback.\")\n",
        "    !pip install -q torch torchvision torchaudio\n",
        "\n",
        "# Try to install PyG via the recommended wheel index (works in most Colab setups)\n",
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cpu' if torch.version.cuda is None else 'cu' + torch.version.cuda.replace('.', '')\n",
        "PYG_WHL = f'https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html'\n",
        "print(\"Attempting to install PyG from:\", PYG_WHL)\n",
        "!pip install -q --find-links {PYG_WHL} torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric || pip install -q torch_geometric\n",
        "\n",
        "# Final quick import check\n",
        "import torch, torch_geometric, matplotlib, pandas, networkx\n",
        "print(\"torch:\", torch.__version__, \"torch_geometric:\", torch_geometric.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlK2TN9up-Yo"
      },
      "source": [
        "c-1 -— Install dependencies (robust PyG install)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uenBBa0Qom5B"
      },
      "outputs": [],
      "source": [
        "# This cell tries to reuse a PyG `data` in the notebook (e.g., your sampled Ethereum Data object).\n",
        "# If `data` is not defined, it downloads the Elliptic PyG dataset as a small working example.\n",
        "\n",
        "from IPython.display import display\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    data  # if user already created `data` in earlier cells\n",
        "    print(\"Found `data` in environment; using existing PyG Data object.\")\n",
        "except NameError:\n",
        "    print(\"No `data` found. Loading Elliptic dataset as a fallback (small & labeled).\")\n",
        "    from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "    dataset = EllipticBitcoinDataset('data/elliptic/pyg')\n",
        "    data = dataset[0]\n",
        "    print(\"Loaded Elliptic dataset - data.x.shape:\", data.x.shape, \"edge_index.shape:\", data.edge_index.shape)\n",
        "\n",
        "# Basic checks\n",
        "print(\"num_nodes:\", data.num_nodes, \"num_edges:\", data.num_edges)\n",
        "if hasattr(data, 'y'):\n",
        "    print(\"Has labels (y) with unique values:\", torch.unique(data.y))\n",
        "else:\n",
        "    print(\"No labels found in data.y; you'll need labels for supervised training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf97qClTpz7c"
      },
      "source": [
        "\n",
        "\n",
        "c 2 — Prepare data (use your data if present; else fallback to Elliptic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d627648e"
      },
      "outputs": [],
      "source": [
        "# Re-run the sampling strategy cell to define 'sampled'\n",
        "import pandas as pd\n",
        "\n",
        "# Assume df is already loaded from BigQuery (columns: src, dst, ts, eth_value)\n",
        "print(\"Original df shape:\", df.shape)\n",
        "\n",
        "# Choose sampling strategy: \"top_n\", \"neighborhood\", or \"random\"\n",
        "sampling_strategy = \"top_n\"  # change to \"neighborhood\" or \"random\"\n",
        "\n",
        "if sampling_strategy == \"top_n\":\n",
        "    # -----------------------------\n",
        "    # Option 1: Top N active wallets by degree\n",
        "    # -----------------------------\n",
        "    N = 50000\n",
        "    all_addrs = pd.concat([df['src'], df['dst']])\n",
        "    top_addrs = all_addrs.value_counts().head(N).index\n",
        "    sampled = df[(df['src'].isin(top_addrs)) & (df['dst'].isin(top_addrs))].copy()\n",
        "    print(f\"[Top N] Sampled edges (top {N} wallets):\", sampled.shape)\n",
        "\n",
        "elif sampling_strategy == \"neighborhood\":\n",
        "    # -----------------------------\n",
        "    # Option 2: Neighborhood around known labeled addresses\n",
        "    # -----------------------------\n",
        "    # Example labeled wallets (replace with actual list of known exchange or flagged addresses)\n",
        "    labeled_wallets = ['0xabc123...', '0xdef456...']  # placeholder\n",
        "    radius = 2  # neighborhood radius (2-hop)\n",
        "\n",
        "    # Build adjacency dict\n",
        "    adj = {}\n",
        "    for _, row in df.iterrows():\n",
        "        adj.setdefault(row['src'], set()).add(row['dst'])\n",
        "        adj.setdefault(row['dst'], set()).add(row['src'])\n",
        "\n",
        "    # BFS to collect radius-neighborhood\n",
        "    selected_wallets = set(labeled_wallets)\n",
        "    frontier = set(labeled_wallets)\n",
        "    for _ in range(radius):\n",
        "        next_frontier = set()\n",
        "        for w in frontier:\n",
        "            next_frontier |= adj.get(w, set())\n",
        "        next_frontier -= selected_wallets\n",
        "        selected_wallets |= next_frontier\n",
        "        frontier = next_frontier\n",
        "\n",
        "    sampled = df[df['src'].isin(selected_wallets) & df['dst'].isin(selected_wallets)].copy()\n",
        "    print(f\"[Neighborhood] Sampled edges (radius={radius} around {len(labeled_wallets)} labeled wallets):\", sampled.shape)\n",
        "\n",
        "elif sampling_strategy == \"random\":\n",
        "    # -----------------------------\n",
        "    # Option 3: Random slice for experiments\n",
        "    # -----------------------------\n",
        "    sample_size = 500000  # number of edges\n",
        "    sampled = df.sample(n=min(sample_size, len(df)), random_state=42).copy()\n",
        "    print(f\"[Random] Sampled random {sample_size} edges:\", sampled.shape)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Unknown sampling_strategy. Choose 'top_n', 'neighborhood', or 'random'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UumlZS7Ipp0X"
      },
      "source": [
        "c-3 Define GAT model (with heads) and training utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUEPF4h8o8ln"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "class GATNet(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hid=64, heads=4):\n",
        "        super().__init__()\n",
        "        # first layer: multi-head; concat=True (default) -> out dim = hid * heads\n",
        "        self.gat1 = GATConv(in_dim, hid, heads=heads, dropout=0.4)\n",
        "        # second layer: heads=1 so output dim = hid\n",
        "        self.gat2 = GATConv(hid * heads, hid, heads=1, dropout=0.4)\n",
        "        self.lin = torch.nn.Linear(hid, 2)  # 2 classes (adjust if different)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.elu(self.gat1(x, edge_index))   # shape: [N, hid*heads]\n",
        "        x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = self.gat2(x, edge_index)          # shape: [N, hid] (heads=1)\n",
        "        return self.lin(x)                    # logits [N, 2]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Training helper (simple)\n",
        "def train_epoch(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x.to(device), data.edge_index.to(device))\n",
        "    # use training mask and only known labels (exclude 'unknown' if present)\n",
        "    if hasattr(data, 'train_mask'):\n",
        "        mask = data.train_mask\n",
        "    else:\n",
        "        mask = torch.ones(data.num_nodes, dtype=torch.bool)\n",
        "    if hasattr(data, 'y'):\n",
        "        known = (data.y != 2)  # Elliptic uses 2 for unknown; adapt if your labels differ\n",
        "        mask = mask & known\n",
        "    loss = criterion(out[mask.to(device)], data.y[mask].to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def eval_metrics(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "    if hasattr(data, 'test_mask'):\n",
        "        test_idx = data.test_mask.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "        y_test = data.y[data.test_mask].cpu().numpy()\n",
        "        scores_test = probs[test_idx]\n",
        "    else:\n",
        "        # fallback evaluate on all known nodes\n",
        "        known = (data.y != 2) if hasattr(data,'y') else torch.ones(data.num_nodes, dtype=torch.bool)\n",
        "        idx = np.where(known.cpu().numpy())[0]\n",
        "        y_test = data.y[known].cpu().numpy()\n",
        "        scores_test = probs[idx]\n",
        "    ap = average_precision_score(y_test, scores_test) if len(np.unique(y_test))>1 else float('nan')\n",
        "    roc = roc_auc_score(y_test, scores_test) if len(np.unique(y_test))>1 else float('nan')\n",
        "    return {'ap': ap, 'roc': roc}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jp5DsHSpewc"
      },
      "source": [
        "## c-4-Train a small GAT baseline (quick run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f17RnqCpCcK"
      },
      "outputs": [],
      "source": [
        "# Hyperparams — tune for your environment\n",
        "HID = 64\n",
        "HEADS = 4\n",
        "EPOCHS = 30\n",
        "LR = 0.005\n",
        "WD = 5e-4\n",
        "\n",
        "model = GATNet(data.num_features, hid=HID, heads=HEADS).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Prepare mask handling\n",
        "if not hasattr(data, 'train_mask'):\n",
        "    # If no masks present, create a small pseudo-split (only for demo)\n",
        "    n = data.num_nodes\n",
        "    idx = torch.randperm(n)\n",
        "    train_mask = torch.zeros(n, dtype=torch.bool); train_mask[idx[:int(0.6*n)]] = True\n",
        "    val_mask = torch.zeros(n, dtype=torch.bool);   val_mask[idx[int(0.6*n):int(0.8*n)]] = True\n",
        "    test_mask = torch.zeros(n, dtype=torch.bool);  test_mask[idx[int(0.8*n):]] = True\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "# Ensure labels exist\n",
        "if not hasattr(data, 'y'):\n",
        "    raise RuntimeError(\"data.y not found — need node labels for supervised training.\")\n",
        "\n",
        "# Train loop\n",
        "best_val_ap = 0.0\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    loss = train_epoch(model, data, optimizer, criterion)\n",
        "    metrics = eval_metrics(model, data)\n",
        "    if metrics['ap'] and metrics['ap'] > best_val_ap:\n",
        "        best_val_ap = metrics['ap']\n",
        "        best_state = model.state_dict()\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:3d}  loss: {loss:.4f}  val_AP: {metrics['ap']:.4f}  ROC: {metrics['roc']:.4f}\")\n",
        "\n",
        "# Load best model if found\n",
        "try:\n",
        "    model.load_state_dict(best_state)\n",
        "    print(\"Loaded best model by validation AP.\")\n",
        "except:\n",
        "    print(\"No saved best state (maybe AP was NaN).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4vnaoqXpX-H"
      },
      "source": [
        "c-5 Extract attention weights and show top incoming edges into suspicious nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUqbB46WpQSL"
      },
      "outputs": [],
      "source": [
        "# This is the key cell: extract attention from the second GAT layer (gat2),\n",
        "# which is closer to the final prediction. We compute intermediate x after gat1.\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # 1) compute intermediate node embeddings after gat1\n",
        "    x1 = F.elu(model.gat1(data.x.to(device), data.edge_index.to(device)))\n",
        "    # 2) run gat2 with return_attention_weights=True to get attention per edge\n",
        "    out2, (edge_index_att, att_weights) = model.gat2(\n",
        "        x1, data.edge_index.to(device), return_attention_weights=True\n",
        "    )\n",
        "    # out2 shape: [N, hid], att_weights shape: [E, heads] (heads==1 for gat2 by our design)\n",
        "    print(\"edge_index_att.shape:\", edge_index_att.shape)\n",
        "    print(\"att_weights.shape:\", att_weights.shape)\n",
        "\n",
        "    # 3) final logits & probabilities\n",
        "    logits = model.lin(out2)\n",
        "    probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "\n",
        "# Convert attention info to numpy arrays for inspection\n",
        "edge_index_np = edge_index_att.cpu().numpy()  # shape [2, E]\n",
        "att_np = att_weights.squeeze().cpu().numpy()  # shape [E] if heads==1, else [E,heads] - adjust accordingly\n",
        "\n",
        "# Helper: find top predicted suspicious nodes (highest prob) among test nodes\n",
        "test_idx = data.test_mask.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "test_scores = probs[test_idx]\n",
        "top_k_nodes = test_idx[np.argsort(-test_scores)[:10]]  # top 10 suspicious test nodes\n",
        "\n",
        "print(\"Top suspicious nodes (node_id -> prob):\")\n",
        "for nid in top_k_nodes[:10]:\n",
        "    print(nid, probs[nid])\n",
        "\n",
        "# For each suspicious node, list top incoming edges by attention weight\n",
        "def top_incoming_attention(node_id, topk=10):\n",
        "    # edges where destination == node_id (edge_index[1] is destination)\n",
        "    dsts = edge_index_np[1]\n",
        "    mask = (dsts == node_id)\n",
        "    if mask.sum() == 0:\n",
        "        return []\n",
        "    srcs = edge_index_np[0][mask]\n",
        "    atts = att_np[mask]\n",
        "    order = np.argsort(-atts)[:topk]\n",
        "    return list(zip(srcs[order].tolist(), atts[order].tolist()))\n",
        "\n",
        "# Print top incoming for each top suspicious node\n",
        "for node in top_k_nodes[:5]:\n",
        "    tins = top_incoming_attention(node, topk=10)\n",
        "    print(f\"\\nNode {node} top incoming edges (src -> att):\")\n",
        "    for s,a in tins:\n",
        "        print(f\"  {s}  ->  {a:.4f}\")\n",
        "\n",
        "# OPTIONAL: visualize 1-hop subgraph around the first suspicious node and scale edge widths by attention\n",
        "sel_node = int(top_k_nodes[0])\n",
        "# Build small directed graph from edge list (only edges among the nodes in the 1-hop)\n",
        "edges_src = edge_index_np[0]\n",
        "edges_dst = edge_index_np[1]\n",
        "one_hop_mask = (edges_dst == sel_node) | (edges_src == sel_node)  # incoming + outgoing\n",
        "sub_src = edges_src[one_hop_mask]\n",
        "sub_dst = edges_dst[one_hop_mask]\n",
        "sub_att = att_np[one_hop_mask]\n",
        "\n",
        "G = nx.DiGraph()\n",
        "for s,d,a in zip(sub_src, sub_dst, sub_att):\n",
        "    G.add_edge(int(s), int(d), weight=float(a))\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "edge_weights = [G[u][v]['weight']*5 for u,v in G.edges()]  # scale for visibility\n",
        "node_colors = ['red' if n==sel_node else 'green' for n in G.nodes()]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color='black', width=edge_weights, arrowsize=10)\n",
        "plt.title(f\"1-hop edges around node {sel_node} (edge width ~ attention)\")\n",
        "plt.show()\n",
        "\n",
        "# Save attention table for the selected node to CSV\n",
        "import pandas as pd\n",
        "tbl = []\n",
        "for s,d,a in zip(edges_src[one_hop_mask], edges_dst[one_hop_mask], sub_att):\n",
        "    tbl.append({'src': int(s), 'dst': int(d), 'att': float(a)})\n",
        "pd.DataFrame(tbl).sort_values('att', ascending=False).to_csv('node_{}_atten_edges.csv'.format(sel_node), index=False)\n",
        "print(\"Saved node attention table to:\", f\"node_{sel_node}_atten_edges.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yje7GyoGrGFd"
      },
      "source": [
        "D - Convert your Ethereum transactions into a temporal event stream with source, destination, timestamp, and features. Use PyG’s TemporalData to store events, then train a TGN/TGAT model that learns dynamic node embeddings. Split data into train/test, train the model, and extract embeddings to predict future risks or node behavior over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1YimVjCrnL6"
      },
      "source": [
        "D-1 Install all the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gNxrnd1qiSQ"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet torch torch_geometric torch_geometric_temporal pandas numpy matplotlib\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2R0NpvesARH"
      },
      "source": [
        "D -2 — Prepare event stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLEwX9oUrrJb"
      },
      "outputs": [],
      "source": [
        "# Example: create temporal event tensors\n",
        "# Build address->id mapping (only for sampled addresses)\n",
        "addresses = pd.unique(sampled[['src','dst']].values.ravel())\n",
        "addr2id = {a:i for i,a in enumerate(addresses)}\n",
        "sampled['src_id'] = sampled['src'].map(addr2id)\n",
        "sampled['dst_id'] = sampled['dst'].map(addr2id)\n",
        "\n",
        "src_arr = sampled['src_id'].values\n",
        "dst_arr = sampled['dst_id'].values\n",
        "ts_arr  = pd.to_datetime(sampled['ts']).astype(int) // 10**9  # convert to UNIX timestamp\n",
        "edge_feat_arr = sampled[['eth_value']].values.astype(float)  # simple feature example\n",
        "\n",
        "# Convert to torch tensors\n",
        "src = torch.tensor(src_arr, dtype=torch.long)\n",
        "dst = torch.tensor(dst_arr, dtype=torch.long)\n",
        "ts  = torch.tensor(ts_arr.values, dtype=torch.float) # Convert Series to NumPy array before creating tensor\n",
        "msg = torch.tensor(edge_feat_arr, dtype=torch.float)\n",
        "\n",
        "print(\"src.shape:\", src.shape, \"dst.shape:\", dst.shape, \"ts.shape:\", ts.shape, \"msg.shape:\", msg.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaVQ4pNDsEJl"
      },
      "source": [
        "D 3 Create TemporalData object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeo-f6NLsImU"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import TemporalData\n",
        "\n",
        "# Build temporal graph object\n",
        "td = TemporalData(src=src, dst=dst, t=ts, msg=msg)\n",
        "\n",
        "print(td)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_6F9sKpssOq"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet torch_geometric_temporal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA_2qkwQsRV1"
      },
      "source": [
        "D 4 — Using TGN model (PyG example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le3QmcDGutrq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BLsRL592u7EH"
      },
      "outputs": [],
      "source": [
        "# Upgrade pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Install torch-geometric compatible packages\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-geometric-temporal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLroP94FvU3R"
      },
      "outputs": [],
      "source": [
        "import torch_geometric\n",
        "import torch_geometric_temporal\n",
        "\n",
        "print(\"torch_geometric version:\", torch_geometric.__version__)\n",
        "print(\"torch_geometric_temporal version:\", torch_geometric_temporal.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amJybLA2v1LN"
      },
      "outputs": [],
      "source": [
        "import torch_geometric_temporal\n",
        "print(torch_geometric_temporal.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv8-AEiPwK8u"
      },
      "outputs": [],
      "source": [
        "!pip show torch_geometric_temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbaf3209"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch_geometric_temporal\n",
        "\n",
        "# Reinstall torch-geometric-temporal with compatible packages\n",
        "import torch\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = 'cpu' if torch.version.cuda is None else 'cu' + torch.version.cuda.replace('.', '')\n",
        "PYG_WHL = f'https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html'\n",
        "print(\"Attempting to install PyG from:\", PYG_WHL)\n",
        "\n",
        "!pip install -q --find-links {PYG_WHL} torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric torch_geometric_temporal || pip install -q torch_geometric_temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9AXg-m1guFK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb736312"
      },
      "outputs": [],
      "source": [
        "!pip show torch_geometric_temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65e4a2a0"
      },
      "outputs": [],
      "source": [
        "# Get the installation path from pip show output\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(['pip', 'show', 'torch_geometric_temporal'], capture_output=True, text=True, check=True)\n",
        "    for line in result.stdout.splitlines():\n",
        "        if line.startswith('Location:'):\n",
        "            install_path = line.split(': ')[1].strip()\n",
        "            break\n",
        "    else:\n",
        "        raise ValueError(\"Could not find installation location in pip show output.\")\n",
        "\n",
        "    models_dir = f\"{install_path}/torch_geometric_temporal/nn/models\"\n",
        "    print(f\"Listing contents of: {models_dir}\")\n",
        "    !ls -l {models_dir}\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Could not automatically determine installation path or list directory contents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SofBZGoJgvJx"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import TemporalData\n",
        "\n",
        "# Build temporal graph object\n",
        "td = TemporalData(src=src, dst=dst, t=ts, msg=msg)\n",
        "\n",
        "print(td)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v13ebIlgygS"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from torch_geometric_temporal.nn.models.tgn import TGNMemory\n",
        "    from torch_geometric_temporal.nn.models.tgn import TGN\n",
        "except ImportError as e:\n",
        "    print(f\"ImportError: {e}\")\n",
        "    print(\"Could not import TGNMemory or TGN from torch_geometric_temporal.nn.models.tgn.\")\n",
        "    print(\"This might be because the 'nn.models' directory is missing in your torch_geometric_temporal installation, as we observed previously.\")\n",
        "    print(\"Please refer to the previous steps for troubleshooting the torch_geometric_temporal installation.\")\n",
        "\n",
        "\n",
        "# Node and edge dimensions\n",
        "num_nodes = int(max(src.max(), dst.max()) + 1)\n",
        "# node_features = msg.shape[1]  # feature dimension - not needed for TGNMemory init\n",
        "edge_features = msg.shape[1]\n",
        "message_dimension = msg.shape[1] # message dimension is typically edge features\n",
        "\n",
        "# Memory module\n",
        "memory = TGNMemory(num_nodes=num_nodes,\n",
        "                   memory_dimension=64, # This is the dimension of the memory\n",
        "                   message_dimension=message_dimension, # Use message_dimension instead of raw_node_features\n",
        "                   edge_features=edge_features)\n",
        "\n",
        "# Example TGN model (simplified)\n",
        "tgn_model = TGN(memory=memory,\n",
        "                message_dimension=message_dimension, # Use message_dimension here as well\n",
        "                embedding_dimension=64) # This is the output dimension of the TGN\n",
        "\n",
        "# Move to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tgn_model = tgn_model.to(device)\n",
        "\n",
        "print(\"TGNMemory and TGN models initialized successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAktDZhCktbb"
      },
      "source": [
        "Updated D code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95dyzOx5mFRP"
      },
      "source": [
        "Cell 1 (Install + Imports): Installs PyTorch Geometric and related libraries → required so we can use TGN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw1-R3SpkwC7"
      },
      "outputs": [],
      "source": [
        "# Cell 1 — install (run once)\n",
        "# NOTE: Installing PyG wheels may vary by Colab runtime. This cell tries a robust approach.\n",
        "!pip install --quiet --upgrade pip\n",
        "\n",
        "# basic libs\n",
        "!pip install --quiet pandas numpy matplotlib scikit-learn networkx\n",
        "\n",
        "# BigQuery client (optional; only needed if you will fetch from BigQuery)\n",
        "!pip install --quiet google-cloud-bigquery\n",
        "\n",
        "# Attempt to install PyG and its required binary packages (uses wheel index matching torch version)\n",
        "import sys, subprocess, importlib\n",
        "import torch, os\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda)\n",
        "\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA_TAG = 'cpu' if torch.version.cuda is None else 'cu' + torch.version.cuda.replace('.', '')\n",
        "PYG_WHL_INDEX = f'https://data.pyg.org/whl/torch-{TORCH}+{CUDA_TAG}.html'\n",
        "print(\"Attempting PyG wheel index:\", PYG_WHL_INDEX)\n",
        "\n",
        "# Try install the compiled extensions then torch_geometric\n",
        "!pip install --quiet --find-links {PYG_WHL_INDEX} torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric || pip install --quiet torch_geometric\n",
        "\n",
        "# final import check\n",
        "import importlib, pkgutil\n",
        "import torch\n",
        "try:\n",
        "    import torch_geometric as tg\n",
        "    print(\"torch_geometric version:\", tg.__version__)\n",
        "except Exception as e:\n",
        "    print(\"Warning: torch_geometric import failed:\", e)\n",
        "    print(\"If import fails, re-run this cell after checking your torch and CUDA versions and using https://data.pyg.org/whl/ for wheel install.\")\n",
        "\n",
        "# Also ensure GATConv/TGN availability\n",
        "from torch_geometric.nn import GATConv\n",
        "try:\n",
        "    from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "    print(\"Elliptic dataset loader available.\")\n",
        "except Exception as e:\n",
        "    print(\"Elliptic loader not available:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6_UtPoYmHtE"
      },
      "source": [
        "Cell 2 (Synthetic Temporal Data): Builds temporal edges (src, dst, time, features) → needed because TGN learns from event streams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZGu9eLLlKOm"
      },
      "outputs": [],
      "source": [
        "# Cell 2 — optional BigQuery fetch (adjust date range & LIMIT)\n",
        "import os, pandas as pd, numpy as np\n",
        "\n",
        "df_eth = None\n",
        "\n",
        "try:\n",
        "    # Try to authenticate automatically (works if GOOGLE_APPLICATION_CREDENTIALS set)\n",
        "    from google.cloud import bigquery\n",
        "    client = bigquery.Client()\n",
        "    print(\"BigQuery client created using GOOGLE_APPLICATION_CREDENTIALS or default credentials.\")\n",
        "    use_bq = True\n",
        "except Exception as e:\n",
        "    use_bq = False\n",
        "    print(\"BigQuery client could not be auto-created. To fetch from BigQuery in Colab, you can upload service account json or use interactive auth.\")\n",
        "    print(\"If you want interactive auth, run this in a separate cell: from google.colab import auth; auth.authenticate_user()\")\n",
        "\n",
        "if use_bq:\n",
        "    query = \"\"\"\n",
        "    SELECT\n",
        "      LOWER(from_address) AS src,\n",
        "      LOWER(to_address)   AS dst,\n",
        "      block_timestamp AS ts,\n",
        "      SAFE_DIVIDE(value, 1e18) AS eth_value\n",
        "    FROM `bigquery-public-data.crypto_ethereum.transactions`\n",
        "    WHERE block_timestamp BETWEEN TIMESTAMP('2024-01-01') AND TIMESTAMP('2024-12-31')\n",
        "    AND from_address IS NOT NULL\n",
        "    AND to_address IS NOT NULL\n",
        "    LIMIT 200000;  -- adjust sample size for demo\n",
        "    \"\"\"\n",
        "    print(\"Running BigQuery (LIMIT 200k). This will take a bit and costs may apply.\")\n",
        "    df_eth = client.query(query).to_dataframe()\n",
        "    print(\"df_eth shape:\", df_eth.shape)\n",
        "    display(df_eth.head())\n",
        "\n",
        "if df_eth is None:\n",
        "    print(\"No df_eth loaded from BigQuery. We'll use synthetic data later or you can re-run this cell after authenticating.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGezRu7mmaSK"
      },
      "source": [
        "Cell 3 (TGN Model): Defines memory + message passing layers (TGN) → this is the core architecture to capture evolving patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrrptCOMlNUJ"
      },
      "outputs": [],
      "source": [
        "# Cell 3 — sample and map addresses -> ids (works if df_eth exists)\n",
        "import torch, numpy as np\n",
        "\n",
        "sampled = None\n",
        "addr2id = None\n",
        "addresses = None\n",
        "\n",
        "if 'df_eth' in globals() and isinstance(df_eth, pd.DataFrame) and len(df_eth)>0:\n",
        "    df = df_eth.copy()\n",
        "    # simple sampling: Top N addresses by occurrence\n",
        "    N_TOP = 50000  # adjust for memory; use smaller values if memory issues\n",
        "    all_addrs = pd.concat([df['src'], df['dst']])\n",
        "    top_addrs = all_addrs.value_counts().head(N_TOP).index\n",
        "    sampled = df[(df['src'].isin(top_addrs)) & (df['dst'].isin(top_addrs))].copy()\n",
        "    print(\"Sampled edges (within top addrs):\", sampled.shape)\n",
        "    addresses = pd.Index(pd.unique(sampled[['src','dst']].values.ravel()))\n",
        "    addr2id = {a:i for i,a in enumerate(addresses)}\n",
        "    sampled['src_id'] = sampled['src'].map(addr2id)\n",
        "    sampled['dst_id'] = sampled['dst'].map(addr2id)\n",
        "    print(\"Num sampled nodes:\", len(addresses))\n",
        "else:\n",
        "    print(\"No Ethereum df available — building a small synthetic sampled dataset (for TGN demo).\")\n",
        "    # Build synthetic sampled (small)\n",
        "    num_nodes = 2000\n",
        "    num_edges = 20000\n",
        "    rng = np.random.default_rng(42)\n",
        "    probs = np.linspace(1,5,num_nodes)\n",
        "    probs = probs/probs.sum()\n",
        "    s = rng.choice(num_nodes, size=num_edges, p=probs)\n",
        "    d = rng.choice(num_nodes, size=num_edges, p=probs)\n",
        "    mask = s == d\n",
        "    d[mask] = (d[mask] + 1) % num_nodes\n",
        "    base_ts = 1_700_000_000\n",
        "    ts = base_ts + np.arange(num_edges)\n",
        "    val = rng.lognormal(mean=0.0, sigma=1.0, size=num_edges)\n",
        "    sampled = pd.DataFrame({'src_id': s, 'dst_id': d, 'ts': pd.to_datetime(ts, unit='s'), 'eth_value': val})\n",
        "    # Map synthetic numeric ids to string addresses for consistency\n",
        "    addresses = pd.Index([f'0x{idx:040d}' for idx in range(num_nodes)])\n",
        "    addr2id = {a:i for i,a in enumerate(addresses)}\n",
        "    print(\"Synthetic sampled shape:\", sampled.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNxUjXM_meVF"
      },
      "source": [
        "Cell 4 (Training Loop): Trains model on events, updates memory after each batch → ensures learning from graph evolution over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKCoieeplRQj"
      },
      "outputs": [],
      "source": [
        "# Cell 4 — create PyG Data for static experiments\n",
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "\n",
        "# Build edge_index tensor from sampled\n",
        "edge_index = torch.tensor([sampled['src_id'].values, sampled['dst_id'].values], dtype=torch.long)\n",
        "\n",
        "# Node features: simple engineered ones (in-degree, out-degree, total_value)\n",
        "n_nodes = int(max(edge_index.max().item(), 0) + 1)\n",
        "import numpy as np\n",
        "deg_in = np.zeros(n_nodes, dtype=np.float32)\n",
        "deg_out = np.zeros(n_nodes, dtype=np.float32)\n",
        "val_sum = np.zeros(n_nodes, dtype=np.float32)\n",
        "\n",
        "for s_id, d_id, v in zip(sampled['src_id'].values, sampled['dst_id'].values, sampled['eth_value'].values):\n",
        "    deg_out[int(s_id)] += 1.0\n",
        "    deg_in[int(d_id)] += 1.0\n",
        "    val_sum[int(s_id)] += float(v)\n",
        "    val_sum[int(d_id)] += float(v)\n",
        "\n",
        "x = torch.tensor(np.stack([deg_in, deg_out, val_sum], axis=1), dtype=torch.float)\n",
        "\n",
        "data_static = Data(x=x, edge_index=edge_index)\n",
        "print(\"Static Data: num_nodes\", data_static.num_nodes, \"num_edges\", data_static.num_edges, \"x.shape\", data_static.x.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG-JSSeLnwOB"
      },
      "source": [
        "Cell 5 (Evaluation): Tests model on unseen events and prints accuracy → validates that TGN generalizes to new temporal edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFybNm0rlWdS"
      },
      "outputs": [],
      "source": [
        "# Cell 5 — Load Elliptic dataset and train a GAT baseline (C)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "\n",
        "# Load Elliptic dataset\n",
        "dataset = EllipticBitcoinDataset(root=\"data\")\n",
        "data = dataset[0]\n",
        "print(data)\n",
        "\n",
        "# Define GAT baseline model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1)\n",
        "        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize model, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GAT(in_channels=data.x.size(1), hidden_channels=64, out_channels=2).to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "# Training loop\n",
        "best_ap, best_state = 0, None\n",
        "for epoch in range(1, 21):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    logits = model.lin(out)\n",
        "    loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out2 = model(data.x, data.edge_index)\n",
        "        logits2 = model.lin(out2)\n",
        "        probs = torch.softmax(logits2, dim=1)[:, 1].detach().cpu().numpy()\n",
        "\n",
        "        test_mask = data.test_mask\n",
        "        test_labels = data.y[test_mask].cpu().numpy()\n",
        "        test_probs = probs[test_mask.cpu().numpy()]\n",
        "\n",
        "        from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "        ap = average_precision_score(test_labels, test_probs)\n",
        "        roc = roc_auc_score(test_labels, test_probs)\n",
        "\n",
        "        if ap > best_ap:\n",
        "            best_ap, best_state = ap, model.state_dict()\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch} loss {loss:.4f}  test_AP {ap:.4f}  ROC {roc:.4f}\")\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(best_state)\n",
        "print(\"Loaded best model with AP\", best_ap)\n",
        "\n",
        "# Inspect top predicted suspicious nodes in test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out2 = model(data.x, data.edge_index)\n",
        "    logits = model.lin(out2)\n",
        "    probs_final = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()  # FIXED\n",
        "\n",
        "test_idx = data.test_mask.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "top_nodes = test_idx[np.argsort(-probs_final[test_idx])[:10]]\n",
        "\n",
        "print(\"Top 10 suspicious nodes:\", top_nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki4AphBvlng5"
      },
      "outputs": [],
      "source": [
        "# Cell 6 — TemporalData and TGN skeleton (streaming link-prediction demo)\n",
        "import torch\n",
        "from torch_geometric.data import TemporalData\n",
        "\n",
        "# Build TemporalData fields: src, dst, t (unix sec), msg (edge features)\n",
        "if 'sampled' not in globals() or sampled is None:\n",
        "    raise RuntimeError(\"No sampled DataFrame found. Run Cell 3 (BigQuery fetch) or use synthetic fallback.\")\n",
        "\n",
        "# If sampled has string addresses -> use src_id/dst_id numeric; else if synthetic already contains numeric ids, fine.\n",
        "if 'src_id' in sampled.columns and 'dst_id' in sampled.columns:\n",
        "    src_arr = sampled['src_id'].values\n",
        "    dst_arr = sampled['dst_id'].values\n",
        "else:\n",
        "    # if keys are already numeric\n",
        "    src_arr = sampled['src'].values\n",
        "    dst_arr = sampled['dst'].values\n",
        "\n",
        "# ensure ts is datetime; convert to unix seconds\n",
        "ts_col = sampled['ts'] if 'ts' in sampled.columns else None\n",
        "if ts_col is None:\n",
        "    # synthetic where ts numeric not present: build sequential timestamps\n",
        "    ts_unix = np.arange(len(src_arr)).astype(np.int64) + 1_700_000_000\n",
        "else:\n",
        "    ts_unix = pd.to_datetime(ts_col).astype('int64')//10**9\n",
        "\n",
        "# edge features (use eth_value or fallback to zeros)\n",
        "if 'eth_value' in sampled.columns:\n",
        "    edge_feats = np.log1p(sampled['eth_value'].astype(float).values).astype(np.float32).reshape(-1,1)\n",
        "else:\n",
        "    edge_feats = np.zeros((len(src_arr),1), dtype=np.float32)\n",
        "\n",
        "src = torch.tensor(src_arr, dtype=torch.long)\n",
        "dst = torch.tensor(dst_arr, dtype=torch.long)\n",
        "t   = torch.tensor(ts_unix.values if isinstance(ts_unix, pd.Series) else ts_unix, dtype=torch.long)\n",
        "msg = torch.tensor(edge_feats, dtype=torch.float)\n",
        "\n",
        "td = TemporalData(src=src, dst=dst, t=t, msg=msg)\n",
        "print(\"TemporalData built:\", td)\n",
        "\n",
        "# === Minimal TGN training using PyG's TGN utilities (if available) ===\n",
        "# The following code uses TGNMemory and LastNeighborLoader from torch_geometric.nn.models.tgn\n",
        "try:\n",
        "    from torch_geometric.nn.models.tgn import TGNMemory, IdentityMessage, LastAggregator, LastNeighborLoader\n",
        "    print(\"TGN components imported successfully.\")\n",
        "    use_tgn = True\n",
        "except Exception as e:\n",
        "    print(\"TGN components not available in this PyG installation:\", e)\n",
        "    use_tgn = False\n",
        "\n",
        "if use_tgn:\n",
        "    # small config (reduce memory footprint for demo)\n",
        "    num_nodes = int(max(int(src.max()) , int(dst.max())) + 1)\n",
        "    raw_msg_dim = msg.size(-1)\n",
        "    memory_dim = 64\n",
        "    time_dim = 16\n",
        "    embed_dim = 64\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    memory = TGNMemory(num_nodes=num_nodes,\n",
        "                       raw_msg_dim=raw_msg_dim,\n",
        "                       memory_dim=memory_dim,\n",
        "                       time_dim=time_dim,\n",
        "                       message_module=IdentityMessage(raw_msg_dim, memory_dim, time_dim),\n",
        "                       aggregator_module=LastAggregator()).to(device)\n",
        "\n",
        "    # small GNN and linear head\n",
        "    from torch_geometric.nn import TransformerConv\n",
        "    gnn = TransformerConv(in_channels=memory_dim, out_channels=embed_dim, heads=2).to(device)\n",
        "    lin = torch.nn.Linear(embed_dim, 1).to(device)\n",
        "\n",
        "    # simple streaming training skeleton (in-batch negatives)\n",
        "    optimizer = torch.optim.Adam(list(memory.parameters()) + list(gnn.parameters()) + list(lin.parameters()), lr=1e-3)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # chronological split by time\n",
        "    N = src.size(0)\n",
        "    i_train = int(0.8*N); i_val = int(0.9*N)\n",
        "    slices = {'train': slice(0,i_train), 'val': slice(i_train,i_val), 'test': slice(i_val,N)}\n",
        "    batch_size = 2048\n",
        "\n",
        "    def run_epoch(split='train', train_mode=True):\n",
        "        sl = slices[split]\n",
        "        s = src[sl].to(device); d = dst[sl].to(device); tt = t[sl].to(device); mm = msg[sl].to(device)\n",
        "        if train_mode:\n",
        "            memory.train(); gnn.train(); lin.train()\n",
        "        else:\n",
        "            memory.eval(); gnn.eval(); lin.eval()\n",
        "        # reset memory (optional) for demo\n",
        "        memory.reset_state()\n",
        "        losses = []\n",
        "        ys, ps = [], []\n",
        "        for st in range(0, s.size(0), batch_size):\n",
        "            ed = min(s.size(0), st+batch_size)\n",
        "            b_s = s[st:ed]; b_d = d[st:ed]; b_t = tt[st:ed]; b_m = mm[st:ed]\n",
        "            # query memory to get node embeddings for nodes touched in this batch\n",
        "            touched = torch.cat([b_s, b_d]).unique()\n",
        "            z_mem, last_update = memory(touched.to(device))\n",
        "            # For demo, if no graph history produce direct mapping\n",
        "            # Build simple embeddings lookup: map b_s and b_d into z_mem via memory.assoc info (internal)\n",
        "            # Some PyG TGN APIs maintain assoc mapping; for brevity we use the following approach:\n",
        "            # compute embeddings for touched nodes and then index by the positions (this depends on TGN impl)\n",
        "            # We'll compute scores using a simple approach: average memory vector for source/dest\n",
        "            # NOTE: the exact bookkeeping depends on TGN version. This skeleton shows the idea.\n",
        "            # For thorough experiments follow official example: examples/tgn.py in PyG repo.\n",
        "            if z_mem is None:\n",
        "                continue\n",
        "            # naive scoring — use dot(src,dst)\n",
        "            # create mapping from touched nodes -> positions\n",
        "            touched = touched.to(device)\n",
        "            # we need to produce embeddings for each b_s and b_d — simplest if memory has .get_memory or internal assoc\n",
        "            # For demonstration, assume touched are indexed 0..len(touched)-1 and that mapping is available:\n",
        "            # Build a dict mapping node_id->index:\n",
        "            node_to_idx = {int(n.item()): idx for idx, n in enumerate(touched)}\n",
        "            # Now fetch embeddings for b_s and b_d:\n",
        "            src_idx = torch.tensor([node_to_idx[int(x.item())] for x in b_s.cpu()], dtype=torch.long, device=device)\n",
        "            dst_idx = torch.tensor([node_to_idx[int(x.item())] for x in b_d.cpu()], dtype=torch.long, device=device)\n",
        "            emb = z_mem  # shape [len(touched), memory_dim]\n",
        "            src_z = emb[src_idx]\n",
        "            dst_z = emb[dst_idx]\n",
        "            # in-batch negative by permuting dst\n",
        "            neg_dst_idx = dst_idx[torch.randperm(dst_idx.size(0))]\n",
        "            neg_z = emb[neg_dst_idx]\n",
        "            pos_logits = (src_z * dst_z).sum(dim=-1, keepdim=True)\n",
        "            neg_logits = (src_z * neg_z).sum(dim=-1, keepdim=True)\n",
        "            logits = torch.cat([pos_logits, neg_logits], dim=0).squeeze(-1)\n",
        "            labels = torch.cat([torch.ones(pos_logits.size(0), device=device), torch.zeros(neg_logits.size(0), device=device)], dim=0)\n",
        "            loss = bce(logits, labels)\n",
        "            if train_mode:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "            with torch.no_grad():\n",
        "                probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                ps.append(probs); ys.append(labels.detach().cpu().numpy())\n",
        "            # update memory with observed positive edges\n",
        "            memory.update_state(b_s, b_d, b_t, b_m)\n",
        "            memory.detach()  # detach to truncate graph\n",
        "        if len(ps)==0:\n",
        "            return None, None, None\n",
        "        ps = np.concatenate(ps); ys = np.concatenate(ys)\n",
        "        from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "        ap = average_precision_score(ys, ps)\n",
        "        auc = roc_auc_score(ys, ps)\n",
        "        return np.mean(losses), ap, auc\n",
        "\n",
        "    # Quick train for 1 epoch (demo)\n",
        "    for epoch in range(1,3):\n",
        "        tr_loss, tr_ap, tr_auc = run_epoch('train', train_mode=True)\n",
        "        val_loss, val_ap, val_auc = run_epoch('val', train_mode=False)\n",
        "        print(f\"Epoch {epoch} TRAIN loss {tr_loss:.4f} AUPRC {tr_ap:.4f} AUC {tr_auc:.4f} || VAL loss {val_loss:.4f} AUPRC {val_ap:.4f} AUC {val_auc:.4f}\")\n",
        "\n",
        "    # final test\n",
        "    te_loss, te_ap, te_auc = run_epoch('test', train_mode=False)\n",
        "    print(f\"TEST loss {te_loss:.4f} AUPRC {te_ap:.4f} AUC {te_auc:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"PyG TGN modules not available in this installation. To run TGN, ensure your torch_geometric version contains torch_geometric.nn.models.tgn components. As a fallback, you can install 'torch-geometric-temporal' and use its streaming iterators (pip install torch_geometric_temporal).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BJ9DSCsn75g"
      },
      "source": [
        "E - Step E — Hybrid: Graph + Sequence\n",
        "\n",
        "What: Combine graph embeddings (GNN) with transaction sequences (LSTM/Transformer) for each wallet.\n",
        "\n",
        "Why: Some patterns are purely sequential (recent transactions) while others are structural (network). Combining both improves prediction.\n",
        "\n",
        "How:\n",
        "\n",
        "Build transaction sequences for each node (last L txns).\n",
        "\n",
        "Encode sequences with LSTM → get seq embeddings.\n",
        "\n",
        "Encode graph with GNN → get graph embeddings.\n",
        "\n",
        "Concatenate embeddings → feed into MLP classifier → train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuDF96_Fo0OK"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Step E — Hybrid Graph + Sequence encoder\n",
        "# ===============================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.nn import GATConv, SAGEConv\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load existing sampled + static data from previous steps ---\n",
        "# (assuming you already created sampled_df and data_static in A,B,C,D)\n",
        "TOP_K = 5000\n",
        "# Ensure sampled_df exists and is a DataFrame\n",
        "if 'sampled_df' not in globals() or not isinstance(sampled_df, pd.DataFrame):\n",
        "    raise RuntimeError(\"sampled_df not found. Please run the previous steps to create it.\")\n",
        "\n",
        "sampled_df = sampled_df.copy()\n",
        "# Ensure data_static exists\n",
        "if 'data_static' not in globals():\n",
        "     raise RuntimeError(\"data_static not found. Please run the previous steps to create it.\")\n",
        "data_static = data_static  # GNN input from step C/D\n",
        "\n",
        "\n",
        "# --- Fix ts dtype issue ---\n",
        "# Check if the column is not already a datetime type\n",
        "if sampled_df['ts'].dtype.kind != 'M':\n",
        "    sampled_df['ts'] = pd.to_datetime(sampled_df['ts'], utc=True, errors='coerce')\n",
        "# Drop rows where conversion failed\n",
        "sampled_df = sampled_df.dropna(subset=['ts'])\n",
        "# ensure int unix timestamps\n",
        "sampled_df['ts'] = sampled_df['ts'].astype('int64') // 10**9\n",
        "\n",
        "# --- Explicitly convert eth_value to float ---\n",
        "try:\n",
        "    sampled_df['eth_value'] = sampled_df['eth_value'].astype(float)\n",
        "except ValueError as e:\n",
        "    print(f\"Error converting 'eth_value' to float: {e}\")\n",
        "    print(\"This might be due to non-numeric values in the 'eth_value' column.\")\n",
        "    # Optionally, inspect non_numeric values:\n",
        "    # non_numeric_values = sampled_df[pd.to_numeric(sampled_df['eth_value'], errors='coerce').isna()]['eth_value'].unique()\n",
        "    # print(\"Example non-numeric 'eth_value' entries:\", non_numeric_values)\n",
        "    raise # Re-raise the error after printing info\n",
        "\n",
        "\n",
        "# Keep only TOP_K nodes\n",
        "# Map original node IDs to new contiguous IDs (0 to TOP_K-1)\n",
        "original_nodes = data_static.x.shape[0]\n",
        "if TOP_K < original_nodes:\n",
        "    # Assuming data_static's nodes correspond to 0 to original_nodes-1\n",
        "    # and we want to keep the first TOP_K nodes for simplicity in this example.\n",
        "    # In a real scenario, you might select nodes based on activity, labels, etc.\n",
        "    nodes_keep = np.arange(TOP_K)\n",
        "else:\n",
        "    nodes_keep = np.arange(original_nodes)\n",
        "TOP_K = len(nodes_keep) # Adjust TOP_K to the actual number of nodes kept\n",
        "\n",
        "# Create a mapping from original node ID to the new contiguous ID\n",
        "original_to_new_id = {original_id: new_id for new_id, original_id in enumerate(nodes_keep)}\n",
        "\n",
        "# Filter sampled_df to include only edges where both src and dst are in nodes_keep\n",
        "mask_keep = sampled_df['src_id'].isin(nodes_keep) & sampled_df['dst_id'].isin(nodes_keep)\n",
        "sampled_df = sampled_df[mask_keep].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Map the src_id and dst_id in sampled_df to the new contiguous IDs\n",
        "sampled_df['src_id_new'] = sampled_df['src_id'].map(original_to_new_id)\n",
        "sampled_df['dst_id_new'] = sampled_df['dst_id'].map(original_to_new_id)\n",
        "\n",
        "\n",
        "print(f\"Using {len(nodes_keep)} nodes out of {original_nodes} total (TOP_K={TOP_K})\")\n",
        "print(f\"Sampled_df shape after filtering and remapping: {sampled_df.shape}\")\n",
        "# --- Sequence dataset builder ---\n",
        "def build_sequences(df, nodes_to_process, L=10):\n",
        "    # Use new contiguous IDs for sequence building\n",
        "    seqs = {nid: [] for nid in range(len(nodes_to_process))}\n",
        "\n",
        "    # Iterate through the filtered and remapped DataFrame\n",
        "    for _, row in df.iterrows():\n",
        "        src_new_id = row['src_id_new']\n",
        "        dst_new_id = row['dst_id_new']\n",
        "        eth_value = row['eth_value'] # Now should be float\n",
        "        timestamp = row['ts']\n",
        "\n",
        "        # Append sequences using the new IDs\n",
        "        if src_new_id in seqs:\n",
        "             seqs[src_new_id].append([eth_value, timestamp, 1])  # outgoing\n",
        "        if dst_new_id in seqs:\n",
        "             seqs[dst_new_id].append([eth_value, timestamp, 0])  # incoming\n",
        "\n",
        "\n",
        "    # pad or trim\n",
        "    X = []\n",
        "    for nid in range(len(nodes_to_process)):\n",
        "        hist = sorted(seqs[nid], key=lambda x: x[1])[-L:]\n",
        "        hist = np.array(hist)\n",
        "        if len(hist) < L:\n",
        "            pad = np.zeros((L-len(hist), 3))\n",
        "            hist = np.vstack([pad, hist])\n",
        "        X.append(hist)\n",
        "    return torch.tensor(np.array(X), dtype=torch.float)\n",
        "\n",
        "# Pass the original nodes_keep list to build_sequences to know how many nodes to process\n",
        "seq_tensor = build_sequences(sampled_df, nodes_keep, L=10).to(device)  # [N_kept, L, 3]\n",
        "\n",
        "# --- Graph encoder (GraphSAGE) ---\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hid=64, out_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hid)\n",
        "        self.conv2 = SAGEConv(hid, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "# --- Sequence encoder (LSTM-based example) ---\n",
        "class SeqEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # sequences shape: [N, L, input_dim]\n",
        "        lstm_out, _ = self.lstm(sequences)\n",
        "        # Use the last hidden state of the LSTM\n",
        "        last_hidden_state = lstm_out[:, -1, :] # shape: [N, hidden_dim]\n",
        "        return self.linear(last_hidden_state)  # shape: [N, output_dim]\n",
        "\n",
        "\n",
        "# --- Hybrid model ---\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, g_in, g_hid=64, g_out=64, s_in=3, s_hid=32, s_out=64, final_hid=64):\n",
        "        super().__init__()\n",
        "        self.gnn = GraphEncoder(g_in, g_hid, g_out)\n",
        "        self.seq = SeqEncoder(s_in, s_hid, s_out)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(g_out+s_out, final_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_hid, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, seqs):\n",
        "        g_emb = self.gnn(x, edge_index)          # [N, g_out]\n",
        "        s_emb = self.seq(seqs)                   # [N, s_out]\n",
        "        joint = torch.cat([g_emb, s_emb], dim=1) # [N, g_out+s_out]\n",
        "        return self.mlp(joint)\n",
        "\n",
        "# --- Prepare model ---\n",
        "# Ensure data_static's node features are for the nodes we are keeping\n",
        "data_static_kept = data_static.x[nodes_keep].to(device)\n",
        "\n",
        "# Rebuild edge_index for the kept nodes using the new contiguous IDs\n",
        "# Need to map original edge_index nodes to the new contiguous IDs\n",
        "original_edge_index = data_static.edge_index.cpu().numpy()\n",
        "# Filter edges where both source and destination are in nodes_keep\n",
        "edge_mask_kept = np.isin(original_edge_index[0], nodes_keep) & np.isin(original_edge_index[1], nodes_keep)\n",
        "filtered_edge_index = original_edge_index[:, edge_mask_kept]\n",
        "\n",
        "# Map the original IDs in the filtered edge_index to the new contiguous IDs\n",
        "mapped_edge_index_src = np.array([original_to_new_id[node] for node in filtered_edge_index[0]])\n",
        "mapped_edge_index_dst = np.array([original_to_new_id[node] for node in filtered_edge_index[1]])\n",
        "edge_index_kept = torch.tensor([mapped_edge_index_src, mapped_edge_index_dst], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "model = HybridModel(g_in=data_static_kept.shape[1]).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- Create dummy labels and masks for the kept nodes ---\n",
        "# Assuming binary classification for simplicity\n",
        "y = torch.zeros(TOP_K, dtype=torch.long).to(device) # Dummy labels (all zeros)\n",
        "\n",
        "# Simple train/test split on the kept nodes\n",
        "num_train = int(0.8 * TOP_K)\n",
        "train_mask = torch.zeros(TOP_K, dtype=torch.bool).to(device)\n",
        "test_mask = torch.zeros(TOP_K, dtype=torch.bool).to(device)\n",
        "# Randomly select nodes for train and test masks\n",
        "perm = torch.randperm(TOP_K)\n",
        "train_mask[perm[:num_train]] = True\n",
        "test_mask[perm[num_train:]] = True\n",
        "\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(1, 11):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # Use the data for the kept nodes and the remapped edge_index\n",
        "    out = model(data_static_kept,\n",
        "                edge_index_kept,\n",
        "                seq_tensor) # seq_tensor already filtered and built for kept nodes\n",
        "\n",
        "    loss = F.cross_entropy(out[train_mask], y[train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Use the data for the kept nodes and the remapped edge_index for evaluation\n",
        "            out_eval = model(data_static_kept,\n",
        "                             edge_index_kept,\n",
        "                             seq_tensor) # seq_tensor already filtered and built for kept nodes\n",
        "\n",
        "            preds = out_eval[test_mask].argmax(dim=1)\n",
        "            # Ensure y[test_mask] is not empty before calculating accuracy\n",
        "            if y[test_mask].size(0) > 0:\n",
        "                acc = (preds == y[test_mask]).float().mean().item()\n",
        "                print(f\"Epoch {epoch}, Loss {loss.item():.4f}, Test Acc {acc:.4f}\")\n",
        "            else:\n",
        "                 print(f\"Epoch {epoch}, Loss {loss.item():.4f}, No test nodes to evaluate accuracy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYh3wNz7rmsa"
      },
      "source": [
        "Step - F\n",
        "Computes AUPRC, Precision@K, ROC-AUC, and plots PR curve.\n",
        "\n",
        "Displays confusion matrix for threshold 0.5.\n",
        "\n",
        "Shows a live demo: top predicted illicit node → 1-hop subgraph + risk score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "904b4331"
      },
      "outputs": [],
      "source": [
        "# --- Load Elliptic Dataset ---\n",
        "# This dataset includes node features, edge index, labels, and train/test masks.\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import SAGEConv # Using SAGEConv as in the previous HybridModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Loading Elliptic dataset...\")\n",
        "dataset = EllipticBitcoinDataset(root=\"data/elliptic/pyg\")\n",
        "data = dataset[0]\n",
        "print(\"Elliptic data loaded:\", data)\n",
        "\n",
        "# --- Adapt Sequence Building for Elliptic Data ---\n",
        "# The Elliptic dataset has a 'time_step' feature (data.x[:, 1]).\n",
        "# We can use this to build sequences of recent transactions for each node.\n",
        "\n",
        "# Need a mapping from node index to original transaction ID to link with edge list\n",
        "# The Elliptic dataset stores txId in the first column of the original features file.\n",
        "# We need to ensure feat_df is loaded and has the correct column names\n",
        "BASE = \"data/elliptic/elliptic_bitcoin_dataset\"\n",
        "try:\n",
        "    # Try to reuse feat_df if it exists and has the correct columns\n",
        "    if 'feat_df' not in globals() or 'txId' not in feat_df.columns or 'time_step' not in feat_df.columns:\n",
        "        print(\"feat_df not found or missing columns. Reloading elliptic_txs_features.csv.\")\n",
        "        feat_df = pd.read_csv(f\"{BASE}/elliptic_txs_features.csv\", header=None)\n",
        "        feat_df = feat_df.rename(columns={0: \"txId\", 1: \"time_step\"})\n",
        "    else:\n",
        "        print(\"Reusing existing feat_df.\")\n",
        "except NameError:\n",
        "    print(\"feat_df not found. Reloading elliptic_txs_features.csv.\")\n",
        "    feat_df = pd.read_csv(f\"{BASE}/elliptic_txs_features.csv\", header=None)\n",
        "    feat_df = feat_df.rename(columns={0: \"txId\", 1: \"time_step\"})\n",
        "\n",
        "\n",
        "txids = feat_df['txId'].tolist() # Use the new column name 'txId' after renaming\n",
        "# Create a mapping from the original transaction ID (node in edge list) to the PyG node index\n",
        "# The PyG dataset reorders nodes, so we need to align.\n",
        "# The first column of data.x corresponds to the original txId in the raw features file.\n",
        "# We can build the mapping based on the first feature column. This is a bit fragile,\n",
        "# but often the first feature corresponds to a remapped ID or is ordered consistently.\n",
        "# A more robust way would be if PyG exposed the original txId mapping.\n",
        "# Let's assume the order is consistent for this demo:\n",
        "id_to_node_idx = {int(data.x[i, 0].item()): i for i in range(data.num_nodes)}\n",
        "# And the reverse mapping\n",
        "node_idx_to_id = {i: int(data.x[i, 0].item()) for i in range(data.num_nodes)}\n",
        "\n",
        "# Now build sequences using the edge list and time steps\n",
        "try:\n",
        "    # Try to reuse edge_df if it exists\n",
        "    if 'edge_df' not in globals() or not isinstance(edge_df, pd.DataFrame):\n",
        "        print(\"edge_df not found. Reloading elliptic_txs_edgelist.csv.\")\n",
        "        edge_df = pd.read_csv(f\"{BASE}/elliptic_txs_edgelist.csv\")\n",
        "    else:\n",
        "        print(\"Reusing existing edge_df.\")\n",
        "except NameError:\n",
        "    print(\"edge_df not found. Reloading elliptic_txs_edgelist.csv.\")\n",
        "    edge_df = pd.read_csv(f\"{BASE}/elliptic_txs_edgelist.csv\")\n",
        "\n",
        "\n",
        "# Add time steps to the edge_df by merging with feat_df\n",
        "edge_df_timed = edge_df.merge(feat_df[['txId', 'time_step']], left_on='txId1', right_on='txId', how='left')\n",
        "edge_df_timed = edge_df_timed.merge(feat_df[['txId', 'time_step']], left_on='txId2', right_on='txId', how='left', suffixes=('_src', '_dst'))\n",
        "edge_df_timed = edge_df_timed.drop(columns=['txId_src', 'txId_dst'])\n",
        "\n",
        "# Filter edges to only include those where both source and destination are in our PyG data (nodes)\n",
        "# This is important because PyG might not include all nodes from the original edge list\n",
        "# We need to map original txIds in edge_df to PyG node indices\n",
        "edge_df_timed['txId1_idx'] = edge_df_timed['txId1'].map(id_to_node_idx)\n",
        "edge_df_timed['txId2_idx'] = edge_df_timed['txId2'].map(id_to_node_idx)\n",
        "edge_df_timed_mapped = edge_df_timed.dropna(subset=['txId1_idx', 'txId2_idx']).copy()\n",
        "edge_df_timed_mapped['txId1_idx'] = edge_df_timed_mapped['txId1_idx'].astype(int)\n",
        "edge_df_timed_mapped['txId2_idx'] = edge_df_timed_mapped['txId2_idx'].astype(int)\n",
        "\n",
        "# Sort edges by time step for sequence building\n",
        "edge_df_timed_mapped = edge_df_timed_mapped.sort_values('time_step_src') # Or time_step_dst, or average\n",
        "\n",
        "# Now build sequences based on these mapped edges and time steps\n",
        "def build_sequences_elliptic(df_edges, num_nodes, L=10):\n",
        "    seqs = {nid: [] for nid in range(num_nodes)}\n",
        "\n",
        "    for _, row in df_edges.iterrows():\n",
        "        src_idx = row['txId1_idx']\n",
        "        dst_idx = row['txId2_idx']\n",
        "        # Use a simple edge feature, e.g., time step difference or a constant\n",
        "        # For simplicity, let's use the source node's time step and a type indicator (1 for outgoing, 0 for incoming)\n",
        "        timestamp = row['time_step_src']\n",
        "        # You could add other features here if available and relevant\n",
        "\n",
        "        if src_idx in seqs:\n",
        "             # Sequence element: [feature1, ..., timestamp, type]\n",
        "             # Let's use time_step_dst for incoming for consistency, and a dummy feature\n",
        "             seqs[src_idx].append([1.0, timestamp, 1])  # outgoing: [dummy_feature, timestamp, type=1]\n",
        "        if dst_idx in seqs:\n",
        "             seqs[dst_idx].append([1.0, row['time_step_dst'], 0])  # incoming: [dummy_feature, timestamp, type=0]\n",
        "\n",
        "\n",
        "    # pad or trim\n",
        "    X = []\n",
        "    for nid in range(num_nodes):\n",
        "        # Sort history by timestamp and take the last L\n",
        "        hist = sorted(seqs[nid], key=lambda x: x[1])[-L:]\n",
        "        hist = np.array(hist)\n",
        "\n",
        "        # Handle empty history case\n",
        "        if len(hist) == 0:\n",
        "             hist_padded = np.zeros((L, 3)) # Create a full padding array\n",
        "        elif len(hist) < L:\n",
        "            # Pad if history is shorter than L\n",
        "            pad = np.zeros((L-len(hist), 3)) # Assuming sequence elements are [feature, timestamp, type]\n",
        "            hist_padded = np.vstack([pad, hist])\n",
        "        else:\n",
        "            # History is already L or longer, just use the last L (already done by [-L:])\n",
        "            hist_padded = hist\n",
        "\n",
        "        X.append(hist_padded)\n",
        "\n",
        "    return torch.tensor(np.array(X), dtype=torch.float)\n",
        "\n",
        "# Build sequence tensor for all nodes in the Elliptic dataset\n",
        "seq_tensor_elliptic = build_sequences_elliptic(edge_df_timed_mapped, data.num_nodes, L=10).to(device)\n",
        "print(\"Elliptic sequence tensor built:\", seq_tensor_elliptic.shape)\n",
        "\n",
        "# --- Define Hybrid Model (same as before) ---\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hid=64, out_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_dim, hid)\n",
        "        self.conv2 = SAGEConv(hid, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "class SeqEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        lstm_out, _ = self.lstm(sequences)\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "        return self.linear(last_hidden_state)\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, g_in, g_hid=64, g_out=64, s_in=3, s_hid=32, s_out=64, final_hid=64):\n",
        "        super().__init__()\n",
        "        self.gnn = GraphEncoder(g_in, g_hid, g_out)\n",
        "        self.seq = SeqEncoder(s_in, s_hid, s_out)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(g_out+s_out, final_hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(final_hid, 2) # 2 classes (licit/illicit)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, seqs):\n",
        "        g_emb = self.gnn(x, edge_index)\n",
        "        s_emb = self.seq(seqs)\n",
        "        joint = torch.cat([g_emb, s_emb], dim=1)\n",
        "        return self.mlp(joint)\n",
        "\n",
        "# --- Prepare Model, Optimizer, Loss ---\n",
        "model_elliptic = HybridModel(g_in=data.x.shape[1], s_in=seq_tensor_elliptic.shape[-1]).to(device)\n",
        "optimizer_elliptic = torch.optim.Adam(model_elliptic.parameters(), lr=0.001)\n",
        "criterion_elliptic = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop on Elliptic Data ---\n",
        "# Use Elliptic's built-in train/test masks\n",
        "train_mask_elliptic = data.train_mask.to(device)\n",
        "test_mask_elliptic = data.test_mask.to(device)\n",
        "y_elliptic = data.y.to(device)\n",
        "\n",
        "# Filter out 'unknown' class (label 2) from training\n",
        "known_mask_elliptic = (y_elliptic != 2)\n",
        "train_mask_known = train_mask_elliptic & known_mask_elliptic\n",
        "\n",
        "print(\"Starting training on Elliptic dataset...\")\n",
        "EPOCHS = 20 # Reduced epochs for faster demo\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model_elliptic.train()\n",
        "    optimizer_elliptic.zero_grad()\n",
        "    # Use full data object for GNN, and the elliptic sequence tensor\n",
        "    out = model_elliptic(data.x.to(device), data.edge_index.to(device), seq_tensor_elliptic)\n",
        "    # Only compute loss on known training nodes\n",
        "    loss = criterion_elliptic(out[train_mask_known], y_elliptic[train_mask_known])\n",
        "    loss.backward()\n",
        "    optimizer_elliptic.step()\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        model_elliptic.eval()\n",
        "        with torch.no_grad():\n",
        "            out_eval = model_elliptic(data.x.to(device), data.edge_index.to(device), seq_tensor_elliptic)\n",
        "            # Evaluate on test set (which includes known labels)\n",
        "            test_logits = out_eval[test_mask_elliptic]\n",
        "            test_labels = y_elliptic[test_mask_elliptic]\n",
        "\n",
        "            # Filter out unknown labels from the test set for evaluation metrics\n",
        "            test_known_mask = (test_labels != 2)\n",
        "            test_logits_known = test_logits[test_known_mask]\n",
        "            test_labels_known = test_labels[test_known_mask]\n",
        "\n",
        "            if test_labels_known.size(0) > 0:\n",
        "                test_probs = torch.softmax(test_logits_known, dim=1)[:, 1].cpu().numpy()\n",
        "                test_labels_np = test_labels_known.cpu().numpy()\n",
        "\n",
        "                # Calculate metrics (AUPRC, ROC-AUC)\n",
        "                if len(np.unique(test_labels_np)) > 1:\n",
        "                     ap = average_precision_score(test_labels_np, test_probs)\n",
        "                     roc = roc_auc_score(test_labels_np, test_probs)\n",
        "                     print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Test AUPRC: {ap:.4f} | Test ROC-AUC: {roc:.4f}\")\n",
        "                else:\n",
        "                     print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Test set has only one class among known labels.\")\n",
        "            else:\n",
        "                 print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | No known labels in the test set.\")\n",
        "\n",
        "print(\"\\nTraining finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ru77U_Yvykd"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Step F — Metrics, evaluation & live demo\n",
        "# ===============================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Ensure required variables exist from previous steps ---\n",
        "if 'model_elliptic' not in globals():\n",
        "    raise RuntimeError(\"model_elliptic not found. Please run the Elliptic training cell (Alternative Step E).\")\n",
        "if 'data' not in globals() or 'test_mask_elliptic' not in globals() or 'y_elliptic' not in globals():\n",
        "     raise RuntimeError(\"Elliptic data (data, test_mask_elliptic, y_elliptic) not found. Please run the Elliptic training cell.\")\n",
        "if 'node_idx_to_id' not in globals() or 'edge_df' not in globals():\n",
        "     raise RuntimeError(\"Mapping (node_idx_to_id) or original edge_df not found. Please run the Elliptic training cell or relevant data loading cells.\")\n",
        "if 'seq_tensor_elliptic' not in globals():\n",
        "     raise RuntimeError(\"seq_tensor_elliptic not found. Please run the Elliptic training cell.\")\n",
        "\n",
        "\n",
        "model_elliptic.eval()\n",
        "\n",
        "# Forward pass using the Elliptic data and sequence tensor\n",
        "with torch.no_grad():\n",
        "    out = model_elliptic(data.x.to(device),\n",
        "                         data.edge_index.to(device),\n",
        "                         seq_tensor_elliptic.to(device))  # [num_nodes, 2]\n",
        "    probs_all = torch.softmax(out, dim=1)[:, 1].cpu().numpy()  # probability of class 1 (illicit)\n",
        "\n",
        "# Get true labels and probabilities for the test set (on Elliptic data)\n",
        "test_mask_np = test_mask_elliptic.cpu().numpy()\n",
        "y_true_test_all = y_elliptic.cpu().numpy()\n",
        "\n",
        "# Filter to get labels and probabilities for the nodes in the test mask\n",
        "y_true_test = y_true_test_all[test_mask_np]\n",
        "probs_test = probs_all[test_mask_np]\n",
        "\n",
        "# Filter out 'unknown' class (label 2) for evaluation metrics\n",
        "known_mask_test = (y_true_test != 2)\n",
        "y_true_test_known = y_true_test[known_mask_test]\n",
        "probs_test_known = probs_test[known_mask_test]\n",
        "\n",
        "\n",
        "# --- 1. Average Precision (AUPRC)\n",
        "if len(np.unique(y_true_test_known)) > 1:\n",
        "    ap = average_precision_score(y_true_test_known, probs_test_known)\n",
        "    print(\"Average Precision (AUPRC) on Test Set:\", ap)\n",
        "else:\n",
        "    print(\"AUPRC cannot be calculated: Test set contains only one known class.\")\n",
        "    ap = float('nan')\n",
        "\n",
        "\n",
        "# --- 2. Precision@K\n",
        "def precision_at_k(y_true, y_scores, k=10):\n",
        "    # Ensure k is not greater than the number of test samples with known labels\n",
        "    k = min(k, len(y_true))\n",
        "    if k == 0 or len(y_true) == 0:\n",
        "        return float('nan')\n",
        "    # Sort by scores in descending order and get top k indices\n",
        "    order = np.argsort(y_scores)[::-1]\n",
        "    topk_indices = order[:k]\n",
        "    # Count how many of the top k predicted are actually class 1 (illicit)\n",
        "    # Need to map back to original labels if y_true is filtered\n",
        "    # Assuming y_true here is already filtered to known classes (0 or 1)\n",
        "    return (y_true[topk_indices] == 1).sum() / k\n",
        "\n",
        "\n",
        "print(\"\\nPrecision@K on Test Set (among known labels):\")\n",
        "for K in [10, 50, 100]:\n",
        "     if len(y_true_test_known) > 0:\n",
        "        print(f\"Precision@{K}: {precision_at_k(y_true_test_known, probs_test_known, K):.4f}\")\n",
        "     else:\n",
        "        print(f\"Precision@{K}: NaN (No test nodes with known labels)\")\n",
        "\n",
        "\n",
        "# --- 3. ROC-AUC and PR curves\n",
        "if len(np.unique(y_true_test_known)) > 1:\n",
        "    roc = roc_auc_score(y_true_test_known, probs_test_known)\n",
        "    print(\"\\nROC-AUC on Test Set (among known labels):\", roc)\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true_test_known, probs_test_known)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(recall, precision, label=\"PR Curve\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve on Test Set (Known Labels)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nROC-AUC and PR curve cannot be calculated: Test set contains only one known class.\")\n",
        "    roc = float('nan')\n",
        "\n",
        "\n",
        "# --- 4. Confusion matrix (threshold 0.5)\n",
        "if len(y_true_test_known) > 0:\n",
        "    preds_test_known = (probs_test_known > 0.5).astype(int)\n",
        "    cm = confusion_matrix(y_true_test_known, preds_test_known)\n",
        "    print(\"\\nConfusion Matrix on Test Set (Known Labels, Threshold 0.5):\\n\", cm)\n",
        "else:\n",
        "     print(\"\\nConfusion Matrix cannot be calculated: No test nodes with known labels.\")\n",
        "\n",
        "\n",
        "# --- 5. Sample wallet live demo (top predicted illicit node from test set with known label)\n",
        "# Find top predicted illicit node among the test nodes with known labels (0 or 1)\n",
        "test_idx_np = test_mask_elliptic.nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
        "test_idx_known = test_idx_np[known_mask_test] # Indices of test nodes with known labels\n",
        "\n",
        "if len(test_idx_known) > 0:\n",
        "    # Get the probabilities specifically for these known test nodes\n",
        "    probs_known_test_nodes = probs_all[test_idx_known]\n",
        "\n",
        "    # Find the index (within test_idx_known) of the node with the highest illicit probability\n",
        "    top_test_node_idx_in_known_test = np.argmax(probs_known_test_nodes)\n",
        "    # Get the actual node index (within the full data.x)\n",
        "    top_test_node_idx_overall = test_idx_known[top_test_node_idx_in_known_test]\n",
        "\n",
        "    # Get the original transaction ID using the mapping\n",
        "    if top_test_node_idx_overall in node_idx_to_id:\n",
        "        top_test_node_original_id = node_idx_to_id[top_test_node_idx_overall]\n",
        "        risk_score = probs_all[top_test_node_idx_overall]\n",
        "        print(f\"\\nTop predicted illicit node (original ID): {top_test_node_original_id}, Risk score: {risk_score:.4f}\")\n",
        "\n",
        "        # Build 1-hop subgraph around the node using networkx (using original IDs from edge_df)\n",
        "        if 'edge_df' in globals() and isinstance(edge_df, pd.DataFrame):\n",
        "            G_original = nx.DiGraph() # Elliptic edges are directed\n",
        "            # Add edges with original txIds\n",
        "            G_original.add_edges_from(edge_df[['txId1', 'txId2']].values)\n",
        "\n",
        "            # Check if the top node ID exists in the original graph\n",
        "            if top_test_node_original_id in G_original:\n",
        "                # Get 1-hop neighbors (predecessors and successors)\n",
        "                predecessors = list(G_original.predecessors(top_test_node_original_id))\n",
        "                successors = list(G_original.successors(top_test_node_original_id))\n",
        "                sub_nodes_original = [top_test_node_original_id] + predecessors + successors\n",
        "\n",
        "                # Create subgraph containing these nodes and edges between them\n",
        "                subG_original = G_original.subgraph(sub_nodes_original).copy()\n",
        "\n",
        "                plt.figure(figsize=(8, 8))\n",
        "                pos = nx.spring_layout(subG_original, seed=42) # Layout algorithm\n",
        "\n",
        "                # Color nodes based on label if available in the original class_df\n",
        "                node_colors = []\n",
        "                node_labels_dict = class_df.set_index('txId')['class'].to_dict() if 'class_df' in globals() else {}\n",
        "                for n in subG_original.nodes():\n",
        "                    label = node_labels_dict.get(n, 'unknown')\n",
        "                    if label == '1': # Illicit\n",
        "                        node_colors.append('red')\n",
        "                    elif label == '2': # Licit\n",
        "                         node_colors.append('green')\n",
        "                    elif n == top_test_node_original_id: # Top predicted node\n",
        "                         node_colors.append('gold') # Highlight the target node\n",
        "                    else: # Unknown or other\n",
        "                        node_colors.append('lightblue')\n",
        "\n",
        "                # Draw the graph\n",
        "                nx.draw(subG_original, pos, with_labels=True, node_color=node_colors,\n",
        "                        node_size=700, edge_color='gray', alpha=0.8, arrows=True) # Added arrows=True for directed graph\n",
        "                plt.title(f\"1-hop subgraph for original transaction ID {top_test_node_original_id}\")\n",
        "                plt.show()\n",
        "            else:\n",
        "                 print(f\"Original node ID {top_test_node_original_id} not found in the original edge list graph.\")\n",
        "        else:\n",
        "             print(\"Original edge_df not found. Cannot visualize subgraph using original IDs.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Could not map node index {top_test_node_idx_overall} to original transaction ID.\")\n",
        "\n",
        "\n",
        "    # Optional: show embedding vector for the top predicted node (using its index in the full data)\n",
        "    if hasattr(model_elliptic, 'gnn') and hasattr(model_elliptic, 'seq'):\n",
        "        with torch.no_grad():\n",
        "            # Get the embedding for the top predicted node using its index in the full data\n",
        "            g_emb = model_elliptic.gnn(data.x.to(device), data.edge_index.to(device))\n",
        "            s_emb = model_elliptic.seq(seq_tensor_elliptic.to(device))\n",
        "            joint_emb = torch.cat([g_emb, s_emb], dim=1)\n",
        "            # The index within joint_emb is top_test_node_idx_overall\n",
        "            print(f\"\\nNode embedding vector (full data index {top_test_node_idx_overall}):\\n\", joint_emb[top_test_node_idx_overall].cpu().numpy())\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo test nodes with known labels available to find the top predicted illicit node for visualization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_javoAJKxIX2"
      },
      "source": [
        "Step G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaALNS5esSHT"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Step G — Visuals / Subgraph extraction\n",
        "# ===============================\n",
        "\n",
        "!pip install --quiet pyvis networkx\n",
        "\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "import torch\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# --- Assume src, dst, weights arrays from your Ethereum/Bitcoin graph (Steps B/C/D/E)\n",
        "# We will use the Elliptic dataset's edge_index for visualization as it was used for training\n",
        "if 'data' not in globals():\n",
        "    raise RuntimeError(\"Elliptic data (data) not found. Please run the Elliptic training cell.\")\n",
        "\n",
        "src = data.edge_index[0].cpu().numpy()\n",
        "dst = data.edge_index[1].cpu().numpy()\n",
        "# If you have edge weights (e.g., transaction amount), else set 1\n",
        "# The Elliptic dataset doesn't have explicit edge weights in data.edge_attr\n",
        "weights = np.ones(len(src)) # Use dummy weights if none available\n",
        "\n",
        "\n",
        "# --- Build directed graph\n",
        "G = nx.DiGraph() # Use DiGraph for directed edges in Elliptic\n",
        "for s, d, w in zip(src, dst, weights):\n",
        "    G.add_edge(int(s), int(d), weight=float(w))\n",
        "\n",
        "# --- Choose a suspicious node (top predicted illicit node from Step F)\n",
        "# Ensure top_test_node_original_id is available from Step F\n",
        "if 'top_test_node_original_id' not in globals():\n",
        "     raise RuntimeError(\"top_test_node_original_id not found. Please run Step F first.\")\n",
        "\n",
        "suspicious_node_id = int(top_test_node_original_id) # Ensure it's a standard integer\n",
        "radius = 2  # 1-hop or 2-hop neighbors\n",
        "\n",
        "# Extract induced subgraph with radius r\n",
        "# Use the original graph G with original node IDs\n",
        "nodes = set([suspicious_node_id])\n",
        "# Need to iterate through neighbors in the original graph G\n",
        "for _ in range(radius):\n",
        "    neighbors = set()\n",
        "    for v in list(nodes): # Iterate over a copy of nodes to avoid modifying while iterating\n",
        "        if v in G: # Check if node exists in the graph\n",
        "            neighbors.update(G.predecessors(v))\n",
        "            neighbors.update(G.successors(v))\n",
        "    nodes.update(neighbors)\n",
        "\n",
        "# Create subgraph from the original graph G using the collected nodes\n",
        "subG = G.subgraph(nodes).copy()\n",
        "\n",
        "print(f\"Subgraph nodes ({len(subG.nodes)}): {subG.nodes}\")\n",
        "print(f\"Subgraph edges ({len(subG.edges)}): {subG.edges}\")\n",
        "\n",
        "# --- Visualization using PyVis (interactive)\n",
        "# Wrap PyVis visualization in a try-except block as it can be inconsistent in notebooks\n",
        "try:\n",
        "    net = Network(height=\"600px\", width=\"100%\", directed=True) # directed=True for Elliptic\n",
        "    # Add nodes to PyVis network\n",
        "    # Need mapping from original ID back to PyG index for potential coloring based on model output\n",
        "    # We'll color based on original labels if class_df is available\n",
        "    node_labels_dict = class_df.set_index('txId')['class'].to_dict() if 'class_df' in globals() else {}\n",
        "    if 'probs_all' in globals() and 'id_to_node_idx' in globals():\n",
        "         # Use model probability for coloring if available\n",
        "         node_colors_map = {}\n",
        "         for n in subG.nodes:\n",
        "             color = 'lightblue' # Default color\n",
        "             if n in id_to_node_idx:\n",
        "                  pyg_idx = id_to_node_idx[n]\n",
        "                  if pyg_idx < len(probs_all): # Ensure index is within bounds\n",
        "                      prob = probs_all[pyg_idx]\n",
        "                      if prob > 0.8: # Example threshold for 'likely illicit'\n",
        "                          color = 'red'\n",
        "                      elif prob < 0.2: # Example threshold for 'likely licit'\n",
        "                           color = 'green'\n",
        "             node_colors_map[n] = color\n",
        "    else:\n",
        "         # Fallback to coloring based on original labels or default\n",
        "         node_colors_map = {}\n",
        "         for n in subG.nodes:\n",
        "             label = node_labels_dict.get(n, 'unknown')\n",
        "             if label == '1':\n",
        "                 color = 'red'\n",
        "             elif label == '2':\n",
        "                 color = 'green'\n",
        "             elif n == suspicious_node_id:\n",
        "                 color = 'gold'\n",
        "             else:\n",
        "                 color = 'lightblue'\n",
        "             node_colors_map[n] = color\n",
        "\n",
        "\n",
        "    for n in subG.nodes:\n",
        "        net.add_node(int(n), label=str(n), color=node_colors_map.get(n, 'lightblue'), size=15) # Ensure node ID is int\n",
        "\n",
        "    for u, v, d in subG.edges(data=True):\n",
        "        # Use weight for edge thickness (e.g., transaction value if available, otherwise dummy)\n",
        "        # For Elliptic, we don't have edge weights in data.edge_attr, use dummy weight or 1\n",
        "        edge_weight = d.get('weight', 1.0) # Get weight if exists, else 1\n",
        "        net.add_edge(int(u), int(v), value=edge_weight) # Ensure u,v are int\n",
        "\n",
        "\n",
        "    # Show interactive network in Colab\n",
        "    net.show(\"subgraph.html\")\n",
        "    from IPython.display import IFrame\n",
        "    display(IFrame(\"subgraph.html\", width=800, height=600))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error generating PyVis visualization: {e}\")\n",
        "    print(\"Interactive visualization with PyVis might not be fully supported or requires specific setup in this environment.\")\n",
        "    print(\"The subgraph nodes and edges have been printed above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2984548e"
      },
      "source": [
        "Step H - Analyze Model Performance\n",
        "\n",
        "Analyze the hybrid model's performance on different subsets of the Elliptic dataset, focusing on the known classes (licit and illicit) and potentially across time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8d13947"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Ensure required variables exist ---\n",
        "if 'model_elliptic' not in globals():\n",
        "    raise RuntimeError(\"model_elliptic not found. Please run the Elliptic training cell (Step E Alternative).\")\n",
        "if 'data' not in globals() or 'y_elliptic' not in globals() or 'test_mask_elliptic' not in globals():\n",
        "     raise RuntimeError(\"Elliptic data (data, y_elliptic, test_mask_elliptic) not found. Please run Step E Alternative.\")\n",
        "if 'probs_all' not in globals():\n",
        "     print(\"probs_all not found. Running forward pass to get predictions.\")\n",
        "     model_elliptic.eval()\n",
        "     with torch.no_grad():\n",
        "          out = model_elliptic(data.x.to(device), data.edge_index.to(device), seq_tensor_elliptic.to(device))\n",
        "          probs_all = torch.softmax(out, dim=1)[:, 1].cpu().numpy() # Probability of class 1 (illicit)\n",
        "\n",
        "\n",
        "# --- Analyze performance on known test nodes (licit vs illicit) ---\n",
        "test_mask_np = test_mask_elliptic.cpu().numpy()\n",
        "y_true_test_all = y_elliptic.cpu().numpy()\n",
        "probs_test_all = probs_all # Probs for all nodes\n",
        "\n",
        "# Filter to test set and known labels (0 or 1)\n",
        "test_idx_np = np.where(test_mask_np)[0]\n",
        "y_true_test_known_all_idx = test_idx_np[(y_true_test_all[test_idx_np] != 2)]\n",
        "y_true_test_known = y_true_test_all[y_true_test_known_all_idx]\n",
        "probs_test_known = probs_test_all[y_true_test_known_all_idx]\n",
        "\n",
        "# Separate based on true labels\n",
        "illicit_mask_test_known = (y_true_test_known == 1)\n",
        "licit_mask_test_known = (y_true_test_known == 2) # Note: Elliptic uses 2 for licit in test, 0 in train for known\n",
        "# Need to adjust for the original labels in y_elliptic: 1 for illicit, 2 for licit (in test set)\n",
        "# In the training step we filtered out label 2 for *training*, but for test evaluation it is a known class.\n",
        "# Let's use the original labels from y_elliptic for clarity in analysis.\n",
        "test_idx_illicit = test_idx_np[(y_true_test_all[test_idx_np] == 1)]\n",
        "test_idx_licit = test_idx_np[(y_true_test_all[test_idx_np] == 2)]\n",
        "\n",
        "\n",
        "print(\"\\n--- Performance Analysis on Known Test Nodes ---\")\n",
        "print(f\"Total known test nodes: {len(y_true_test_known)}\")\n",
        "print(f\"Illicit test nodes: {len(test_idx_illicit)}\")\n",
        "print(f\"Licit test nodes: {len(test_idx_licit)}\")\n",
        "\n",
        "if len(test_idx_illicit) > 0:\n",
        "    probs_illicit = probs_all[test_idx_illicit]\n",
        "    avg_prob_illicit = np.mean(probs_illicit)\n",
        "    # AUPRC and ROC-AUC on the full test set with known labels were already calculated in Step F.\n",
        "    # We can look at the distribution of probabilities for illicit nodes.\n",
        "    print(f\"Average predicted probability for illicit nodes: {avg_prob_illicit:.4f}\")\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.hist(probs_illicit, bins=50, color='red', alpha=0.7)\n",
        "    plt.title(\"Distribution of predicted probabilities for Illicit nodes (Test Set)\")\n",
        "    plt.xlabel(\"Predicted Probability of Illicit\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No illicit nodes in the test set with known labels for detailed analysis.\")\n",
        "\n",
        "if len(test_idx_licit) > 0:\n",
        "    probs_licit = probs_all[test_idx_licit]\n",
        "    avg_prob_licit = np.mean(probs_licit)\n",
        "    print(f\"Average predicted probability for licit nodes: {avg_prob_licit:.4f}\")\n",
        "    plt.figure(figsize=(6,3))\n",
        "    plt.hist(probs_licit, bins=50, color='green', alpha=0.7)\n",
        "    plt.title(\"Distribution of predicted probabilities for Licit nodes (Test Set)\")\n",
        "    plt.xlabel(\"Predicted Probability of Illicit\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No licit nodes in the test set with known labels for detailed analysis.\")\n",
        "\n",
        "# --- Optional: Analyze performance across time steps (for known test nodes) ---\n",
        "if 'feat_df' in globals() and 'time_step' in feat_df.columns and 'id_to_node_idx' in globals():\n",
        "    print(\"\\n--- Performance Analysis Across Time Steps (Known Test Nodes) ---\")\n",
        "    # Get time steps for known test nodes\n",
        "    time_steps_all = feat_df['time_step'].values\n",
        "    time_steps_test_known = time_steps_all[y_true_test_known_all_idx]\n",
        "\n",
        "    # Create a DataFrame for easy grouping\n",
        "    analysis_df = pd.DataFrame({\n",
        "        'time_step': time_steps_test_known,\n",
        "        'true_label': y_true_test_known,\n",
        "        'predicted_prob': probs_test_known\n",
        "    })\n",
        "\n",
        "    # Group by time step and calculate average probability for illicit nodes\n",
        "    illicit_analysis = analysis_df[analysis_df['true_label'] == 1].groupby('time_step')['predicted_prob'].mean()\n",
        "    licit_analysis = analysis_df[analysis_df['true_label'] == 2].groupby('time_step')['predicted_prob'].mean()\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    illicit_analysis.plot(label='Illicit (Avg Prob)')\n",
        "    licit_analysis.plot(label='Licit (Avg Prob)')\n",
        "    plt.title(\"Average Predicted Probability by Time Step (Known Test Nodes)\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Average Predicted Probability of Illicit\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping time step analysis: feat_df or time_step column not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bxrhn0_ykyA"
      },
      "source": [
        "Step I Preparing to deploy in Streamlit cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2EonNZKyjWk"
      },
      "outputs": [],
      "source": [
        "!ls -R /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV4ueoMN3KGq"
      },
      "outputs": [],
      "source": [
        "# scripts/train_elliptic_sage.py  (shortened)\n",
        "import torch, torch.nn.functional as F, numpy as np\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "class SageNet(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hid=128, out=2):\n",
        "        super().__init__()\n",
        "        self.c1 = SAGEConv(in_dim, hid)\n",
        "        self.c2 = SAGEConv(hid, hid)\n",
        "        self.out = torch.nn.Linear(hid, out)\n",
        "    def forward(self, x, ei):\n",
        "        x = F.relu(self.c1(x, ei)); x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = F.relu(self.c2(x, ei))\n",
        "        return self.out(x)\n",
        "\n",
        "ds = EllipticBitcoinDataset(root='data/elliptic'); d = ds[0]\n",
        "y = d.y.numpy(); known = y!=0; labels = (y==2).astype(int)\n",
        "\n",
        "# simple time-based split if available\n",
        "# Note: The standard EllipticBitcoinDataset Data object does not have a '.time' attribute.\n",
        "# Using a random split instead.\n",
        "# if hasattr(d, 'time'):\n",
        "#     t = d.time.numpy()\n",
        "#     tr, va, te = (t<=30)&known, (t>30)&(t<=36)&known, (t>36)&known\n",
        "# else:\n",
        "idx = np.where(known)[0]; np.random.RandomState(42).shuffle(idx)\n",
        "n=len(idx); tr,va,te = np.zeros(d.num_nodes,bool),np.zeros(d.num_nodes,bool),np.zeros(d.num_nodes,bool)\n",
        "tr[idx[:int(.7*n)]] = True; va[idx[int(.7*n):int(.85*n)]] = True; te[idx[int(.85*n):]] = True\n",
        "\n",
        "model = SageNet(d.num_node_features).train()\n",
        "opt = torch.optim.AdamW(model.parameters(), 1e-3, weight_decay=1e-5)\n",
        "\n",
        "for ep in range(25):\n",
        "    logits = model(d.x, d.edge_index)\n",
        "    # Elliptic dataset labels are 1 (illicit), 2 (licit), 0 (unknown) in raw.\n",
        "    # PyG maps them to 0, 1, 2. Check dataset doc for exact mapping if needed.\n",
        "    # For this SAGE example, assuming binary classification on known nodes (0/1)\n",
        "    # The d.y tensor contains original labels (0, 1, 2).\n",
        "    # We filter for 'known' nodes (y != 0) and map 2->1 for binary classification loss.\n",
        "    # The target labels for cross_entropy should be 0 or 1.\n",
        "    # d.y[tr][known[tr]] gives labels for known training nodes. Need to map 2 to 1.\n",
        "    train_labels_known = d.y[tr][known[tr]]\n",
        "    train_labels_binary = (train_labels_known == 2).long() # Map 2 (licit) to 1, 1 (illicit) to 0 if needed, or based on how model output relates to classes.\n",
        "    # Based on the original script's `labels = (y==2).astype(int)` and loss calculation:\n",
        "    # it seems the model is trained to predict label 2 (licit) as class 1 and others as class 0?\n",
        "    # Let's align with the original script's apparent intent for binary classification.\n",
        "    # Original: labels = (y==2).astype(int) -> 1 for licit, 0 for illicit/unknown.\n",
        "    # Loss on logits[tr] and d.y[tr].long(). d.y[tr] contains 0, 1, 2.\n",
        "    # If loss is CrossEntropyLoss with 2 classes, target labels must be 0 or 1.\n",
        "    # The model outputs 2 classes. The loss should be applied to known training nodes.\n",
        "    # Let's use d.y[tr] and filter for known, and map the labels (1, 2) to (0, 1) for binary CE loss.\n",
        "    train_mask_known = tr & known\n",
        "    # Filter d.y to known training nodes and map labels 1 -> 0 and 2 -> 1 for binary CrossEntropyLoss\n",
        "    target_labels_binary = (d.y[train_mask_known] == 2).long() # Map 2 (licit) to 1, 1 (illicit) to 0\n",
        "\n",
        "\n",
        "    loss = F.cross_entropy(logits[train_mask_known], target_labels_binary)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "# metrics + save\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    p = torch.softmax(model(d.x, d.edge_index), 1)[:,1].numpy() # Prob of class 1 (which the loss target implies is Licit)\n",
        "# Evaluate AUPRC/AUC on test set known nodes\n",
        "test_mask_known = te & known\n",
        "test_labels_known = d.y[test_mask_known].numpy()\n",
        "test_probs_known = p[test_mask_known]\n",
        "\n",
        "# Create binary labels for evaluation metrics (1 for Licit, 0 for illicit) matching the training target\n",
        "test_labels_binary = (test_labels_known == 2).astype(int)\n",
        "\n",
        "# Ensure there are at least two classes in the test set for metrics\n",
        "if len(np.unique(test_labels_binary)) > 1:\n",
        "    ap = average_precision_score(test_labels_binary, test_probs_known)\n",
        "    auc = roc_auc_score(test_labels_binary, test_probs_known)\n",
        "    print(\"AUPRC (Predicting Licit):\", ap, \"AUC (Predicting Licit):\", auc)\n",
        "else:\n",
        "    print(\"Test set does not contain at least two known classes for AUPRC/AUC calculation.\")\n",
        "    ap = float('nan')\n",
        "    auc = float('nan')\n",
        "\n",
        "# Save model state dict\n",
        "# Create models directory if it doesn't exist\n",
        "import os\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "torch.save(model.state_dict(), \"models/elliptic_sage.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYRKE8ij7Asf"
      },
      "source": [
        "Streamlit app code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70gu-1nG6r4c"
      },
      "outputs": [],
      "source": [
        "import os, json, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import torch, torch.nn.functional as F\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "from torch_geometric.nn import SAGEConv, GATConv\n",
        "from streamlit.components.v1 import html\n",
        "import inspect # Import inspect to check function signatures\n",
        "\n",
        "# ---------- UI SETUP ----------\n",
        "st.set_page_config(page_title=\"Crypto Graph Fraud — GNN Demo\", layout=\"wide\")\n",
        "st.title(\"🕸️ Crypto Graph Fraud — GNN Demo (Elliptic)\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Controls\")\n",
        "    model_name = st.selectbox(\"Model\", [\"GraphSAGE (baseline)\", \"GAT (influence)\"])\n",
        "    k_top = st.slider(\"Top suspicious to list\", 10, 500, 50, step=10)\n",
        "    khop = st.slider(\"Neighborhood radius (k-hop)\", 1, 3, 2)\n",
        "    score_thr = st.slider(\"Alert threshold (prob ≥)\", 0.50, 0.99, 0.90, 0.01)\n",
        "    show_attn = st.checkbox(\"Show attention (GAT only)\", value=True, disabled=(model_name!=\"GAT (influence)\"))\n",
        "\n",
        "st.info(\"Dataset: Elliptic Bitcoin Transaction Graph — node task (illicit vs licit).\")\n",
        "\n",
        "# ---------- MODELS ----------\n",
        "class SageNet(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hid=128, out=2):\n",
        "        super().__init__()\n",
        "        self.c1 = SAGEConv(in_dim, hid)\n",
        "        self.c2 = SAGEConv(hid, hid)\n",
        "        self.out = torch.nn.Linear(hid, out)\n",
        "    def forward(self, x, ei):\n",
        "        x = F.relu(self.c1(x, ei))\n",
        "        x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = F.relu(self.c2(x, ei))\n",
        "        return self.out(x)\n",
        "\n",
        "class GATNet(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hid=64, heads=4, out=2):\n",
        "        super().__init__()\n",
        "        self.g1 = GATConv(in_dim, hid, heads=heads, dropout=0.4)\n",
        "        self.g2 = GATConv(hid*heads, hid, heads=1, dropout=0.4)\n",
        "        self.out = torch.nn.Linear(hid, out)\n",
        "    def forward(self, x, ei, return_attention=False):\n",
        "        if return_attention:\n",
        "            h1, attn1 = self.g1(x, ei, return_attention_weights=True)\n",
        "            h1 = F.elu(h1); h1 = F.dropout(h1, 0.5, training=self.training)\n",
        "            h2, attn2 = self.g2(h1, ei, return_attention_weights=True)\n",
        "            logits = self.out(h2)\n",
        "            return logits, (attn1, attn2)\n",
        "        h1 = F.elu(self.g1(x, ei))\n",
        "        h1 = F.dropout(h1, 0.5, training=self.training)\n",
        "        h2 = self.g2(h1, ei)\n",
        "        return self.out(h2)\n",
        "\n",
        "# ---------- LOAD DATA ----------\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_elliptic():\n",
        "    ds = EllipticBitcoinDataset(root=\"data/elliptic\")\n",
        "    d = ds[0]\n",
        "    # labels: 0 unknown, 1 licit, 2 illicit\n",
        "    y = d.y.numpy()\n",
        "    known = y != 0\n",
        "    labels = (y == 2).astype(int)\n",
        "    # simple time-based split if present\n",
        "    # Note: The standard EllipticBitcoinDataset Data object does not have a '.time' attribute.\n",
        "    # Using a random split instead.\n",
        "    # if hasattr(d, \"time\"):\n",
        "    #     t = d.time.numpy()\n",
        "    #     tr = (t<=30)&known; va = (t>30)&(t<=36)&known; te = (t>36)&known\n",
        "    # else:\n",
        "    idx = np.where(known)[0]; rng = np.random.RandomState(42); rng.shuffle(idx)\n",
        "    n = len(idx)\n",
        "    tr = np.zeros(d.num_nodes, bool); va = np.zeros(d.num_nodes, bool); te = np.zeros(d.num_nodes, bool)\n",
        "    tr[idx[:int(.7*n)]] = True; va[idx[int(.7*n):int(.85*n)]] = True; te[idx[int(.85*n):]] = True\n",
        "    meta = dict(num_nodes=int(d.num_nodes), num_edges=int(d.edge_index.shape[1]),\n",
        "                pos=int(labels.sum()), neg=int((labels==0).sum()))\n",
        "    return d, labels, tr, va, te, meta\n",
        "\n",
        "data, labels, train_mask, val_mask, test_mask, meta = load_elliptic()\n",
        "\n",
        "# ---------- LOAD WEIGHTS / BUILD MODEL ----------\n",
        "def load_model(which):\n",
        "    if which == \"GraphSAGE (baseline)\":\n",
        "        m = SageNet(in_dim=data.num_node_features)\n",
        "        ckpt_path = \"models/elliptic_sage.pt\"\n",
        "    else:\n",
        "        m = GATNet(in_dim=data.num_node_features)\n",
        "        ckpt_path = \"models/elliptic_gat.pt\"\n",
        "    if os.path.exists(ckpt_path):\n",
        "        m.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
        "        st.success(f\"Loaded weights: {ckpt_path}\")\n",
        "    else:\n",
        "        st.warning(f\"No weights found at {ckpt_path}. Running in demo mode (random init). \"\n",
        "                   \"For real results, train offline and commit the .pt file.\")\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "model = load_model(model_name)\n",
        "\n",
        "# ---------- INFERENCE ----------\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def score_all_nodes(model_name):\n",
        "    if model_name == \"GraphSAGE (baseline)\":\n",
        "        logits = model(data.x, data.edge_index)\n",
        "    else:\n",
        "        # If GAT model, ensure forward is called without return_attention initially for full inference\n",
        "        logits = model(data.x, data.edge_index)\n",
        "    probs = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "probs = score_all_nodes(model_name)\n",
        "df_rank = pd.DataFrame({\n",
        "    \"node_id\": np.arange(data.num_nodes),\n",
        "    \"prob_illicit\": probs,\n",
        "    \"label\": labels\n",
        "})\n",
        "df_rank[\"is_alert\"] = df_rank[\"prob_illicit\"] >= score_thr\n",
        "df_rank = df_rank.sort_values(\"prob_illicit\", ascending=False)\n",
        "\n",
        "# ---------- HEADER STATS ----------\n",
        "c1, c2, c3, c4 = st.columns(4)\n",
        "c1.metric(\"Nodes\", f\"{meta['num_nodes']:,}\")\n",
        "c2.metric(\"Edges\", f\"{meta['num_edges']:,}\")\n",
        "c3.metric(\"Illicit (labels)\", f\"{meta['pos']:,}\")\n",
        "c4.metric(\"Licit (labels)\", f\"{meta['neg']:,}\")\n",
        "\n",
        "# metrics if we have a test mask\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve\n",
        "test_idx = np.where(test_mask)[0]\n",
        "if len(test_idx)>0:\n",
        "    ap = average_precision_score(labels[test_idx], probs[test_idx])\n",
        "    auc = roc_auc_score(labels[test_idx], probs[test_idx])\n",
        "    st.write(f\"**Test AUPRC:** {ap:.4f} &nbsp;&nbsp; **Test AUC:** {auc:.4f}\")\n",
        "\n",
        "# ---------- TOP LIST + NODE PICKER ----------\n",
        "left, right = st.columns([2,1])\n",
        "\n",
        "with left:\n",
        "    st.subheader(\"Top suspicious nodes\")\n",
        "    # Fix: Replace use_container_width=True with width='stretch'\n",
        "    st.dataframe(df_rank.head(k_top), width='stretch', height=420)\n",
        "\n",
        "with right:\n",
        "    st.subheader(\"Investigate a node\")\n",
        "    default_node = int(df_rank.head(k_top).iloc[0][\"node_id\"])\n",
        "    node_id = st.number_input(\"Node ID\", min_value=0, max_value=int(data.num_nodes-1),\n",
        "                              value=default_node, step=1)\n",
        "    st.write(\"Score:\", float(probs[node_id]))\n",
        "    st.write(\"Label (if known):\", int(labels[node_id]))\n",
        "\n",
        "# ---------- SUBGRAPH VISUAL ----------\n",
        "st.subheader(\"Neighborhood subgraph\")\n",
        "subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
        "    node_id, khop, data.edge_index, relabel_nodes=True)\n",
        "\n",
        "# attention (GAT only; compute on the subgraph to color edges)\n",
        "attn_width = None\n",
        "if model_name == \"GAT (influence)\" and show_attn:\n",
        "    with torch.no_grad():\n",
        "        # run model forward with attention on the subgraph only\n",
        "        x_sub = data.x[subset]\n",
        "        ei_sub = edge_index\n",
        "        # Check if the model has the 'return_attention' capability\n",
        "        if hasattr(model, 'forward') and 'return_attention' in inspect.signature(model.forward).parameters:\n",
        "            logits_sub, (attn1, attn2) = model(x_sub, ei_sub, return_attention=True)\n",
        "            # use last layer attention for visualization\n",
        "            _, alpha = attn2\n",
        "            attn = alpha.detach().cpu().numpy().flatten()\n",
        "            # normalize to [1, 8] for edge widths\n",
        "            if len(attn) > 0:\n",
        "                a = (attn - attn.min()) / (attn.ptp() + 1e-9)\n",
        "                attn_width = 1.0 + 7.0*a\n",
        "        else:\n",
        "            st.warning(\"Selected GAT model does not support attention visualization.\")\n",
        "\n",
        "\n",
        "# build small HTML with pyvis-like SVG (pure HTML for portability)\n",
        "# lightweight manual draw (no external libs) using D3 force simulation would be long;\n",
        "# Instead, render a simple table + edge list; plus provide a downloadable CSV.\n",
        "sub_nodes = pd.DataFrame({\"node\": subset.numpy()})\n",
        "src = edge_index[0].numpy(); dst = edge_index[1].numpy()\n",
        "edge_df = pd.DataFrame({\"src\": subset.numpy()[src], \"dst\": subset.numpy()[dst]})\n",
        "edge_df[\"w\"] = 1.0\n",
        "if attn_width is not None and len(attn_width)==len(edge_df):\n",
        "    edge_df[\"w\"] = attn_width\n",
        "\n",
        "st.write(\"Nodes in subgraph:\", len(sub_nodes), \"Edges:\", len(edge_df))\n",
        "# Fix: Replace use_container_width=True with width='stretch'\n",
        "st.dataframe(edge_df.head(50), width='stretch', height=240)\n",
        "\n",
        "# tiny viz: highlight seed node and alerts in markdown (portable everywhere)\n",
        "seed = int(subset[mapping])\n",
        "seed_score = float(probs[seed])\n",
        "seed_lab = int(labels[seed])\n",
        "st.markdown(f\"**Seed node:** `{seed}` &nbsp; **Score:** {seed_score:.3f} &nbsp; **Label:** {seed_lab}\")\n",
        "\n",
        "# downloadable CSVs for examiner\n",
        "sub_nodes.to_csv(\"subgraph_nodes.csv\", index=False)\n",
        "edge_df.to_csv(\"subgraph_edges.csv\", index=False)\n",
        "st.download_button(\"Download subgraph nodes CSV\", data=open(\"subgraph_nodes.csv\",\"rb\"), file_name=\"subgraph_nodes.csv\")\n",
        "st.download_button(\"Download subgraph edges CSV\", data=open(\"subgraph_edges.csv\",\"rb\"), file_name=\"subgraph_edges.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmFLd_2M8UAD"
      },
      "outputs": [],
      "source": [
        "# make folders\n",
        "!mkdir -p /content/crypto-graph-demo/scripts\n",
        "!mkdir -p /content/crypto-graph-demo/models\n",
        "!mkdir -p /content/crypto-graph-demo/data\n",
        "\n",
        "# move files into correct folders (if they exist in /content)\n",
        "!mv /content/streamlit_app.py /content/crypto-graph-demo/\n",
        "!mv /content/requirements.txt /content/crypto-graph-demo/\n",
        "!mv /content/train_elliptic_sage.py /content/crypto-graph-demo/scripts/ 2>/dev/null\n",
        "!mv /content/models/elliptic_sage.pt /content/crypto-graph-demo/models/ 2>/dev/null\n",
        "\n",
        "# check structure\n",
        "!ls -R /content/crypto-graph-demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJPI0TPA-FLi"
      },
      "outputs": [],
      "source": [
        "%cd /content/crypto-graph-demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "580f93b7"
      },
      "outputs": [],
      "source": [
        "# Save the Streamlit app code to a file\n",
        "# The code from cell 70gu-1nG6r4c will be saved as streamlit_app.py\n",
        "streamlit_app_code = \"\"\"\n",
        "import os, json, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import torch, torch.nn.functional as F\n",
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "from torch_geometric.nn import SAGEConv, GATConv\n",
        "from streamlit.components.v1 import html\n",
        "import inspect # Import inspect to check function signatures\n",
        "\n",
        "# ---------- UI SETUP ----------\n",
        "st.set_page_config(page_title=\"Crypto Graph Fraud — GNN Demo\", layout=\"wide\")\n",
        "st.title(\"🕸️ Crypto Graph Fraud — GNN Fraud — GNN Demo (Elliptic)\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Controls\")\n",
        "    model_name = st.selectbox(\"Model\", [\"GraphSAGE (baseline)\", \"GAT (influence)\"])\n",
        "    k_top = st.slider(\"Top suspicious to list\", 10, 500, 50, step=10)\n",
        "    khop = st.slider(\"Neighborhood radius (k-hop)\", 1, 3, 2)\n",
        "    score_thr = st.slider(\"Alert threshold (prob ≥)\", 0.50, 0.99, 0.90, 0.01)\n",
        "    show_attn = st.checkbox(\"Show attention (GAT only)\", value=True, disabled=(model_name!=\"GAT (influence)\"))\n",
        "\n",
        "st.info(\"Dataset: Elliptic Bitcoin Transaction Graph — node task (illicit vs licit).\")\n",
        "\n",
        "# ---------- MODELS ----------\n",
        "class SageNet(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hid=128, out=2):\n",
        "        super().__init__()\n",
        "        self.c1 = SAGEConv(in_dim, hid)\n",
        "        self.c2 = SAGEConv(hid, hid)\n",
        "        self.out = torch.nn.Linear(hid, out)\n",
        "    def forward(self, x, ei):\n",
        "        x = F.relu(self.c1(x, ei))\n",
        "        x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = F.relu(self.c2(x, ei))\n",
        "        return self.out(x)\n",
        "\n",
        "class GATNet(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hid=64, heads=4, out=2):\n",
        "        super().__init__()\n",
        "        self.g1 = GATConv(in_dim, hid, heads=heads, dropout=0.4)\n",
        "        self.g2 = GATConv(hid*heads, hid, heads=1, dropout=0.4)\n",
        "        self.out = torch.nn.Linear(hid, out)\n",
        "    def forward(self, x, ei, return_attention=False):\n",
        "        if return_attention:\n",
        "            h1, attn1 = self.g1(x, ei, return_attention_weights=True)\n",
        "            h1 = F.elu(h1); h1 = F.dropout(h1, 0.5, training=self.training)\n",
        "            h2, attn2 = self.g2(h1, ei, return_attention_weights=True)\n",
        "            logits = self.out(h2)\n",
        "            return logits, (attn1, attn2)\n",
        "        h1 = F.elu(self.g1(x, ei))\n",
        "        h1 = F.dropout(h1, 0.5, training=self.training)\n",
        "        h2 = self.g2(h1, ei)\n",
        "        return self.out(h2)\n",
        "\n",
        "# ---------- LOAD DATA ----------\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_elliptic():\n",
        "    ds = EllipticBitcoinDataset(root=\"data/elliptic\")\n",
        "    d = ds[0]\n",
        "    # labels: 0 unknown, 1 licit, 2 illicit\n",
        "    y = d.y.numpy()\n",
        "    known = y != 0\n",
        "    labels = (y == 2).astype(int)\n",
        "    # simple time-based split if present\n",
        "    # Note: The standard EllipticBitcoinDataset Data object does not have a '.time' attribute.\n",
        "    # Using a random split instead.\n",
        "    # if hasattr(d, \"time\"):\n",
        "    #     t = d.time.numpy()\n",
        "    #     tr = (t<=30)&known; va = (t>30)&(t<=36)&known; te = (t>36)&known\n",
        "    # else:\n",
        "    idx = np.where(known)[0]; rng = np.random.RandomState(42); rng.shuffle(idx)\n",
        "    n = len(idx)\n",
        "    tr = np.zeros(d.num_nodes, bool); va = np.zeros(d.num_nodes, bool); te = np.zeros(d.num_nodes, bool)\n",
        "    tr[idx[:int(.7*n)]] = True; va[idx[int(.7*n):int(.85*n)]] = True; te[idx[int(.85*n):]] = True\n",
        "    meta = dict(num_nodes=int(d.num_nodes), num_edges=int(d.edge_index.shape[1]),\n",
        "                pos=int(labels.sum()), neg=int((labels==0).sum()))\n",
        "    return d, labels, tr, va, te, meta\n",
        "\n",
        "data, labels, train_mask, val_mask, test_mask, meta = load_elliptic()\n",
        "\n",
        "# ---------- LOAD WEIGHTS / BUILD MODEL ----------\n",
        "def load_model(which):\n",
        "    if which == \"GraphSAGE (baseline)\":\n",
        "        m = SageNet(in_dim=data.num_node_features)\n",
        "        ckpt_path = \"models/elliptic_sage.pt\"\n",
        "    else:\n",
        "        m = GATNet(in_dim=data.num_node_features)\n",
        "        ckpt_path = \"models/elliptic_gat.pt\"\n",
        "    if os.path.exists(ckpt_path):\n",
        "        m.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
        "        st.success(f\"Loaded weights: {ckpt_path}\")\n",
        "    else:\n",
        "        st.warning(f\"No weights found at {ckpt_path}. Running in demo mode (random init). \"\n",
        "                   \"For real results, train offline and commit the .pt file.\")\n",
        "    m.eval()\n",
        "    return m\n",
        "\n",
        "model = load_model(model_name)\n",
        "\n",
        "# ---------- INFERENCE ----------\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def score_all_nodes(model_name):\n",
        "    if model_name == \"GraphSAGE (baseline)\":\n",
        "        logits = model(data.x, data.edge_index)\n",
        "    else:\n",
        "        # If GAT model, ensure forward is called without return_attention initially for full inference\n",
        "        logits = model(data.x, data.edge_index)\n",
        "    probs = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "probs = score_all_nodes(model_name)\n",
        "df_rank = pd.DataFrame({\n",
        "    \"node_id\": np.arange(data.num_nodes),\n",
        "    \"prob_illicit\": probs,\n",
        "    \"label\": labels\n",
        "})\n",
        "df_rank[\"is_alert\"] = df_rank[\"prob_illicit\"] >= score_thr\n",
        "df_rank = df_rank.sort_values(\"prob_illicit\", ascending=False)\n",
        "\n",
        "# ---------- HEADER STATS ----------\n",
        "c1, c2, c3, c4 = st.columns(4)\n",
        "c1.metric(\"Nodes\", f\"{meta['num_nodes']:,}\")\n",
        "c2.metric(\"Edges\", f\"{meta['num_edges']:,}\")\n",
        "c3.metric(\"Illicit (labels)\", f\"{meta['pos']:,}\")\n",
        "c4.metric(\"Licit (labels)\", f\"{meta['neg']:,}\")\n",
        "\n",
        "# metrics if we have a test mask\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve\n",
        "test_idx = np.where(test_mask)[0]\n",
        "if len(test_idx)>0:\n",
        "    ap = average_precision_score(labels[test_idx], probs[test_idx])\n",
        "    auc = roc_auc_score(labels[test_idx], probs[test_idx])\n",
        "    st.write(f\"**Test AUPRC:** {ap:.4f} &nbsp;&nbsp; **Test AUC:** {auc:.4f}\")\n",
        "\n",
        "# ---------- TOP LIST + NODE PICKER ----------\n",
        "left, right = st.columns([2,1])\n",
        "\n",
        "with left:\n",
        "    st.subheader(\"Top suspicious nodes\")\n",
        "    st.dataframe(df_rank.head(k_top), width='stretch', height=420)\n",
        "\n",
        "with right:\n",
        "    st.subheader(\"Investigate a node\")\n",
        "    default_node = int(df_rank.head(k_top).iloc[0][\"node_id\"])\n",
        "    node_id = st.number_input(\"Node ID\", min_value=0, max_value=int(data.num_nodes-1),\n",
        "                              value=default_node, step=1)\n",
        "    st.write(\"Score:\", float(probs[node_id]))\n",
        "    st.write(\"Label (if known):\", int(labels[node_id]))\n",
        "\n",
        "# ---------- SUBGRAPH VISUAL ----------\n",
        "st.subheader(\"Neighborhood subgraph\")\n",
        "subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
        "    node_id, khop, data.edge_index, relabel_nodes=True)\n",
        "\n",
        "# attention (GAT only; compute on the subgraph to color edges)\n",
        "attn_width = None\n",
        "if model_name == \"GAT (influence)\" and show_attn:\n",
        "    with torch.no_grad():\n",
        "        # run model forward with attention on the subgraph only\n",
        "        x_sub = data.x[subset]\n",
        "        ei_sub = edge_index\n",
        "        # Check if the model has the 'return_attention' capability\n",
        "        if hasattr(model, 'forward') and 'return_attention' in inspect.signature(model.forward).parameters:\n",
        "            logits_sub, (attn1, attn2) = model(x_sub, ei_sub, return_attention=True)\n",
        "            # use last layer attention for visualization\n",
        "            _, alpha = attn2\n",
        "            attn = alpha.detach().cpu().numpy().flatten()\n",
        "            # normalize to [1, 8] for edge widths\n",
        "            if len(attn) > 0:\n",
        "                a = (attn - attn.min()) / (attn.ptp() + 1e-9)\n",
        "                attn_width = 1.0 + 7.0*a\n",
        "        else:\n",
        "            st.warning(\"Selected GAT model does not support attention visualization.\")\n",
        "\n",
        "\n",
        "# build small HTML with pyvis-like SVG (pure HTML for portability)\n",
        "# lightweight manual draw (no external libs) using D3 force simulation would be long;\n",
        "# Instead, render a simple table + edge list; plus provide a downloadable CSV.\n",
        "sub_nodes = pd.DataFrame({\"node\": subset.numpy()})\n",
        "src = edge_index[0].numpy(); dst = edge_index[1].numpy()\n",
        "edge_df = pd.DataFrame({\"src\": subset.numpy()[src], \"dst\": subset.numpy()[dst]})\n",
        "edge_df[\"w\"] = 1.0\n",
        "if attn_width is not None and len(attn_width)==len(edge_df):\n",
        "    edge_df[\"w\"] = attn_width\n",
        "\n",
        "st.write(\"Nodes in subgraph:\", len(sub_nodes), \"Edges:\", len(edge_df))\n",
        "st.dataframe(edge_df.head(50), width='stretch', height=240)\n",
        "\n",
        "# tiny viz: highlight seed node and alerts in markdown (portable everywhere)\n",
        "seed = int(subset[mapping])\n",
        "seed_score = float(probs[seed])\n",
        "seed_lab = int(labels[seed])\n",
        "st.markdown(f\"**Seed node:** `{seed}` &nbsp; **Score:** {seed_score:.3f} &nbsp; **Label:** {seed_lab}\")\n",
        "\n",
        "# downloadable CSVs for examiner\n",
        "sub_nodes.to_csv(\"subgraph_nodes.csv\", index=False)\n",
        "edge_df.to_csv(\"subgraph_edges.csv\", index=False)\n",
        "st.download_button(\"Download subgraph nodes CSV\", data=open(\"subgraph_nodes.csv\",\"rb\"), file_name=\"subgraph_nodes.csv\")\n",
        "st.download_button(\"Download subgraph edges CSV\", data=open(\"subgraph_edges.csv\",\"rb\"), file_name=\"subgraph_edges.csv\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"streamlit_app.py\", \"w\") as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "# Create a requirements.txt file\n",
        "requirements_content = \"\"\"\n",
        "streamlit\n",
        "torch\n",
        "torch_geometric\n",
        "numpy\n",
        "pandas\n",
        "scikit-learn\n",
        "networkx\n",
        "\"\"\"\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "print(\"Created streamlit_app.py and requirements.txt\")\n",
        "!ls"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}