{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshi1707/FinalProject/blob/main/Reinforcement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3jBuYZXqw1_",
        "outputId": "f10da0fd-38fb-4223-f123-bac7ceecf6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "nohup: appending output to 'nohup.out'\n",
            "Streamlit app running at: NgrokTunnel: \"https://0c2492ad11fe.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q streamlit pyngrok pandas numpy scikit-learn tensorflow torch gymnasium stable-baselines3\n",
        "\n",
        "# Import libraries\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gymnasium import Env\n",
        "from gymnasium.spaces import Box\n",
        "from stable_baselines3 import PPO, SAC\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# Define a simple Gymnasium environment for crypto token optimization\n",
        "class CryptoEnv(Env):\n",
        "    def __init__(self):\n",
        "        super(CryptoEnv, self).__init__()\n",
        "        self.action_space = Box(low=-1, high=1, shape=(1,), dtype=np.float32)  # Adjust token amount\n",
        "        self.observation_space = Box(low=0, high=1e6, shape=(3,), dtype=np.float32)  # Amount, rating, time\n",
        "        self.state = None\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 100\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self.state = np.array([1000.0, 100.0, 0.0])  # Initial: amount, rating, time\n",
        "        self.step_count = 0\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        amount, rating, time = self.state\n",
        "        action = action[0]  # Adjust token amount\n",
        "        amount += action * 200  # Scale action\n",
        "        rating += np.random.normal(0, 2)  # Random rating fluctuation (simulating gas/trust)\n",
        "        time += 1\n",
        "        self.state = np.array([max(0, amount), max(0, rating), time])\n",
        "        reward = amount * (rating / 10) if amount > 0 else -abs(amount)  # Optimize for value, reward high trust\n",
        "        done = self.step_count >= self.max_steps or amount < 0\n",
        "        return self.state, reward, done, False, {}\n",
        "\n",
        "# Simple A3C Actor-Critic Network\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, 64)\n",
        "        self.actor = nn.Linear(64, action_dim)\n",
        "        self.critic = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc(x))\n",
        "        action = torch.tanh(self.actor(x))  # Bounded action\n",
        "        value = self.critic(x)\n",
        "        return action, value\n",
        "\n",
        "# A3C Training (Simplified)\n",
        "def train_a3c(env, model, optimizer, episodes=10):\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action, value = model(state_tensor)\n",
        "            # Keep action as tensor, convert to numpy only for env.step\n",
        "            next_state, reward, done, _, _ = env.step(action.detach().cpu().numpy()[0])\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            _, next_value = model(next_state_tensor)\n",
        "            td_error = reward + (1 - done) * 0.99 * next_value - value\n",
        "            # Ensure action is a tensor for log\n",
        "            loss = -torch.log(action[0]) * td_error + td_error.pow(2)  # Simplified loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            state = next_state\n",
        "    return model\n",
        "\n",
        "# Autoencoder for anomaly detection\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(32, activation='relu')(input_layer)\n",
        "    encoded = Dense(16, activation='relu')(encoded)\n",
        "    decoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "# Simple Transformer for time-series prediction\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim=32):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=4, dim_feedforward=128), num_layers=2)\n",
        "        self.fc = nn.Linear(model_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_fc(x)\n",
        "        x = self.encoder(x.unsqueeze(0))\n",
        "        return self.fc(x.squeeze(0))\n",
        "\n",
        "# Write Streamlit app to file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gymnasium import Env\n",
        "from gymnasium.spaces import Box\n",
        "from stable_baselines3 import PPO, SAC\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "# Define CryptoEnv\n",
        "class CryptoEnv(Env):\n",
        "    def __init__(self):\n",
        "        super(CryptoEnv, self).__init__()\n",
        "        self.action_space = Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = Box(low=0, high=1e6, shape=(3,), dtype=np.float32)\n",
        "        self.state = None\n",
        "        self.step_count = 0\n",
        "        self.max_steps = 100\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self.state = np.array([1000.0, 100.0, 0.0])\n",
        "        self.step_count = 0\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        amount, rating, time = self.state\n",
        "        action = action[0]\n",
        "        amount += action * 200\n",
        "        rating += np.random.normal(0, 2)\n",
        "        time += 1\n",
        "        self.state = np.array([max(0, amount), max(0, rating), time])\n",
        "        reward = amount * (rating / 10) if amount > 0 else -abs(amount)\n",
        "        done = self.step_count >= self.max_steps or amount < 0\n",
        "        return self.state, reward, done, False, {}\n",
        "\n",
        "# Simple A3C Actor-Critic Network\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, 64)\n",
        "        self.actor = nn.Linear(64, action_dim)\n",
        "        self.critic = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc(x))\n",
        "        action = torch.tanh(self.actor(x))  # Bounded action\n",
        "        value = self.critic(x)\n",
        "        return action, value\n",
        "\n",
        "# A3C Training (Simplified)\n",
        "def train_a3c(env, model, optimizer, episodes=10):\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action, value = model(state_tensor)\n",
        "            # Keep action as tensor, convert to numpy only for env.step\n",
        "            next_state, reward, done, _, _ = env.step(action.detach().cpu().numpy()[0])\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            _, next_value = model(next_state_tensor)\n",
        "            td_error = reward + (1 - done) * 0.99 * next_value - value\n",
        "            # Ensure action is a tensor for log\n",
        "            loss = -torch.log(action[0]) * td_error + td_error.pow(2)  # Simplified loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            state = next_state\n",
        "    return model\n",
        "\n",
        "# Autoencoder for anomaly detection\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(32, activation='relu')(input_layer)\n",
        "    encoded = Dense(16, activation='relu')(encoded)\n",
        "    decoded = Dense(32, activation='relu')(encoded)\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder\n",
        "\n",
        "# Simple Transformer for time-series prediction\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim=32):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=4, dim_feedforward=128), num_layers=2)\n",
        "        self.fc = nn.Linear(model_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_fc(x)\n",
        "        x = self.encoder(x.unsqueeze(0))\n",
        "        return self.fc(x.squeeze(0))\n",
        "\n",
        "# Streamlit dashboard\n",
        "st.set_page_config(page_title=\"Crypto Token Optimization\", layout=\"wide\")\n",
        "st.title(\"Real-Time Crypto Token Optimization\")\n",
        "\n",
        "# Load Bitcoin OTC dataset from public URL\n",
        "try:\n",
        "    df = pd.read_csv('https://snap.stanford.edu/data/soc-sign-bitcoinotc.csv.gz', compression='gzip', header=None, names=['source', 'target', 'rating', 'time'])\n",
        "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
        "    X = df[['rating', 'time']].copy()\n",
        "    X['time'] = (X['time'] - X['time'].min()).dt.total_seconds()  # Convert time to seconds\n",
        "    X = X.fillna(0)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "except Exception as e:\n",
        "    st.error(f\"Dataset load error: {e}. Using synthetic data.\")\n",
        "    X_scaled = np.random.rand(1000, 2)\n",
        "    df = pd.DataFrame({'rating': X_scaled[:, 0], 'time': X_scaled[:, 1]})\n",
        "\n",
        "# Anomaly detection - Autoencoder\n",
        "input_dim = X_scaled.shape[1]\n",
        "autoencoder = build_autoencoder(input_dim)\n",
        "autoencoder.fit(X_scaled, X_scaled, epochs=5, batch_size=128, verbose=0)\n",
        "recon = autoencoder.predict(X_scaled)\n",
        "mse = np.mean(np.power(X_scaled - recon, 2), axis=1)\n",
        "threshold = np.percentile(mse, 95)\n",
        "autoencoder_anomalies = mse > threshold\n",
        "\n",
        "# Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
        "iso_anomalies = iso_forest.fit_predict(X_scaled) == -1\n",
        "\n",
        "# Hybrid Model (Autoencoder + Isolation Forest)\n",
        "hybrid_anomalies = np.logical_or(autoencoder_anomalies, iso_anomalies)\n",
        "\n",
        "# Transformer prediction\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transformer = SimpleTransformer(input_dim=X_scaled.shape[1]).to(device)\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
        "for _ in range(5):\n",
        "    inputs = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
        "    targets = inputs[:, 0].unsqueeze(1)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = transformer(inputs)\n",
        "    loss = nn.MSELoss()(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# RL models\n",
        "env = CryptoEnv()\n",
        "ppo_model = PPO(\"MlpPolicy\", env, verbose=0).learn(total_timesteps=1000)\n",
        "sac_model = SAC(\"MlpPolicy\", env, verbose=0).learn(total_timesteps=1000)\n",
        "a3c_model = ActorCritic(input_dim=3, action_dim=1).to(device)\n",
        "a3c_optimizer = optim.Adam(a3c_model.parameters(), lr=0.001)\n",
        "a3c_model = train_a3c(env, a3c_model, a3c_optimizer)\n",
        "\n",
        "# Simulate RL\n",
        "def simulate_rl(model, env, n_steps=100):\n",
        "    obs, _ = env.reset()\n",
        "    rewards = []\n",
        "    for _ in range(n_steps):\n",
        "        if isinstance(model, ActorCritic):\n",
        "            state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
        "            action, _ = model(state_tensor)\n",
        "            action = action.detach().cpu().numpy()[0]\n",
        "        else:\n",
        "            action, _ = model.predict(obs)\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "    return rewards\n",
        "\n",
        "ppo_rewards = simulate_rl(ppo_model, env)\n",
        "sac_rewards = simulate_rl(sac_model, env)\n",
        "a3c_rewards = simulate_rl(a3c_model, env)\n",
        "\n",
        "# Dashboard\n",
        "st.header(\"Anomaly Detection\")\n",
        "col1, col2, col3 = st.columns(3)\n",
        "with col1:\n",
        "    st.subheader(\"Autoencoder\")\n",
        "    st.write(f\"Anomalies: {sum(autoencoder_anomalies)}\")\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(mse, bins=50, log=True)\n",
        "    ax.axvline(threshold, color='r', linestyle='--')\n",
        "    st.pyplot(fig)\n",
        "with col2:\n",
        "    st.subheader(\"Isolation Forest\")\n",
        "    st.write(f\"Anomalies: {sum(iso_anomalies)}\")\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=iso_anomalies, cmap='coolwarm')\n",
        "    st.pyplot(fig)\n",
        "with col3:\n",
        "    st.subheader(\"Hybrid (Auto + Iso)\")\n",
        "    st.write(f\"Anomalies: {np.sum(hybrid_anomalies)}\")\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=hybrid_anomalies, cmap='coolwarm')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "st.header(\"RL Optimization Performance\")\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.cumsum(ppo_rewards), label='PPO')\n",
        "ax.plot(np.cumsum(sac_rewards), label='SAC')\n",
        "ax.plot(np.cumsum(a3c_rewards), label='A3C')\n",
        "ax.set_title(\"Cumulative Rewards (Token Optimization)\")\n",
        "ax.legend()\n",
        "st.pyplot(fig)\n",
        "\n",
        "st.header(\"Transformer Prediction\")\n",
        "with torch.no_grad():\n",
        "    pred = transformer(torch.tensor(X_scaled[:10], dtype=torch.float32).to(device)).cpu().numpy()\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(pred, label='Predicted Rating')\n",
        "ax.plot(X_scaled[:10, 0], label='Actual Rating')\n",
        "ax.legend()\n",
        "st.pyplot(fig)\n",
        "\"\"\")\n",
        "\n",
        "# Run Streamlit with ngrok\n",
        "!ngrok authtoken 31r5s9qEOLsZND89IKnKitqkKKp_48taEqEuyUcMZ7r5RhVqj\n",
        "!nohup streamlit run app.py --server.port 8501 &\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app running at: {public_url}\")"
      ]
    }
  ]
}